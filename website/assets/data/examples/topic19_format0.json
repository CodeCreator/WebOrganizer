[
    {
        "url":"https:\/\/collaborate.princeton.edu\/en\/publications\/microstructure-characterization-and-bulk-properties-of-disordered",
        "text":"Microstructure characterization and bulk properties of disordered two-phase media\n\nResearch output: Contribution to journalArticle\n\n91 Scopus citations\n\n\nA general formalism is developed to statistically characterize the microstructure of porous and other composite media composed of inclusions (particles) distributed throughout a matrix phase (which, in the case of porous media, is the void phase). This is accomplished by introducing a new and general n-point distribution function Hn and by deriving two series representations of it in terms of the probability density functions that characterize the configuration of particles; quantities that, in principle, are known for the ensemble under consideration. In the special case of an equilibrium ensemble, these two equivalent but topologically different series for the Hn are generalizations of the Kirkwood-Salsburg and Mayer hierarchies of liquid-state theory for a special mixture of particles described in the text. This methodology provides a means of calculating any class of correlation functions that have arisen in rigorous bounds on transport properties (e.g., conductivity and fluid permeability) and mechanical properties (e.g., elastic moduli) for nontrivial models of two-phase disordered media. Asymptotic and bounding properties of the general function Hn are described. To illustrate the use of the formalism, some new results are presented for the Hn and it is shown how such information is employed to compute bounds on bulk properties for models of fully penetrable (i.e., randomly centered) spheres, totally impenetrable spheres, and spheres distributed with arbitrary degree of impenetrability. Among other results, bounds are computed on the fluid permeability, for assemblages of impenetrable as well as penetrable spheres, with heretofore unattained accuracy.\n\nOriginal languageEnglish (US)\nPages (from-to)843-873\nNumber of pages31\nJournalJournal of Statistical Physics\nIssue number5-6\nStatePublished - Dec 1 1986\nExternally publishedYes\n\nAll Science Journal Classification (ASJC) codes\n\n  \u2022 Statistical and Nonlinear Physics\n  \u2022 Mathematical Physics\n\n\n  \u2022 Disordered media\n  \u2022 Kirkwood-Salsburg equations\n  \u2022 Mayer equations\n  \u2022 composite and porous materials\n  \u2022 effective conductivity\n  \u2022 elastic moduli\n  \u2022 fluid permeability\n  \u2022 liquids\n  \u2022 microstructure\n  \u2022 n-point distribution functions\n  \u2022 scaled-particle theory\n\nFingerprint Dive into the research topics of 'Microstructure characterization and bulk properties of disordered two-phase media'. Together they form a unique fingerprint.\n\n  \u2022 Cite this",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9969065785,
        "format_confidence":0.9624068737
    },
    {
        "url":"https:\/\/www.floridamuseum.ufl.edu\/mammals\/projects\/caribbean-biodiversity\/",
        "text":"Caribbean Biodiversity Projects\n\nPatterns of Gene Flow in the Bahamas\n\nCB_geneflow.jpgWe have a currently funded project that is just getting off the ground to look at patterns of gene flow in the 6 species of bats that occur in the Bahamas. This project seeks to explore the extent to which narrow oceanic straits present barriers to gene flow. We are using a multi-locus data set to estimate changes in effective population size over time and patterns of gene flow. Coupled with the niche modeling data shown below, we can generate testable models of bat evolutionary history in the Bahamas using coalescent simulations. These simulations will allow us to determine which evolutionary models are most likely to produce the kinds of molecular data we observe in modern bats. This also allows us to predict how future climate change might affect Bahamian bat populations.\n\nCB_nichemod.jpgCaribbean Niche Modeling\n\nEcological niche models are routinely used to not only generate predicted distributions for species at the present time, but they are also used to generate past species distributions (e.g., those at the last glacial maximum). There has been a disturbing absence of testing to determine the degree to which these models accurately predict the past distribution of species. We are using the bat species of the Bahamas as a test case whereby we are generating predictive models of species distribution using various methods and testing their accuracy based on the fossil record of bats from the Caribbean. This study involves considerable field collection, computational effort (niche modeling), and molecular data analysis (to define taxonomic boundaries for niche modeling).",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9967273474,
        "format_confidence":0.8582923412
    },
    {
        "url":"http:\/\/yanran.li\/peppypapers\/2016\/01\/09\/nips-2015-deep-learning-symposium-part-ii.html",
        "text":"The part II of my notes and thoughts on NIPS 2015 Deep Learning Symposium. Followings are the papers included in this post:\n\n  1. Early stopping is nonparametric variational inference1\n  2. Natural neural networks2\n  3. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning3\n  4. Neural Turing Machines4\n  5. End-to-end memory networks5\n  6. Grid long-short term memory6\n\nBayesian Deep Learning\n\nEarly stopping is nonparametric variational inference\n\nA Highly recommendated optimization work. Instead of optimizing training loss as usually do, we could also optimize marginal likelihood, which has several advantages: First, we no longer need those validation set-related tricks, e.g., early stopping because we can directly utilize marginal likelihood estimator to evaluate performance. Then, how could this be realized? In this work, they aim at using Bayesian interpretation for optimization process: Every step in the process will \u201cgenerate\u201d a distribution. Hence, a sequence of distributions is produced after the whole optimization process has been done. From the Bayesian prospective, the sequence can be treated as samples drawn from a true posterior distribution, where is the iteration number as a variational parameter. With such interpretation, (1) early stopping is then a trick aiming at optimizing variational lower bound, and (2) ensembling random initializations equals to ensembling various independent variational samples.\n\nBesides the interpretation, this work contributes to constructing a marginal likelihood estimator to select training stop, model selection capacity and model hypermeters.\n\nNote that the optimization method proposed in this work may not beat those methods for training loss such as SGD. This work is recommended because: (1) It presents extensive thoughts on optimization. For example, which one is more trustable, training loss or marginal likelihood? Is a high variational lower bound always led to a high model accuracy? What if the bound indicates reversely against the validation error\/test error? (2) The interpretation is quite powerful, which should be useful for variational sequence learning.\n\nNatural Neural Networks\n\nThis work motivates to optimize distribution gradient instead of point gradient as used by SGD. The authors argue that it is easy to capture distribution in complex NN architectures (also claimed in the above work1).\n\nTo calculate distribution gradient, however, requires solving KL divergence measurement and Fisher matrix. The latter, Fisher matrix requires large computation cost due to the large matrix size and the inverse option. This is why existing work based on Fisher matrix is not scalable.\n\nThe key is to design a constrained Fisher matrix-based NN and its optimization algorithm therefore is similar with Mirror Descent Algorithm. The contribution of this work is to provide a Bayesian framework to incorporate NN tricks: batch normalization (before non-linearity), zero-mean activations and so on.\n\nDropout as a Bayesian approximation: Representing model uncertainty in deep learning\n\nAnother Bayesian work explaining why dropout works. Beyond the sparse regualarization as an explaination, this work is more general and thought-provoking.\n\nThe authors prooved that in theory, dropout equals to Bayesian approximation of a Gaussian Process. This is very similar to dropout as noise regularization, but more mathematical. With this explaination, one can obtain model uncertainty from neural networks with dropout. The uncertainty matters, as always in Bayesian. Recall that existing NN often outputs a prediction with propability for a label using a softmat layer. However, it lacks a certainty for its prediction. If we train a NN with all dog pictures but ask it to predict a cat photo. What will this NN do? It will predicts with a high propability true but a higher uncertainty if could So, now, under the dropout as approximation, we could obtain such uncertainty by using moment-matching technique.\n\nEquipped with this uncertainty, perfomance of lots of tasks, including regression, classification and reinforcement learning, has been improved. Additionally, such Bayesian prospective also benefits the model interpretation. For more additional resources, one can find their related blog here.\n\nBTW, the other two work in this Syposium, \u00abStochastic backpropagation and approximate inference in deep generative models\u00bb and \u00abScalable Bayesian optimization using deep neural networks\u00bb are similar with this work.\n\nUtilizing External Memory\n\nNeural Turing Machines\n\nNeural Turing Machine (NTM) might be the most famous work in this Symposium. Its motivations are: (1) to utilize and interact with external memory beyond the internal memory modeled by hidden units in NN; (2) RNN is proved turing-complete and the natural question is: can we design a turing machine based on RNN?\n\nNeural Turing Machine As the figure above, a NTM contains a Controller, a Read+Write Heads and an External Memory, where Controller is NN. In other words, NTM is added with read+write heads to fulfill interaction with external memory. If we treat the Controller in NTM as the CPU in computer, the memory is RAM and the hidden states are the registers in CPU. Note that the most important are the Read+Write Heads. First of all, they can implement content-based\/location-based options, and thus can mimic the focus\/attention mechanism. This allows to find similar data (content-based) by using content addressing. And after content addressing, interpolation provides gate mechanism, and convolutional shift provides location-based addressing. Based on the above-mentioned modules, NTM is able to mimic turing machine and implement certain algorithms. Moreover, NTM is end-to-end differentiable.\n\nNeural Turing Machine From the two motivations, we can approach to its two goals: (1) NTM is aimed at improving RNN and thus it should be at least able to solve problems like RNN. (2) NTM is designed to mimic turing machine and thus is expected to learn internal algorithms. These two goals lead to the experiments in this work, such as copy task, priority sort task, in comparison with three architectures: NTM with LSTM, NTM with feedforward and standard LSTM.\n\nEnd-to-end memory networks\n\nSimilar with NTM4, this work motivates to utilize large body external memory into neural networks. Different with NTM4, this work only utilizes external memory but lacks in interaction.\n\nEnd-to-end Memory Networks And the authors attempt several ways in this paper to fulfill their goal. First, the single-layer or multi-layer, and then the transformation of feature space. If one separate the output of the end-to-end memory networks, they can be parallized with typical RNN. The output comprises of two parts, internal ouptut and external output, which can be parallized to RNN\u2019s memory and predicted label, respectively.\n\nGrid Long Short-Term Memory\n\nThis work, based on LSTM, proposes a higher framework, which is more flexible and has better computation capability, compared to LSTM.\n\nThe fundamental concepts in this work are grid\/block, stacked and depth. Grid\/block, is a component in the new framework deciding the directions to progagate the information in the networks associated with flexible dimensions. Each dimension has its memory and hidden cells. And the 1-dimensional Grid LSTM, under their framework, seems very similar to the highway networks. The stacked concept is exactly the same as in other neural networks, except the dimension of the input remains unchanged. Stacked 2D Grid LSTM is still 2D, not 3D. What changes the dimension is the depth, which functions inside a block. The number of layers in a block is the depth of the Grid LSTM.\n\nSo, how can this framework deal with the \u201cgradient vanishing\u201d problem? In classical LSTM, gradient descent errors from each dimension are calculated together in each layer. Differently, Grid LSTM separate the calculation by dimensions. For each grid, there are N incoming memory cells and hidden cells, as well as N outgoings (N = size of dimension). By sharing the large H hidden layer information, the Grid LSTM holds both the interaction and information flow.\n\nThe application of their Grid LSTM is interesting, for example, Machine Translation can be transformed as a 3D Grid LSTM problem. When two dimensions are bi-LSTMs, the third dimension depth, the performance is outstanding.\n\nClick here to see the part I.\n\n\n  1. Dougal Maclaurin, David Duvenaud, Ryan P. Adams. Early stopping is nonparametric variational inference. 2015. arXiv preprint: 1504.01344.\u00a0\u00a02\n\n  2. Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray Kavukcuoglu. Natural neural networks. 2015. In Proceedings of NIPS.\u00a0\n\n  3. Yarin Gal, Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. 2015. arXiv preprint: 1506.02142.\u00a0\n\n  4. Alex Graves, Greg Wayne, Ivo Danihelka. Neural Turing Machines. 2014. arXiv preprint: 1410.5401.\u00a0\u00a02\u00a03\n\n  5. End-to-end memory networks. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus. 2015. In Proceedings of NIPS.\u00a0\n\n  6. Nal Kalchbrenner, Ivo Danihelka, Alex Graves. Grid Long Short-Term Memory. 2015. arXiv preprint arXiV:1507.01526.\u00a0\n\nHighway Networks and Deep Residual Networks\n\nRecently, a breakthrough news spread over social networks. In this post, I will explain this ResNet as a special case of Highway Networks, which has been proposed before. Both of the work is amazing and thought-provoking. Continue reading\n\nNIPS 2015 Deep Learning Symposium Part I\n\nPublished on December 11, 2015\n\nICLR 2016 Submission Highlights\n\nPublished on November 30, 2015",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.5566956401,
        "format_confidence":0.7180866599
    },
    {
        "url":"http:\/\/www.nap.edu\/openbook.php?record_id=10561&page=23",
        "text":"in the various categories of use rate do not change over time. Divergences from these assumptions and identification of remedies are the subject of ongoing research.\n\nOnce these and other uses of field performance data have been institutionalized with the accompanying benefits, the quality of such data is likely to improve. One important application in which industrial field performance data have recently been improving in quality is sensors that can collect the entire history of use of, say, an automobile, including stresses, speeds of use, temperature, etc. That information can be linked to information on system reliability or performance to support much richer statistical reliability modeling. Use of such sensors could be valuable in operational testing for similar reasons.1 Several companies are undertaking efforts to save additional data in their warranty databases so the data can be used not only for financial purposes, but also for reliability assessment and estimation. Such efforts represent a cultural change. A hurdle is that development or expansion of such a database sometimes requires innovative funding approaches.\n\nDiscussion of Gaver, Sen, Scholz, and Meeker Papers\n\nIn his discussion of the papers by Sen and Gaver, Paul Ellner addressed the complication involved in reliability growth modeling of translating reliability estimates from developmental test to predictions of reliability in the field from operational test. At present, analysts may use a reliability growth model to extrapolate from developmental test results to operational test results. This approach can be severely biased, producing overly optimistic reliability estimates since the failure modes can be substantially different in the two situations (actually three\u2014developmental test, operational test, and field performance). Efforts to perform this translation face the following challenges: (1) determining the (approximate) relationship between failure modes that occur jointly in developmental and operational environments, and (2) identifying a function expressing the probability of failure in operational test as a function of the probability of failure in operational test due to failure modes not present in developmental test. This translation probably cannot be done at the system level; it must be carried out at the com-\n\n\nThese sensors could be used to monitor reliability degradation during field use and to support efficient logistics management.\n\nThe National Academies | 500 Fifth St. N.W. | Washington, D.C. 20001\nCopyright \u00a9 National Academy of Sciences. All rights reserved.\nTerms of Use and Privacy Statement",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.8424150944,
        "format_confidence":0.9665883183
    },
    {
        "url":"http:\/\/all.net\/books\/iwar\/notes.html",
        "text":"Copyright(c) Management Analytics, 1995 - All Rights Reserved\n\nNote 1:\n\nLet's look at what a typical text says on the subject: ``The noise analysis of communications systems is customarily based on an idealized form of noise called `white noise', the power spectral density of which is independent of the operating frequency. The adjective `white' is used in the sense that white light contains equal amounts of all frequencies within the visible band of electromagnetic radiation. ...'' [Haykin83] The reason cited for the random noise models is the ease of analysis, [Minoli80] but ease and adequacy of analysis are not always compatible.\n\nOne of the most common techniques for detecting corruption in memory and transmission is the use of a `parity' bit associated with each byte. The parity bit is set to 1 or 0 to make the total number of `1's even (or odd, depending on whether the even or odd parity convention is being used). This technique detects ALL single bit errors, which is quite effective against particular sorts of random noise that cause transient faults. It is not effective against an intentional attacker who can change sets of bits collectively while maintaining parity, thus keeping the parity the same while corrupting the information and avoiding detection.\n\nOn disk storage, LAN packets, and in some satellite transmission, CRC (Cyclical Redundancy Check) codes are used to detect classes of faults that result in errors to linear sequences of bits of at most some predefined length. [ISO8208] Again, these codes are ineffective against an intentional attacker, because it is easy to determine the constant coefficients of the coding equations by watching packets, and from this it is easy to forge packets at will undetected. [Cohen94-2]\n\nNote 2:\n\nThis work is essentially oriented toward designing a perfect system wherein all inputs, states, outputs, and state transitions are specified in full detail, and mathematical proofs are provided to show that the design is properly implemented. [Gove91] [Lubbes91] Although this type of solution may be applicable to certain limited control problems in embedded systems, these sorts of solutions are computationally infeasible for any large system, cover ONLY sufficiency and not necessity [Cohen86-2] , only cover limited function systems against disruption, [Cohen94-2] and are beyond current and anticipated capabilities over the next 20 years for the sorts of systems desired in the DII.\n\nAn alternative path to a similar solution is the use of automated program generating programs. In this technology, a small number of programs are designed to automatically write the rest of the programs. Designers spend a great deal of time and effort in perfecting the design automation system which, in turn, designs other systems. [Siewiorek90] This technology is far from perfected, and even if it were perfected, it leaves the problem of writing perfect specifications, which is at least as hard as writing a perfect programs.\n\nIn the hardware realm, design automation has been highly successful, but this does not imply that it will be successful in the software realm. There are substantial differences between hardware and software. For example, the complexity of current software is many orders of magnitude higher than the most complex automated hardware design; the physical properties of hardware are abstracted out of most software design; software is designed based on a finite but unbounded randomly accessible space, while hardware is designed based on a relatively small finite and bounded space with only local access as provided by explicitly created wires. Furthermore, hardware design automation takes substantial amounts of computer time, still leaves design flaws such as data dependencies that have resulted in disruption, and is based on specifications that are vulnerable to errors.\n\nAnother alternative is the use of extremely intensive testing to detect the presence of errors and correct them. The problem with this approach is that testing for 100 percent coverage is as complex as perfect design. Imperfect testing leaves systems that fail when `rare' events occur. In one study, the combination of two events characterized as low probability caused 50 percent of systematically designed, well tested, small control programs to fail. [Hecht93] If this is the current state of the art for low probability events in small programs, extremes in testing are not likely to be successful against intentional attacks on large globally networked infrastructures.\n\nNote 3:\n\nGateways, terminal servers, and routers are commonly used to control traffic in networked environments, and they are quite effective against random or accidental misrouting of information, but in a hostile environment, they commonly fall prey to disruptive attacks. General purpose computers used as gateways are easily overwhelmed and corrupted. Terminal servers are commonly accessible by users logged into any computer in the network and can be altered to remove usage restrictions, connect users to wrong systems, or even lock out legitimate terminal server administrators. [Cohen93-3] Routers designed to control network traffic and prevent overloading in large networks are also easily bypassed by using the administrative mechanisms which permit remote control of the router or forgery of machine IDs with authorized access. [Cohen92]\n\nThe public telecommunications networks are a critical part of the current DII, and are likely to be a major component of the DII into the future, but they lack the information assurance features required for military operations. Service assurance features are designed into these systems at every level, [Falconer90] and yet they still fail to meet even the challenge of accidental errors and omissions. As an example, in 1991, there was a major failure in telephone switches in several large US cities that lasted for several days, and was finally traced to a 3 bit error (a `D' instead of a `6') in one byte of a software upgrade. [FCC91] This is the simple sort of mistake that even minimal software change control detects. This change was apparently never tested at all, was put into widespread use, and caused widespread harm. In 1990, AT&T's long distance network shut down due to a protocol error that impacted millions of customers nationwide for over 4 hours. [Alexander93]\n\nIn many cases, telecommunications disruptions must be resolved in very short timeframes. For example, some telephone switching systems must be repaired within 1.5 seconds or the circuit failure errors passing through the network will cause a propagating positive feedback which may deadlock more of the network, [Pekarske90] eventually cascading into a major problem. An attacker only needs to disrupt two sites for 1.5 seconds to cause such a cascading effect.\n\nOne quarterly report of large scale disruption incidents for the fall of 1993 includes an FAA (Federal Aviation Administration) computer systems failure delaying regional traffic for 90 minutes (cause unknown), an FAA weather computer system failure for 12 hours due to a time activated logic bomb, a programming error in an X-ray system that resulted in wrong dosages to about 1,045 cancer patients, and a software `bug' that crashed the Hamburg ISDN (Integrated Services Digital Network) telecommunications services for over 11 hours, and this is only one of several similar publications that report different incidents. [edpacs]\n\nSimilar lapses in policies and procedures seem to be common for major software manufacturers. As an example, in 1992, Novell released a virus to tens of thousands of customers when it was noticed after quality control was completed that a key file was missing from the master distribution disks then being transported to the disk duplication facility. Instead of returning to the quality control process, the person transporting the disks for duplication loaded the file from the most convenient computer, which by chance contained a virus that was transferred to the floppy disk. The disk was sent to duplication, packaged, and shipped to customers. [Lefkon92] The disks were apparently never tested at random after duplication for problems, the disks en-route to the duplication facility were not sealed or permanently write protected, the personnel were not properly trained, and the initial quality control process never detected that the correct file was not on the disk!\n\nNote 4:\n\nFive books on computer viruses, including two that are tutorials on writing viruses, discuss military use of this type of software. [Hoffman90] [Ludwig91] [McAfee89] [Burger88] [Ferbrache92] A recent popular novel has the central theme of crippling attacks on US computers by means of viruses, computer terminal eavesdropping, high energy radio frequency `guns', and electromagnetic pulses. The author's virus examples are not as subtle or malicious as a real attack by experts. [Schwartau91] An interactive movie on CD-ROM, released in October, 1993, illustrates information and infrastructure warfare against the US. It includes details about crippling and corrupting time standards, which affect precision weapon targeting and long distance telephone switches. [Xiphias]\n\nThe Chaos Computer Club in Germany, maintains an annotated list of the Internet addresses of US DoD command, control, supply, and logistics computers on one of their computer accounts in Germany. [Simon91] Apparently selected from hundreds of publicly available military computer Internet addresses, listed systems are primarily Army, Navy and Air Force logistics, computer, communications, and research sites. This listing is not kept in publicly available bulletin boards throughout the world, but access to it was attained via an international connection. To demonstrate one possible utility of this list in attacking the DII, during this study two simple, safe, legal, and well controlled experiments were performed.\n\nIn the first experiment, e-mai... (truncated)",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.8570812941,
        "format_confidence":0.8268706203
    },
    {
        "url":"https:\/\/www.tdi.ox.ac.uk\/publications\/994617",
        "text":"Cookies on this website\nWe use cookies to ensure that we give you the best experience on our website. If you click 'Continue' we'll assume that you are happy to receive all cookies and you won't see this message again. Click 'Find out more' for information on how to change your cookie settings.\n\nRapidly proliferating cells reshape their metabolism to satisfy their ever-lasting need for cellular building blocks. This phenomenon is exemplified in certain malignant conditions such as cancer but also during embryonic development when cells rely heavily on glycolytic metabolism to exploit its metabolic intermediates for biosynthetic processes. How cells reshape their metabolism is not fully understood. Here we report that loss of cathepsin L (Cts L) is associated with a fast proliferation rate and enhanced glycolytic metabolism that depend on lactate dehydrogenase A (LDHA) activity. Using mass spectrometry analysis of cells treated with a pan cathepsin inhibitor, we observed an increased abundance of proteins involved in central carbon metabolism. Further inspection of putative Cts L targets revealed an enrichment for glycolytic metabolism that was independently confirmed by metabolomic and biochemical analyses. Moreover, proteomic analysis of Cts L-knockout cells identified LDHA overexpression that was demonstrated to be a key metabolic junction in these cells. Lastly, we show that Cts L inhibition led to increased LDHA protein expression, suggesting a causal relationship between LDHA expression and function. In conclusion, we propose that Cts L regulates this metabolic circuit to keep cell division under control, suggesting the therapeutic potential of targeting this protein and its networks in cancer.\n\nOriginal publication\n\n\n\n\nJournal article\n\n\nMol Cell Proteomics\n\nPublication Date\n\n\n\nCathepsin L, Gene Expression*, Glycolysis, Metabolomics, Pathway Analysis, Proliferation, Proteases*, Proteolysis*",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9475551248,
        "format_confidence":0.9801018238
    },
    {
        "url":"https:\/\/www.nsi-mi.com\/library\/technical-papers\/2004-technical-papers\/423-implementation-of-a-geometric-error-correction-system-for-extremely-high-probe-position-accuracy-in-spherical-near-field-scanning",
        "text":"Implementation of a Geometric-Error Correction System for Extremely High Probe Position Accuracy in Spherical Near-Field Scanning\n\nAuthors:Scott Pierce, James Langston\nPublication: AMTA 2004\nCopyright Owner: NSI-MI Technologies\n\nIn this paper, we describe a new method for improving the true-position accuracy of a very large, spherical near-field measurement system. The mechanical positioning subsystem consists of 10-meter diameter, 180\u00b0 circular-arc scanner and an MI Technologies MI-51230 azimuth rotator and position controller.\n\nThe principle components of the error correction method are the error measurement system, the position correction algorithm, and a pair of very high precision, mechanical error correction stages. Using a tracking laser interferometer, error maps are constructed for radial, planar and elevation errors. A position correction algorithm utilizes these discrete-point error maps to generate error correction terms over the continuous range of the elevation axis. The small position correction motions required in the radial and planar directions are performed using the mechanical correction stages. Corrections to the position of the elevation axis are made using the primary elevation axis drive.\n\nResults are presented that show the geometry of the spherical scanning system before and after error correction. It is observed that the accuracy of the radial, planar and elevation axes can be significantly improved using the error correction system.\n\n\n\n1125 Satellite Blvd., Suite 100\nSuwanee, GA 30024 USA\n\n+1 678 475 8300\n+1 678 542 2601\n\nLos Angeles\n\n19730 Magellan Dr.\nTorrance, CA 90502 USA\n\n+1 310 525 7000\n+1 310 525 7100\n\n\nUnit 51 Harley Rd.\nSheffield, S11 9SE UK\n\n+44 7493 235224\n\n\nVirtual Conference Finding your local time... 25 Days\n\nLatest Tweets\n\nThis site is using cookies for analytical purposes and to provide a better user experience. Read our Privacy Policy for more information.",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9951812625,
        "format_confidence":0.8621684313
    },
    {
        "url":"https:\/\/jhu.pure.elsevier.com\/en\/publications\/p120-catenin-represses-transcriptional-activity-through-kaiso-in--6",
        "text":"P120 catenin represses transcriptional activity through Kaiso in endothelial cells\n\nJihang Zhang, James J. O'Donnell, Oksana Holian, Peter A. Vincent, Kwang S. Kim, Hazel Lum\n\nResearch output: Contribution to journalArticle\n\n\nP120 catenin (p120ctn) belongs to the family of Armadillo repeat-containing proteins, which are believed to have dual functions of cell-cell adhesion and transcriptional regulation. In vascular endothelium, p120ctn is mostly recognized for its cell-cell adhesion function through its ability to regulate VE-cadherin. The current study investigated whether p120ctn in endothelial cells also has the capability to signal transcription events. Examination of several endothelial cell types indicated that Kaiso, a p120ctn-binding transcription factor, was abundantly expressed, with a predominant localization to the perinuclear region. Immunoprecipitation of endothelial cell lysates with a p120ctn antibody resulted in p120ctn-Kaiso complex formation, confirming the interactions of the two proteins. Transfection of the KBS (Kaiso-binding sequence) luciferase reporter plasmid into endothelial cells resulted in a 40% lower reporter activity compared to the mutant Kaiso-insensitive construct or empty vector pGL3, indicating that the suppressed reporter activity was attributed to endogenous Kaiso. The knock-down of p120ctn increased the KBS reporter activity 2-fold over control, but had no effects on the mutant KBS reporter activity. Furthermore, p120ctn knock-down also reduced Kaiso expression, suggesting that p120ctn functioned to stabilize Kaiso. Overall, the findings provide evidence that in endothelial cells, p120ctn has a transcription repression function through regulation of Kaiso, possibly as a cofactor with the transcription factor.\n\nOriginal languageEnglish (US)\nPages (from-to)233-239\nNumber of pages7\nJournalMicrovascular Research\nIssue number2\nStatePublished - Sep 1 2010\n\n\n\n  \u2022 Armadillo repeat-containing proteins\n  \u2022 Endothelial cells\n  \u2022 Kaiso\n  \u2022 P120 catenin\n  \u2022 Transcription repression\n  \u2022 VE-cadherin\n\nASJC Scopus subject areas\n\n  \u2022 Biochemistry\n  \u2022 Cardiology and Cardiovascular Medicine\n  \u2022 Cell Biology\n\nCite this",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9362317324,
        "format_confidence":0.9848281741
    },
    {
        "url":"http:\/\/www.ingentaconnect.com\/content\/tandf\/grad\/2009\/00000164\/00000002\/art00001;jsessionid=4gjuqsaum8b80.x-ic-live-01",
        "text":"Skip to main content\n\nDensity functional theory - electron paramagnetic resonance study of gamma-irradiated single crystal of amphi-chloroglyoxime\n\nBuy Article:\n\n$55.00 + tax (Refund Policy)\n\nThe density functional theory was studied to examine the suggested type of model radicals RI, RII and RIII that have been formed upon gamma-irradiation of single crystals of amphi-chloroglyoxime (ACG). RI, RII and RIII type model radicals were obtained by abstraction of Cl and H atoms from ACG. The possible conformations of these model radicals were obtained using the semi-empirical Austin Model 1 method. Subsequently, hyperfine coupling constants and atomic spin density calculations of these conformations were performed using the Becke-3-Lee-Yang-Parr functional in combination with a split-valence Gaussian basis set. Theoretically calculated values were compared with the experimental values. Calculated isotropic and anisotropic hyperfine coupling constant values of conformations of model radical RII were in good agreement with experimental values. However, the agreement was rather poor in the case of the model radicals RI and RIII. Thus, the findings from the present study clearly suggest that the experimentally observed radical should be the model radical RII.\nNo Reference information available - sign in for access.\nNo Citation information available - sign in for access.\nNo Supplementary Data.\nNo Article Media\nNo Metrics\n\nKeywords: amphi-chloroglyoxime; density functional theory; electron paramagnetic resonance; isotropic and anisotropic hyperfine coupling constants; spin density\n\nDocument Type: Research Article\n\nAffiliations: Education Faculty, Department of Physics, Selcuk University, Konya, Turkey\n\nPublication date: 01 February 2009\n\n  \u2022 Access Key\n  \u2022 Free content\n  \u2022 Partial Free content\n  \u2022 New content\n  \u2022 Open access content\n  \u2022 Partial Open access content\n  \u2022 Subscribed content\n  \u2022 Partial Subscribed content\n  \u2022 Free trial content\nCookie Policy\nCookie Policy\nIngenta Connect website makes use of cookies so as to keep track of data that you have filled in. I am Happy with this Find out more",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9968428612,
        "format_confidence":0.6137991548
    },
    {
        "url":"http:\/\/nlo.stanford.edu\/content\/periodically-poled-lithium-niobate-modeling-fabrication-and-nonlinear-optical-performance",
        "text":"Periodically Poled Lithium Niobate: Modeling, Fabrication, and Nonlinear-Optical Performance\n\n\nPeriodically poled lithium niobate (PPLN) has become the nonlinear-optical material of choice in many infrared optical parametric oscillators (OPO's) due to its high nonlinearity, readily engineered tuning characteristics, and repeatable fabrication. The domain periods required for these OPO's are typically between 15 um and 30 um, and are made by a process already well developed. The domain periods required for visible light applications are typically between 3 um and 10 um, and are more difficult to fabricate repeatably. The focus of this research was to understand the nature of this difficulty, to develop strategies for repeatable device fabrication, and to demonstrate the nonlinear optical performance of the material.\n\nThis thesis presents a model of the periodic-poling process in lithium niobate, a repeatable PPLN-fabrication process developed using this model, and the nonlinear-optical performance of PPLN fabricated with this process. The model combines ferroelectric properties of the material, measured as part of this research, with the electrostatics of periodic electrodes. The fabrication process developed using the model was used to periodically pole two 75-mm-diameter wafers of 500-um-thick LiNbO3 with a 6.5 um domain period. These wafers yielded 53-mm-long samples, more than five times the length previously reported. Second harmonic generation experiments performed with these samples produced 2.7 watts of 532 nm radiation output with 6.5 watts of continuous-wave 1064 nm Nd:YAG laser radiation input, corresponding to a record 42% single-pass conversion efficiency. These samples were also used to operate the first reported continuous-wave 532-nm-pumped PPLN-based singly-resonant OPO, with a record-low 0.93-watt threshold (relative to other 532-nm-pumped OPO's), 56% conversion efficiency, and continuous tuning from 917-1266 nm. Finally, data are presented that indicate the presence of 532-nm-induced 1064-nm absorption.\n\n\nGregory David Miller\n\n\nJuly, 1998",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9925797582,
        "format_confidence":0.9863944054
    },
    {
        "url":"https:\/\/ablkinase.com\/2019\/07\/09\/innate-defense-regulators-idrs-are-artificial-immunomodulatory-versions-of-natural-host\/",
        "text":"Innate defense regulators (IDRs) are artificial immunomodulatory versions of natural host defense peptides (HDP). were observed. Our investigation demonstrates that in addition to previously reported immunomodulatory activities IDR-1018 promotes wound healing independent of direct antibacterial activity. Interestingly, these effects were not observed in diabetic wounds. It is anticipated that this wound healing activities of IDR-1018 can be attributed to modulation of host immune pathways that are suppressed in diabetic Omniscan irreversible inhibition wounds and provide further evidence of the multiple immunomodulatory activities of IDR-1018. Introduction Cutaneous wound repair is a specialized, multifactorial process that involves a large number Omniscan irreversible inhibition of factors that are involved in the regulation of this process [1]. Although initially characterized for their direct antimicrobial activities [2], [3], [4], there is an increasing appreciation for the immunomodulatory activities of cationic host defense peptides (HDPs; also termed antimicrobial peptides). HDPs are ubiquitous in nature and form central components of the innate immune system of eukaryotes. The immunomodulatory activities of such natural peptides, especially the human cathelicidin LL-37 and human -defensin (hBD) -3, are extremely diverse and include, but are not limited to, the Rabbit Polyclonal to HNRPLL stimulation of epithelial cell migration, Omniscan irreversible inhibition promotion of angiogenesis, and suppression of pro-inflammatory responses [3], [5], [6], [7], [8]. For instance, the individual cathelicidin peptide LL-37 draws in neutrophils, monocytes, mast cells, and T lymphocytes [9], [10], and in addition induces the creation of neutrophil and monocyte chemoattractants by many cell types [3], [11], [12], [13], [14]. Lately, HDPs are also implicated as regulators of cutaneous wound fix by regulating irritation, angiogenesis, and extracellular tissues deposition and redecorating [3], [15], [16], [17], [18]. The extremely different sequences and flexible antimicrobial and immunomodulatory actions of organic HDPs offers a wide variety of web templates for the look and advancement of artificial peptides with actions tailored for scientific applications [3]. Several peptides and peptidomimetic materials are undergoing clinical trials currently; nevertheless, most HDP scientific trials are targeted at mainly at topical ointment applications of peptides and so are predicated on their immediate microbicidal properties (evaluated in [3], [19], [20]). It\u2019s been shown the fact that impact of HDPs on wound fix is not reliant on antimicrobial function and a potential book clinical program for HDPs [21]. For instance, HB-107, a man made HDP produced from the antimicrobial cecropin B originally, was proven to promote wound recovery [21]. A fresh group of man made variations of HDPs, termed innate protection regulators (IDRs), which offer broad-spectrum security against systemic attacks with multidrug-resistant bacterias, have got been recently referred to [22], [23], [24], [25]. IDR-1 and IDR 1002 confer protection against microbial difficulties by enhancing innate immune defenses of the host while suppressing potentially harmful excessive inflammatory responses [22], [23], [24], [25]. Such selective enhancement of innate immunity represents a novel approach to anti-infective therapy with many advantages over directly microbicidal compounds [26], [27], [28]. Although many HDPs exert their direct antimicrobial activity through disruption of microbial membranes, IDRs appear to exert their immunomodulatory activities through non membrane-lytic mechanisms. Thus issues of hemolytic activity and cytotoxicity toward mammalian cells are minimized, as supported by the minimal toxicity of peptide IDR-1 compared with natural cathelicidins such as LL-37 [25], [28]. Despite the increasing desire for utilizing the immunomodulatory activities of HDPs and IDRs for clinical applications, investigations into optimizing and enhancing peptide immunomodulatory properties have thus far been limited. However, it is apparent that the ability to modulate chemotactic activity and chemokine induction are important aspects of the immunomodulatory activities shared by many natural HDPs. Recently, we screened a series of more than 100 distantly related peptide variants of bovine HDP Bac2A (unrelated by sequence to IDR-1, [22]) for improved immunomodulatory activity as assessed by chemokine production in human peripheral blood monocytic cells. The most active peptide observed was IDR-1018 (VRLIVAVRIWRR-NH2) which was shown to form an -helix in neutral membranes [29]. In the current work, we have expanded around the characterization of the immunomodulatory activities of IDR-1018 and exhibited that it also promotes wound healing activities. IDR-1018 promoted wound healing in normal and infected wounds, and its effects were favourably compared to those of occurring LL-37 and synthetic HB-107 as prospective positive controls naturally. Outcomes Ramifications of IDR-1018 on cell migration and viability To research of.",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.5804648995,
        "format_confidence":0.9781165123
    },
    {
        "url":"https:\/\/portalrecerca.uab.cat\/en\/publications\/analysis-of-bleomycin-and-cytosine-arabinoside-induced-chromosome",
        "text":"Analysis of bleomycin- and cytosine arabinoside-induced chromosome aberrations involving chromosomes 1 and 4 by painting FISH\n\nS. Puerto, J. Surrall\u00e9s, M.J. Ram\u00edrez, E. Carbonell, A. Creus, R. Marcos\n\nResearch output: Contribution to journalArticleResearch\n\n22 Citations (Scopus)\nOriginal languageEnglish\nPages (from-to)3-11\nJournalMutation Research. Genetic Toxicology and Environmental Mutagenesis\nIssue number439\nPublication statusPublished - 1 Jan 1999\n\nCite this",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.971480608,
        "format_confidence":0.8174466491
    },
    {
        "url":"http:\/\/mrsl.rice.edu\/papers\/massive-uniform-manipulation-controlling-large-populations-simple-robots-common-input-signal",
        "text":"You are here\n\nMassive Uniform Manipulation: Controlling Large Populations of Simple Robots with a Common Input Signal\n\n\nRoboticists, biologists, and chemists are now producing large populations of simple robots, but controlling large populations of robots with limited capabilities is difficult, due to communication and onboard-computation constraints. Direct human control of large populations seems even more challenging.\n\nIn this paper we investigate control of mobile robots that move in a 2D workspace using three different system models. We focus on a model that uses broadcast control inputs specified in the global reference frame.\n\nIn an obstacle-free workspace this system model is uncontrollable because it has only two controllable degrees of freedom---all robots receive the same inputs and move uniformly. We prove that adding a single obstacle can make the system controllable, for any number of robots. We provide a position control algorithm, and demonstrate through extensive testing with human subjects that many manipulation tasks can be reliably completed, even by novice users, under this system model, with performance benefits compared to the alternate models.\n\nWe compare the sensing, computation, communication, time, and bandwidth costs for all three system models. Results are validated with extensive simulations and hardware experiments using over 100 robots.\n\nAaron Becker and Golnaz Habibi and Justin Werfel and Michael Rubenstein and J. McLurkin\nIEEE\/RSJ International Conference on Intelligent Robots and Systems (IROS) (Accepted)",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9810036421,
        "format_confidence":0.980302453
    },
    {
        "url":"http:\/\/www.chem.uga.edu\/node\/7380",
        "text":"Seminar Series:\nMatthew BloodgoodUniversity of Georgia, Department of Chemistry\nMonday, March 27, 2017 - 11:15am\nChemistry Building, Room 400\n\nTransition metal chalcogenides are a class of van der Waals materials displaying interesting electronic properties that depend on both the transition metal and the chalcogenide involved as well as the observed polymorph. Transition metal dichalcogenides, MX2, are quasi-2D materials that can vary electronically from semiconductors to metallic-like conductors, with some displaying superconductivity and charge density wave behaviors. MX2 materials consist of X-M-X layers where the metal centers are in an octahedral or trigonal prismatic configuration.1-2 Transition metal trichalcogenides, MX3, exhibit lower dimensionality structurally as quasi-1D materials consisting of MX6 trigonal prismatic chains. They do, however, show similar electronic properties including metallic-like conductivity, superconductivity, and charge density waves. These properties depend on orientation of the MX chains in the unit cell which changes depending on the metal and chalcogenide.3-4\n\nFor future electronics applications, it is important to understand the synthesis, characterization, manipulation and properties of MX2 and MX3 materials at the nanoscale. This talk focuses on the oxidative behavior of two important MX2\u2019s, 1T-TiSe2 and 1T-TaSe2, as well as the synthesis and exfoliation behavior of a metallic MX3, TaSe3. The oxidation of 1T-TiSe2 and 1T-TaSe2 resulted in the onset of oxidation near 300 \u00b0C for both MX2\u2019s. Under a critical temperature, 400 \u00b0C for 1T-TiSe2 and 700 \u00b0C for 1T-TaSe2, a self-limiting, surface oxidation was observed, analogous to previous studies with WSe2.5 TaSe3 was also successfully synthesized and characterized with special attention paid to the chemical exfoliation behavior and crystallinity. Partial exfoliation was achieved leading to highly crystalline belts6-7 up to several microns in length and reaching a minimum width of ~5 nm.\n\n\n1.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Lieth, R. M. A.; Terhell, J. C. J. M., Transition Metal Dichalcogenides. In Preparation and Crystal Growth of Materials with Layered Structures, Lieth, R. M. A., Ed. Springer Netherlands: Dordrecht, 1977; pp 141-223.\n\n2.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Samnakay, R.; Wickramaratne, D.; Pope, T. R.; Lake, R. K.; Salguero, T. T.; Balandin, A. A., Zone-Folded Phonons and the Commensurate\u2013Incommensurate Charge-Density-Wave Transition in 1T-TaSe2 Thin Films. Nano Letters 2015, 15 (5), 2965-2973.\n\n3.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Furuseth, S.; Brattas, L.; Kjekshus, A., On the Crystal Structures of TiS3, ZrS3, ZrSe3, ZrTe3, HfS3 and HfSe3. Acta Chemica Scandinavica 1975, 29, 623.\n\n4.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Zybtsev, S. G.; Pokrovskii, V. Y.; Nasretdinova, V. F.; Zaitsev-Zotov, S. V., Growth, crystal structure and transport properties of quasi one-dimensional conductors NbS3. Physica B: Condensed Matter 2012, 407 (11), 1696-1699.\n\n5.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Liu, Y.; Tan, C.; Chou, H.; Nayak, A.; Wu, D.; Ghosh, R.; Chang, H.-Y.; Hao, Y.; Wang, X.; Kim, J.-S.; Piner, R.; Ruoff, R. S.; Akinwande, D.; Lai, K., Thermal Oxidation of WSe2 Nanosheets Adhered on SiO2\/Si Substrates. Nano Letters 2015, 15 (8), 4979-4984.\n\n6.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Stolyarov, M. A.; Liu, G.; Bloodgood, M. A.; Aytan, E.; Jiang, C.; Samnakay, R.; Salguero, T. T.; Nika, D. L.; Rumyantsev, S. L.; Shur, M. S.; Bozhilov, K. N.; Balandin, A. A., Breakdown current density in h-BN-capped quasi-1D TaSe3 metallic nanowires: prospects of interconnect applications. Nanoscale 2016, 8 (34), 15774-15782.\n\n7.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Liu, G.; Rumyantsev, S.; Bloodgood, M. A.; Salguero, T. T.; Shur, M.; Balandin, A. A., Low-Frequency Electronic Noise in Quasi-1D TaSe3 van der Waals Nanowires. Nano Letters 2017, 17 (1), 377-383.",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9947524071,
        "format_confidence":0.8156080842
    },
    {
        "url":"http:\/\/everything2.com\/title\/tacit+knowledge",
        "text":"What is tacit knowledge?\n\n\"Tacit knowledge\" is remarkably difficult to define and its meaning is often contested. Certainly it is something that is known but that is cannot be verbally articulated. It does not easily express itself in the rigid formality of language (especially when written), it is difficult to convey and, as such, is non communicable. The phrase \"tacit knowledge\" has been used in a broad variety of senses and, for the scope of this write up, it is necessary to distinguish what is meant by it. There are two main types of so-called tacit knowledge: things yet to be put into words (such as trade secrets or specific skills that have been overlooked as important or relevant) and things that are inherently inexpressible in words (relating to the senses such as the feeling of sufficiently kneaded bread or the timbre of a well-manufactured musical instrument). The first category has no insuperable barrier that prevents the knowledge from being made explicit and then exchanged. The knowledge has been concealed whether intentionally or not, but there is no reason why it should not be shared and communicated. This need not be tacit. However, the second stricter type includes things such as the open-textured nuances of expected social conduct and knowledge that can only be learned by familiarity. It is argued that this knowledge can only be transmitted by personal contact or experience and not via media such as books or manuals. Collins defines tacit scientific knowledge as \"knowledge or abilities that can be passed between scientists by personal contact but cannot be, or have not been, set out or passed on in formulae, diagrams, or verbal descriptions and instructions for action\" and it is this type that shall be the main topic of discussion in this write up.\n\nWhy is it tacit knowledge?\n\nWhy should some information be easily communicable while certain types of knowledge are formed of tacit rules that are impossible to formulate? I can transfer the first 740 digits of pi by simply storing the information as ink marks on a piece of paper and handing it over to someone. Anyone who has learned the rules for how to interpret decimal notation using Western numerals will have gained the knowledge without ever having to have met me. However, it is infeasible for me to write down the rules for learning to cycle for someone unable to ride a bicycle. There is no set formula that can be followed that will certainly result in a competent cyclist. Very few useful comments regarding technique can be written down and beginner cyclists find instructions to be of little use. There is no easy way to pass on the formula for riding a bike \u2013 the only way to successfully corner and balance at different speeds is through sufficient practice and experience. It is also interesting to note a characteristic of tacit knowledge \u2013 the only genuine test for it is in the doing itself. There is no real way for a cyclist to proof that they are capable of riding a bicycle without actually doing so. Likewise tacit knowledge about something is best demonstrated by using the actual knowledge.\n\nScientific tacit knowledge and the TEA laser\n\nThis idea is important in science because experimental science relies on the reproduction of results and corroborating data. Both modern and ancient experiments have always been complicated and fiddly - even those without technological complexities. For the independent duplication of results that is required to support theories, the knowledge needed to perform the experiment must be transferred between groups. Polanyi failed to make the unequivocal observation that the possession of tacit knowledge is not restricted to individuals. However, it is apparent that the difficulties in communicating knowledge between research groups is integral to scientific progress. Much of science is too difficult to organise coherently. Scientists often give the all-too-often-false impression that algorithmic instructions will be carried out identically and produce similar responses in nature. They assume that practical experiments are a formality when this is rarely the case. Collins states that \"all types of knowledge, however pure, consist, in part, of tacit rules that may be impossible to formulate in principle\" and has performed a lot of work studying the case of manufacturing TEA (Transverse Electrical discharge in gas at Atmospheric pressure) lasers.\n\nThe construction of the first TEA laser involved significant trial and error and apparent duplicates fail to work at all or only unpredictably and sporadically. Several teams methodically followed the instructions of the inventors only to find that, no matter how meticulous they were, they could not make a working model. However, when scientists and technicians joined the other sucessful teams they eventually helped them to construct functional lasers. Building a working laser appears to be impossible without the personal transfer of necessary tacit knowledge. Collins claims that no successful TEA laser|TEA lasers were produced using only published and other written sources. Laboratories constructed working lasers after organising the transfer of personnel for an extended period of time. No laser was successfully constructed relying only on knowledge or contact with someone who had not built their own functioning laser \u2013 it appears that the experience of assembling an operative TEA laser supplies the tacit knowledge required to build more lasers. Apart from reinvention, the only way for another group to make a TEA laser is by personal contact with someone with this tacit knowledge.\n\nThe idea of tacit knowledge also encompasses mismatched salience or the focus by different parties on different variables. Tacit knowledge is evident where teams can perform certain experiments but are unable to transmit that to others because they are unaware of the real reasons for their success. Some features of the experimental set-up were critical to its success but were regarded as marginal or routine, and hence were overlooked in reporting the results. Originally, the critical factors were unintentionally concealed but, when teams work together they can gradually learn what they are.\n\nDigression: tacit knowing\n\nIn his writing on the subject, Polanyi uses the phrase \"tacit knowing\" far more frequently than \"tacit knowledge\". There is a subtle but distinct difference between these two formulations. Tacit knowing is rarely denied because knowing is the process of learning something through experience and practise. However, tacit knowledge is a different matter \u2013 it is knowledge or information that can only be transferred tacitly. Learning the taste of a good pint is something that can only be tacitly learned but it can be argued that the knowledge required to brew a high quality beer is expressible without personal contact. Good brewers use tacit knowledge gleaned through years of experience but it may be technically possible to create precise instructions regarding the fermentation process so that a novice could produce an equally refreshing ale. However, the sheer volume of incredibly detailed information that needs to be transmitted is too large to make it practical. In addition, if knowledge is defined as \"warranted (and not merely logically justified) true belief\", then tacit knowledge must be warranted too. It is difficult to see how something can be warranted without first being made explicit. In any case, Polanyi was concerned with a process or activity rather than a type of knowledge and it is the tacit learning that is of greatest interest rather than the knowledge itself.\n\n\nIn science through the ages, the transfer of tacit knowledge has been essential to advancement and the development of successful theories. It is clear that meteorologists, for example, can predict the weather without applying algorithms to atmospheric readings and without necessarily being aware of or able to say how they did it. Even if they can to describe how they do it, their awareness does not have to be an analytically sophisticated one. Polanyi probably goes too far in suggesting that tacit knowledge allows people to anticipate the solution to problems, but it is certainly important. Tacit knowledge allows one to see the \"thousand varied and changing clues\" of a moving object \"jointly as one single unchanging object\" and recombine the gestalt. The fact that certain skills can only be transferred by physical meetings and cooperation means that the continuation of certain technologies is only possible when people or groups acting nodes with the tacit knowledge exist to spread the required information through the network of scientists.\n\n  \u2022 Collins, H.M. Changing Order: Replication and Induction in Scientific Practice (London, 1985).\n  \u2022 Collins, H.M. \"The TEA Set: Tacit Knowledge and Scientific Networks\", Science Studies, 4, pp. 165-86 (1974).\n  \u2022 MacKenzie, D.A. and Spinardi, G. \"Tacit Knowledge, Weapons Design, and the Uninvention of Nuclear Weapons\", American Journal of Sociology, 101, pp. 44-99 (1995).\n  \u2022 Polanyi, M. Personal Knowledge: Towards a Post-Critical Philosophy (London, 1958).\n\nLog in or registerto write something here or to contact authors.",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.8556578159,
        "format_confidence":0.7588658929
    },
    {
        "url":"https:\/\/eprints.utas.edu.au\/1844\/",
        "text":"Open Access Repository\n\nGeochemical Anatomy of Silica Iron Exhalites: Evidence for Hydrothermal Oxyanion Cycling in Response to Vent Fluid Redox and Thermal Evolution (Mt. Windsor Subprovince, Australia)\n\n\nDownloads per month over past year\n\nDavidson, GJ, Stolz, AJ and Eggins, SM 2001 , 'Geochemical Anatomy of Silica Iron Exhalites: Evidence for Hydrothermal Oxyanion Cycling in Response to Vent Fluid Redox and Thermal Evolution (Mt. Windsor Subprovince, Australia)' , Economic Geology, vol. 96, no. 5 , pp. 1201-1226 , doi: 10.2113\/96.5.1201.\n\n[img] PDF\nDavidson,_Stolz...pdf | Request a copy\nFull text restricted\nAvailable under University of Tasmania Standard License.\n\n\nIn the Cambro-Ordovician Mount Windsor subprovince, well known for its massive sulfide deposits, silica\niron oxide exhalites possess complex textural and geochemical features that provide an insight into the very\nearly stages of typical massive sulfide deposit development. In exploration they are also useful for identifying hotter systems most likely to host massive sulfide deposits. Three examples were mapped and sampled from outcrop and analyzed for magnetic susceptibility, major and trace elements, REE, and Nd and Sr radiogenic\nisotopes. They share a common evolutional history. Early microbially mediated silica iron oxyhydroxides (stage\n1), which grew with very little clastic sediment incorporation, probably developed an Fe, U, V, Mo, As, Ag, Cd, P, Y, Be, Mg, and REE element association that has also been documented from metalliferous sediments on the modern sea floor. This stage is commonly overprinted by siliceous veins (stage 2), indicating that the exhalites directly overlay diffuse hydrothermal upflow zones. Less commonly, the silicification assemblage includes pyrite. Y, U, Be, V, and Mg positive correlations with Fe survived the subsurface silicification. Ag, As, Mo, Sb, REE, and Ba were leached from stage 1 zones during stage 2, presumably liberated during recrystallization of iron oxyhydroxide and were reprecipitated in narrow crosscutting zones within stage 2 silicification.\nThe depositional mechanism is not well understood, but radiogenic isotope trends indicate that interaction between\nhydrothermal fluid and detrital silicates preferentially precipitated some of these metals. The hydrothermal\ntransition from low-temperature (less than 100 degrees C) oxidized to higher temperature ( more than 150 degrees C), H2S-bearing volcanic-hosted massive sulfide (VHMS)-style fluids in some systems is evidenced by the addition of Cu, Pb, Zn, Tl, Mn, Se, and possibly Eu, mainly as trace elements in pyrite.\nThe Sr and Nd isotope systematics of the jaspers can be explained for stage 1 by mixing of seawater, clastic,\nand hydrothermal end members, giving rise to complex isotopic populations. The stage 1 signatures are supplanted\nby relatively simple isotopic compositions with increasing stage 2 alteration intensity. This replacement\nis best expressed in plots of resistant detrital elements and metals such as As, Se, Zn, and Pb versus epsilon Nd and\n87Sr\/86Sri. The hydrothermal component has epsilon Nd(480 Ma) ~ -2, best explained by leaching of the underlying\nTrooper Creek Formation (epsilon Nd (480 Ma) = +3.8 to -7.3) rather than leaching of deeper Mount Windsor Formation\nrhyolitic volcanics (epsilon Nd (480 Ma) = -4.7 to -12.8). There is no support for a magmatic fluid, because no match\nexists with the known Trooper Creek Formation epsilon Nd(480 Ma) magmatic populations (epsilon Nd (480 Ma) = -4.1 to -7.3 and +3.8 to -0.9). The radiogenic isotopes support a shallow convecting model with jasper deposition from rockbuffered seawater. The evolution of fluids from cooler, oxidized to hotter, reduced conditions, either records\nheating induced by arrival of a subsurface thermal plume or the propagation of extensional faults deeper into a layered convective system.\n\nItem Type: Article\nAuthors\/Creators:Davidson, GJ and Stolz, AJ and Eggins, SM\nKeywords: exhalites, iron-silica chemical sediment, microbial, hydrothermal,volcanogenic massive sulfide mineralisation, Cambrian, inorganic geochemistry, radiogenic isotopes, chlorite-carbonate alteration, mineral exploration, Mount Windsor Volcanics, Queensland\nJournal or Publication Title: Economic Geology\nISSN: 0361-0128\nDOI \/ ID Number: 10.2113\/96.5.1201\nAdditional Information:\n\nThe definitive version is available online at\n\nItem Statistics: View statistics for this item\n\nActions (login required)\n\nItem Control Page Item Control Page",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9913731217,
        "format_confidence":0.9740841389
    },
    {
        "url":"http:\/\/opus.bath.ac.uk\/48220\/",
        "text":"Economic optimization of smart distribution networks considering real-time pricing\n\n\nZhang, H., Zhao, D., GU, C., LI, F. and Wang, B., 2014. Economic optimization of smart distribution networks considering real-time pricing. Journal of Modern Power Systems and Clean Energy, 2 (4), pp. 350-356.\n\nRelated documents:\n\nThis repository does not currently have the full-text of this item.\nYou may be able to access a copy if URLs are provided below. (Contact Author)\n\nOfficial URL:\n\nRelated URLs:\n\n\nWith the development of smart meters, a real-time pricing (RTP) demand response is becoming possible for households in distribution networks. The power flow can be bidirectional in distribution networks which become smarter with distributed generators (DGs). It is expensive to import electricity from the generation far from load centers because of the cost of power loss and network use, so that it is more economical to use electricity generated by local distributed generators. Therefore, in order to curtail operating costs of distribution networks, this paper proposes a model of economic optimization conducted by distribution network operators. The electricity purchasing costs for distribution network operators are minimized by optimizing electric power from transmission systems and local distributed generators. Further, based on price elasticity, the formulations of load demand considering RTP are proposed with economic optimization of distribution networks. The economic optimization problems are resolved by an interior point method. The case study shows that peak load demand can be reduced about 3.5% because the household RTP and electricity purchasing costs of distribution network operators can save 28.86 \u00a3 every hour.\n\n\nItem Type Articles\nCreatorsZhang, H., Zhao, D., GU, C., LI, F. and Wang, B.\nRelated URLs\nUncontrolled Keywordsdistribution network,economic operation,electricity cost,real-time pricing\nDepartmentsFaculty of Engineering & Design > Electronic & Electrical Engineering\nResearch CentresCentre for Sustainable Power Distribution\nEPSRC Centre for Doctoral Training in Statistical Mathematics (SAMBa)\nID Code48220\n\n\nActions (login required)\n\nView Item",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9416880608,
        "format_confidence":0.8805202246
    },
    {
        "url":"https:\/\/nosamsresearch.whoi.edu\/conduits-timing-and-processes-sediment-delivery-across-high-relief-continental-margin-continental",
        "text":"Conduits, timing and processes of sediment delivery across a high-relief continental margin: Continental shelf to basin in Late Quaternary, Gulf of Papua\n\nTitleConduits, timing and processes of sediment delivery across a high-relief continental margin: Continental shelf to basin in Late Quaternary, Gulf of Papua\nPublication TypeJournal Article\nYear of Publication2016\nAuthorsSeptama, E, Bentley, SJ, Droxler, AW\nJournalMarine and Petroleum Geology\nPagination447 - 462\n\nAbstract The Gulf of Papua (GoP), between Australia and Papua New Guinea, is the receiving basin for multiple substantial rivers draining southern Papua New Guinea with collective sediment discharge >> 220 million metric tons (Mt) per year, comparable to a continental-scale river, but draining a combined catchment area of only \u223c160,000\u00a0km2. This study of the deepest marginal basins in the Gulf of Papua was undertaken to build a regional late Quaternary lithofacies and stratigraphic framework to better understand processes, timing, and conduits of sediment delivery from terrestrial and shelf settings to deep marginal basins, using the GoP as a natural laboratory. Methods include observations of sediment-core stratigraphy and physical properties, accelerometer mass spectrometry (AMS) C-14 dates, core x-radiographs and thin sections. Six lithofacies across the deep water Gulf of Papua (GoP) are identified based on core visual and textural observation. Chronological constraints permit an assessment of changes in sediment supply and depositional environments across time and space, from marine isotope stage (MIS) -3 to -1, or in the last 40\u00a0cal ka. The sediment delivery to the deep water GoP is dominated by two mechanisms, gravity-driven flows down slopes and into deep sea basin primarily during lowstands in the western portions of the study area, and hemipelagic sediment accumulation during transgression and highstand. Although the sediment flux appears to be overall dominated by sediment-gravity flows, hemipelagic sediment delivery is widespread during periods of sea level highstand. In the eastern portions of the study area, off-shelf sediment delivery continued into the Holocene in sufficient local volumes to produce turbidity currents. This late, localized sediment delivery appears to have been facilitated by oceanographic processes that allowed seaward sediment transport after flooding of the shelf. A simple sediment budget comparing basinal sediment accumulation to modern estimated river-sediment discharge indicates that peak sediment accumulation in proximal basins occurred during MIS-2; and declined thereafter, generally shifting to upper slope locations, except for the eastern margin of Moresby Trough. There, turbidite deposition continued until 7.4\u00a0cal ka, well after drowning of the shelf edge. This continued Holocene deep-sea sediment delivery is likely explained by the local narrow shelf width, and the presence of oceanographic processes capable of transporting sediments from shore to shelf edge.",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9765630364,
        "format_confidence":0.9700275064
    },
    {
        "url":"https:\/\/ir.library.louisville.edu\/etd\/1127\/",
        "text":"Date on Master's Thesis\/Doctoral Dissertation\n\n\nDocument Type\n\nDoctoral Dissertation\n\nDegree Name\n\nPh. D.\n\n\nElectrical and Computer Engineering\n\nCommittee Chair\n\nMcNamara, Shamus\n\nAuthor's Keywords\n\nThermal transpiration; Molecular flow; Gas micropump; Peltier coder; Knudsen pump; Nanoporous thermoelectric\n\n\nFuel pumps\n\n\nThe thesis focuses on improving the flowrate of the Knudsen gas pump. The Knudsen pump uses thermal transpiration as the driving mechanism to pump gas. It is a motionless gas pump as the pump does not require any moving actuators for pumping. The thermally driven gas flow is accomplished in the molecular or transitional gas flow regime. The advantage of this pump is that without any moving parts it avoids friction losses and stiction problems which devices in micro scale are prone to suffering due to scaling issues. Thus, this pump is highly robust and reliable. Knudsen pumps in the past have suffered from the drawback of low flowrates and inability to operate at atmospheric pressure. In the early days lack of micromachining technologies limited minimum channel size which had to be operated at lower than atmospheric pressure to achieve free molecular flow. Various designs have been implemented with an impetus on increasing the flowrate of the pump. The key to this pump is establishing a temperature difference along the length of the channel. A higher temperature difference over a shorter channel length makes the pump more efficient. Pump channels have been made out of various materials like silicon, glass and polymer. The silicon microfabricated single channel conventional design pump suffered from the high thermal conductivity of silicon, which limited the thermal gradient that could be achieved. Silicon was replaced by glass, which has a lower thermal conductivity. The glass micro fluidic pump could pump water in reservoirs but at a slow rate. Renewable forms of Knudsen pump were also made by using nanoporous silica colloidal crystals which are robust and could use solar energy and body heat to create a temperature difference and achieve pumping. The pump powered by body heat produced a maximum pressure differential of 1.5 kPa. However, the use of these pumps is restricted to certain applications due to slow pumping. The polymer material, made of mixed cellulose ester, has a very low thermal conductivity, which aids in maintaining a higher temperature difference between the ends of a channel to achieve a higher flowrate. The polymer material used is in the form of a nanoporous template which has numerous pores each of which acts as a pump and thus the pump's conductance to gas flow is also increased which makes it faster. The pore sizes range from 25 nm to 1200 nm. It has been proven that a smaller channel diameter pump is more efficient. Efficiency decreases as the channel size approaches viscous flow regime. The initial design used a resistive heater to actively heat one end of the channel and a heat sink was used to passively cool the other end of the channel. This design was ineffective in achieving a significant temperature difference for a decent flowrate with the materials like silicon and glass. The conventional Knudsen pump design using a porous polymer matrix as channel material attained a normalized maximum no load flowrate of 135 \u00b5L\/min-cm2 at 3.81 Watts of input power. This number is low compared to other micropumps. This led to the use of a thermoelectric material, which could actively heat and cool the pump channel ends and provide a much higher temperature difference over the same channel length as compared to the conventional Knudsen pumps which used only active heating of the channel's hot side. The thermoelectric strategy also eliminates the need for a heat sink in the pump. This transforms the design to bi-directional modes of operation. The first design using thermoelectrics is a lateral design in which the pump channels closer to the thermoelectric element developed a higher temperature difference across them compared to the channels away from the thermoelectric element. Thus, the thermoelectric energy was underutilized. Changing to the radial design made the pump more efficient compared to the lateral design since the thermoelectric energy was uniformly distributed on all the pump channels. The radial design also reduced air gap resistances and minimized energy losses which enhanced the output for the same input power. At an input power of 4.18 Watts it achieved a normalized no load flowrate of 408 \u00b5L\/min-cm2. It also recorded a maximum normalized flowrate of 1.5 mL\/min-cm2 while moving a drop of water which to date is the maximum flowrate reported by any Knudsen pump. A theoretical model has been developed to compute the pump's efficiency based on the flowrate and pressure difference obtained by the pump. The efficiency of the radial design pump with the thermoelectric is higher when compared to a conventional pump using a resistive heater whose channels are also made from the same material as that of the thermoelectric pump.",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.9919885993,
        "format_confidence":0.9892689586
    },
    {
        "url":"https:\/\/journals.ashs.org\/horttech\/view\/journals\/horttech\/25\/4\/article-p437.xml",
        "text":"A Review of Organic Lawn Care Practices and Policies in North America and the Implications of Lawn Plant Diversity and Insect Pest Management\n\nin HortTechnology\nView More View Less\n  \u2022 1 1Department of Entomology, North Carolina State University, 1110D Grinnells Hall, Raleigh, NC 27695\n  \u2022 | 2 2Department of Horticultural Science, North Carolina State University, 128 Kilgore Hall, Raleigh, NC 27695\n  \u2022 | 3 3Department of Forestry and Environmental Resources, North Carolina State University, Box 7646, Turner House, Raleigh, NC 27695\n\nThere are \u224840 million acres of turfgrass lawns throughout the United States, most of which are managed under chemical-intensive pest and fertilizer programs. \u201cOrganic lawn care\u201d is being adopted more widely; however, unlike the formally defined policies and regulations that govern organic agriculture, the label organic lawn management has not been formally defined and is used to describe a variety of practices. Neighborhoods, cities, states, and provinces across North America are adopting policies regulating the use of pesticides and fertilizers in the landscape. In addition, a small but growing number of public institutions and individual consumers are successfully adopting alternative lawn care methods, including organic lawn care. Although perceived as environmentally friendly, the effects of organic management on insect diversity and pest management remain understudied. Organic lawn management may lead to increased lawn plant diversity, which in agroecosystems has enhanced ecological services provided by beneficial insect species. Effects of vegetative diversity on lawn pest management are less clear. Vegetative complexity and increased plant diversity in urban landscapes may enhance insect predator efficacy. The diversity of predatory insects varies between turfgrass varieties in response to prey populations. Mortality of insectivorous and granivorous ground beetles (Carabidae) while not directly impacted by pest management programs in turfgrass may be indirectly impacted by a reduction in the prevalence of plant species that provide alternative food resources. Previous studies have focused on herbivorous insects as well as predatory and parasitic insects that feed on them. Future studies should assess how lawn plant diversity resulting from organic management practices might impact insect communities in turfgrass.\n\n\nThere are \u224840 million acres of turfgrass lawns throughout the United States, most of which are managed under chemical-intensive pest and fertilizer programs. \u201cOrganic lawn care\u201d is being adopted more widely; however, unlike the formally defined policies and regulations that govern organic agriculture, the label organic lawn management has not been formally defined and is used to describe a variety of practices. Neighborhoods, cities, states, and provinces across North America are adopting policies regulating the use of pesticides and fertilizers in the landscape. In addition, a small but growing number of public institutions and individual consumers are successfully adopting alternative lawn care methods, including organic lawn care. Although perceived as environmentally friendly, the effects of organic management on insect diversity and pest management remain understudied. Organic lawn management may lead to increased lawn plant diversity, which in agroecosystems has enhanced ecological services provided by beneficial insect species. Effects of vegetative diversity on lawn pest management are less clear. Vegetative complexity and increased plant diversity in urban landscapes may enhance insect predator efficacy. The diversity of predatory insects varies between turfgrass varieties in response to prey populations. Mortality of insectivorous and granivorous ground beetles (Carabidae) while not directly impacted by pest management programs in turfgrass may be indirectly impacted by a reduction in the prevalence of plant species that provide alternative food resources. Previous studies have focused on herbivorous insects as well as predatory and parasitic insects that feed on them. Future studies should assess how lawn plant diversity resulting from organic management practices might impact insect communities in turfgrass.\n\nKeywords: turfgrass; landscape\n\nAs a result of the economic boom in the mid-20th century, the middle-class changed its way of life, increasing urban sprawl due to movement away from city centers (Robbins and Sharp, 2003a). From the mid-1950s to 1986, almost 69 million acres of natural habitat was converted to urban or suburban areas (Grey and Deneke, 1986). From 2000 to 2010, urban (urban + suburban) area in the United States increased by 12% each year (U.S. Census Bureau, 2012). In 2012, over 80% of the U.S. population lived in urban areas (U.S. Census Bureau, 2012). Construction of homes and buildings often leads to a loss of natural vegetation (Blubaugh et al., 2011), which is often replaced with turfgrass systems (Tallamy, 2007). Public policies and local housing regulations impose guidelines to maintain aesthetic property values, which often influence homeowners\u2019 needs to maintain lawns (Jenkins, 1994). However, arguably the most influential is the social pressure from neighbors who often ensure enforcement of lawn regulations, to which homeowners often conform (Byrne, 2005).\n\nThere are an estimated 40 million acres of nonnative grass lawns in the United States, covering almost 2% of land, making it the largest irrigated monoculture plant system in the country (Milesi et al., 2005; Tallamy, 2007). Lawns are often high-input systems, requiring significant amounts of time, monetary, and chemical investments to maintain aesthetic property value (Robbins et al., 2001). The prevalence of turf cover throughout the United States and a low tolerance for weed and insect pests coincides with an increase in the use of synthetic chemicals (Alumai et al., 2008). Conventional lawn management strategies are calendar-based applications of synthetic chemical inputs of fertilizers, herbicides, and insecticides (Alumai et al., 2008; Bormann et al., 1993) and have been associated with environmental pollution and human health risks [Robbins and Sharp, 2003a; U.S. Geological Survey (USGS), 1999]. As a result, strategies using integrated pest management (IPM) or \u201corganic\u201d practices have garnered increased attention as management alternatives (Alumai et al., 2008).\n\nSome individual consumers and public institutions have adopted alternative lawn management strategies, which have been promoted as a way to prevent potential negative environmental consequences of the overuse or misuse of pesticides (Henderson et al., 1998). The growing interest in lawn management alternatives has led to changes in public policies which ban the use of cosmetic pesticide applications and favor the adoption of lawn alternatives (Vickers, 2006), including organically managed green spaces (Alumai et al., 2008).\n\nAlthough organic management programs are offered as an alternative to traditional lawn care, their impact on arthropod diversity and pest management is poorly understood. A goal of commercial IPM or organic management is to enhance the overall aesthetic quality of lawns, which includes suppression of pests that invade turf (Alumai et al., 2008). Since little is known about the effects of organic lawn management on arthropod communities, this article reviews literature pertinent to organic lawn care, highlighting studies that have assessed the effects of turf variety and management practices on insect communities. To put this information into context, the article begins with an assessment of what organic lawn management means and the history of public policy regarding its implementation in North America.\n\nDefining Organic Lawn Management\n\nThe U.S. Department of Agriculture (USDA) defines organic agriculture as \u201c\u2026 an ecological production management system that promotes and enhances biodiversity, biological cycles, and soil biological activity. It is based on minimal use of off-farm inputs and on management practices that restore, maintain, and enhance ecological harmony\u201d (USDA, 1995). The USDA National Organic Program (NOP) administers and publishes organic regulations with input from the public and the National Organic Standards Board. Before being certified as organic, farms must undergo a 3-year transition period in which organic production practices are followed, and documentation must be provided that no unapproved products or practices were used in the production system. Producers must then submit an application and fees to a USDA-accredited certifying agent (state departments of agriculture or private organizations). After an on-site inspection and review of the application indicate the producer is in compliance with USDA organic regulations, the certifying agent will issue an organic certificate. Producers are required to provide annual updates to the certifying agent, who performs an annual inspection to ensure USDA regulations are still being met.\n\nExcept for cases where turf is grown commercially for sod or seed production, lawns are not agricultural commodities and so their management is not regulated other than by local municipal and homeowner association aesthetic rules. Because there has been no groundswell of support for consistent definitions of organic lawn management and accompanying regulations like for agricultural commodities, there is no national definition or regulation of organic lawn care. This article refers to organic lawn management with the understanding that there are many varying definitions.\n\nRelevant Public Policy in North America\n\nGrassroots organizations throughout North America are pushing institutions as well as local and regional governments to restrict the use of pesticides and adopt organic or similar management policies for public spaces. These changes are being made primarily because of perceived public health concerns, even though the conse... (truncated)",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.8808879852,
        "format_confidence":0.97637254
    }
]