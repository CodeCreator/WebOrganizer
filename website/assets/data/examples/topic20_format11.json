[
    {
        "url":"https:\/\/www.reliableplant.com\/Read\/31316\/back-up-champion",
        "text":"Back up Your Champion, Not Just Your Database\n\nMany organizations today utilize a software program such as a computerized maintenance management system (CMMS) to help control important aspects of their business. It\u2019s no surprise that information technology (IT) departments encourage backing up these critical databases, as this protects data and allows for continued operation of the programs. However, fewer companies encourage cross-training multiple users with full administrative access of these programs. The process of \u201cbacking up\u201d your champions is equally as important as backing up your database.\n\nLevels of Software Users\n\nIf your organization is running software programs that are in any way related to finance, historical records or feature-rich data, it\u2019s likely your IT department mandates the backing up of those databases. This process protects the integrity of the data and will enable continued and viable operation of the program over many years. Having a backup ensures your data is safe in the event of a hardware failure, fire, earthquake, flood, data breach, corruption due to power outages and other potential incidents.\n\nMany software programs provide various levels of access or permissions for users, including the most comprehensive access commonly referred to as \u201cchampion\u201d or administrator user status. While organizations may provide software training for multiple employees, typically only one or two employees are selected for comprehensive champion training.\n\nEmployee Circumstances Affect Operation\n\nChampions are critical links in the organization. They have control and institutional knowledge of how the software functions. However, it\u2019s the job of the champion\u2019s manager to ensure critical links aren\u2019t broken in the face of changing circumstances, such as someone calls in sick, technology fails, there is a temporary reduction in workforce, etc. Expected or unexpected, these common circumstances can prevent departments from running critical software, maintaining data appropriately and providing program support.\n\nImportance of Cross-training\n\nRedundant positions within organizations offer some degree of flexibility, but redundancy may not apply to champions. Therefore, departments should increase their flexibility through cross-training. This also enables others to experience new and more challenging work responsibilities. Department managers should allow cross-trained employees the opportunity to fill in while champions go on vacation or take on additional job duties.\n\nInsist your champion record his or her basic job responsibilities in addition to each of those responsibilities typically in the \u201call other duties as assigned\u201d category. Ensure the champion be granular in these writings, providing important details. Doing so will provide not only a detailed roadmap for others to follow in order to keep the organization running efficiently but also an opportunity for those hoping to gain knowledge or advance their careers.\n\nAs a manager, your first order of business is to direct your champions to make themselves dispensable. As a champion, you should ensure one person on staff is fully capable of performing your job responsibilities in total, especially as it applies to software knowledge.\n\nPlan Ahead\n\nDuring my maintenance career, I have witnessed situations in which organizations failed to back up many critical managers. Higher level managers often overlook early warnings of coming promotions or retirements, and consequently scramble to adapt to these situations. In some cases, this prompts an emergency contact with the software developer for immediate training. In other instances, the original software program may be abandoned altogether while a new program is implemented.\n\nConsequences like these can result in considerable financial expense, as new programs, training and other factors will carry significant costs. This does not include the lost return on investment (ROI) from the abandoned software, all of which could have been avoided by simply backing up your champion.\n\nAbout the Author\n\nRobert Brieck is a professional services consultant for DPSI, a CMMS provider. He has more than 40 years of experience as a senior level maintenance administrator at a number of organizations, most recently with the Community College of Allegheny County. He has also worked as an instructor for both public and private institutions.\n\nSubscribe to Reliable Plant",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.6153324842,
        "format_confidence":0.6910794973
    },
    {
        "url":"http:\/\/casheer.net\/project-analytics-using-oracle-business-intelligence\/",
        "text":"Project Analytics Using Oracle Business Intelligence\n\nKnowledge is Power. This adage holds true now more than ever. Everyone is looking for fast and reliable information to help them make timely informed decisions. In the olden days information was filtered down to masses in a gradual manner. It took time for information, good or bad, to propagate.\n\nFast forward to today, the progress in information technology is nothing short of a revolution. People are exposed to the 24 Hour Cycle. It takes only a few minutes for an event occurring in a particular part of the world to appear on TV screens and Web Pages across the World.\n\n'Analytics' has been defined as the 'science of analysis'. In the Corporate World, Information has itself become 'commoditized'. Big bucks are spent to gather and 'buy' Information. Fast and reliable information is scarce. Insight out of the information is even scarcer. Big retail firms spend millions on market research of not only their own products but those of their competitors as well. The research firms gather the data and present their results back to the Managers to 'Analyze' and act upon. Crude oil and oil futures are traded publicly. The daily output from OPEC is limited and is public knowledge. The Refining capacity for each country is generally flat. So why does an investor pay more for a future contract of Crude Oil than others? Maybe he knows more than the other investors, or maybe even less. A number of Energy Analysts look at all sorts of data related to the Production, Transportation, Refining and Consumption of Oil. They look at everything from delays in shipping lanes and unrest in Oil producing regions to weather patterns around rigs and consumption trends. Their job is to gather, refine and deliver the data to the investors to help them make an intelligent and informed decision in a timely manner.\n\nThe decision makers do not needarily want all the minute details. What they need is the 'Analytics' support provided by the Analysts. They need the insight from all the data and information. It is this fast and accurate access to this action insight that helps companies and investors out maneuver and beat their competitors.\n\nBusiness leaders have started to realize the benefits of automated software based business analytics in gathering information on their operations, production and transactional activities. Businesses are deploying Analytics applications that automate the information gathering and presentation. There has been increasing demand for Analytics support in a number of additional areas. One of the areas is Project Management.\n\nA number of Enterprise resource planning 'ERP' Systems offer applications related to Project Management. Each ERP system offers a useful of tools in their Project Management application that differentiate them from each other. An increasing number of mid-size to large and even some small companies are deploying Project Management Applications.\n\nERP systems at the core help the company manage their assets, resources and most importantly their finances. As Project Management evolved and standardized in the last decade thanks to professionals and organizations like the PMI, the Industry realized the benefits of managing the progress and budgets of their individual projects separately from their production \/ day to day management.\n\nThe Project Management modules in the ERP systems provide the functionality to itemize, track and manage the Project related transactions for the company. As the Project Management module is tied to the Financials Module, the Project Manager can easily manage the Funding, Invoices, Journals, Revenue recognition and the costs associated with each individual project. The reporting and analytics capacities in these Business Applications are however limited.\n\nOracle is the first major software company to release a comprehensive Project Analytics suite. They have released Project Analytics as part of their Oracle Business Intelligence Enterprise Edition 7.9.6 Release. The Project Analytics Module sources raw Project Data from Oracle EBS and PeopleSoft Enterprise Project Management Applications. It extracts and refines the data into reports and presents the refined data on the Analytics Dashboard. The web based Dashboards provide graphical and tabular representation of customizable data. Oracle Project Analytics displays Project reports related to Project cost, revenue, budget and billing among other things.\n\nIn Today's recessionary environment the CIOs and CEOs are actively looking at ways of reducing costs and maximizing the revenue with limited resources. Oracle Project Analytics provides essential project based metrics on web based dashboards. The information is clearly laid out and gives fast fact based visibility into the Project Performance.\n\nAnyone having access to the Dashboard, from Developers to Project Managers to the CEO can look at the insight provided by Oracle Project Analytics and act quickly and decisively to adjust resources, reduce costs and increase profitability. This comes with a prebuilt Dashboard and Data Warehouse, but the users have the ability to create custom reports and metrics in the Dashboard.\n\nThe Dashboards have role based access. The Administrators have the ability to grant access to users based on their roles within the organization. The level of access can also be controlled. A Project member can have Read access only while the Project Manager may have full edit access to the same report or dashboard.\n\nAfter login the CEO may be taken to the Executive Dashboard with high level metrics for active Projects across his Organization. The Executive can go through the metrics, quickly identify the areas needing his attention and drill down to the detail level for further information.\n\nOther individuals may be denied access to the Executive Dashboard and taken to their customized dashboard. This is extremely important because it gives the users only the information that they need very quickly. The managers do not have to sift through data and turn over pages to get what they want. The Project manager can quickly identify and address the cost variance issue in the project while the Organization Director is in a better position to move resources between projects after going through the Dashboard metrics. It helps the user make informed decisions in a timely manner.\n\nTypically the Project financial data and status is entered into the Project Management ERP Applications by Project Managers, project team members and accountants. Working with these applications requires a level of expertise that a typical management team does not have. These applications provide a good mechanism of project management but lack the reporting flexibility. Reports are generally created despite the ERP applications at the Project or Organization level. Management feeds these numbers in their spreadsheets for management review. There are a few problems with this approach. Firstly there is quite a bit of manual work and calculation required which may be preceded to human errors. Secondly by the time the report filters up to the Executives it might already be too late to act.\n\nSince the warehouse behind the dashboard is sourced from the same ERP application that the Project Team enters data into, the executives can access their favorite reports on the dashboard promptly and not worry about the accuracy of the data. This is the fundamental benefit of the fast fact based analytics.\n\nThe Project Analytics data is currently sourced from Oracle E-Business Suite and the Oracle's PeopleSoft Enterprise ERP Applications. A universal adapter is also provided which can be used by administrators to fetch and display Project data from other Source Systems.\n\nThe data in Project Analytics is tightly coupled with General Ledger and Financials data from the Source System. The users can view the Key Progress Indicators for the Projects against the General Ledger, Accounts receivable and Accounts Payables data.\n\nOne big incentive to the Managers is the implementation and maintenance cost associated with Oracle Project analytics. The Warehouse and Dashboard associated with Oracle Project Analytics is tightly integrated with existing Oracle EBS and PeopleSoft Enterprise Applications. The Warehouse is supported on multiple Databases and can be hosted on an existing Databases Server having sufficient space and memory.\n\nThe setup and administration is not overly complicated. Anyone having Database administration expertise and some web administration experience should be able to manage the administration. This helps keep the training costs in check while also leveraging the skills of the existing workforce.\n\nCreating the custom reports and graphs using the Dashboard looks intuitive. The primary users of the Analytics dashboards are Company managers with limited SQL skills. The Project Analytics dashboard was designed with this in mind. The users can create their own custom reports and graphs with relative ease. They save these metrics on their dashboard and even make them available for others to view.\n\nProject Analytics gives the Management team the ability to view the historical Project data. They can thus make better informed decisions and can browse the metrics for trending and forecasting.\n\nAlerts are another useful feature of the Oracle Business Intelligence Enterprise Edition that Project Analytics can leverage. The users can add custom alerts on metrics. Let's say that a Manager wants to be informed whenever a threshold on cost variance or unrealized Revenue has reached. The manager can set the value on the appropriate report and an Alert will show up at his next login after the alert condition is met. Furthermore, the Alert system can also be configured to send an email to the user whenever the alert condition is met. He can receive the alert notification on his personal computer or the handheld device and take the necessary action.\n\nAlthough... (truncated)",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.7832155228,
        "format_confidence":0.6583589315
    },
    {
        "url":"http:\/\/www.cmswire.com\/cms\/social-business\/intranet-search-more-than-just-a-search-box-023214.php",
        "text":"Social Business, Intranet Search Has to be More Than Just a Search Box\nI\u2019ve just come back from another highly successful KMWorld conference in Washington where over 900 people seemed more than happy to be entombed in the subterranean conference suites of the Renaissance Hotel for four days. With no specific intranet track this year, presentations on intranets popped up in the Knowledge Management (KM), SharePoint and Enterprise Search tracks.\n\nIn the conversations I had there with intranet managers, a common theme stood out: the under-appreciated value of search in intranets.\n\nMany of the delegates I spoke with who had intranet management responsibilities were looking forward to the imminent move of their intranet onto SharePoint 2013, but seemed not to have worked through the implications of moving either from SharePoint 2010 using FAST Search Server for SharePoint or the more radical search upgrade from the search application in the Standard CAL license. Search is much more than an add-in in SharePoint 2013 -- getting the best from the application requires a complete reassessment of the positioning and performance of search in the current version of the intranet.\n\nIn the case of non-SharePoint intranets, it still seems to me that the value of search is often not appreciated, a situation that also applies to a very large number of websites. The use of personas in the development of intranets is now widely adopted but rarely is there enough detail in a persona to provide a good enough basis for optimizing search performance and getting the best blend of browse and search.\n\nI thought it might be useful to review some of the best books and reports on intranet search so that intranet managers had a good range of resources available to them as they prepare to write their intranet development plan for 2014.\n\nDefining User Requirements\n\nThe chapter on search in Information Architecture for the Worldwide Web remains a masterpiece of writing and insight even though it was published in 1998. It is possible to download the chapter as a PDF.\u00a0Although the focus of the book and the chapter is on website architecture the basic principles hold for intranets.\n\nThe definitive book on search development is \"Designing the Search Experience\" by Tony Russell-Rose and Tyler Tate. This was published in late 2012 and without any doubt has to be on the digital bookshelf of any intranet or website manager. What both books emphasis is the need to take a considered view of search based on a sound knowledge of user's needs. Steve Krug\u2019s book \"The User is Always Right\" sets out how to develop personas but the emphasis is on website development -- as far as I am aware there is no equivalent book on intranet persona development. In an enterprise environment at least users can be interviewed and guidance on the process of interviewing is exceptionally well presented by Steve Portigal in \"Interviewing Users.\"\n\nThe Technology of Search\n\nI think that it can be of value to understand just how search works so that the opportunities and challenges can be more easily assessed. My own book on \"Enterprise Search\" has two chapters on search technology and Sue Feldman takes a deeper dive in \"The Answer Machine.\" A requirement to understand the technology of search is especially important in the case of SharePoint 2013. BA-Insight has published two excellent e-books on the subject, Agnes Molnar offers a training video and there is an interesting analysis of just how much FAST technology there is in SharePoint 2010 and 2013 by Marcus Johansson.\n\nSearch Metrics\n\nWhen it comes to search metrics there is one book that says it all -- \"Search Analytics for Your Site\" by Lou Rosenfeld. Although the primary focus is on websites, the book offers intranet managers a wealth of relevant advice. It is especially important to compare and contrast page click traffic and query results metrics so that the right balance is achieved between search and browse.\n\nSearch User Interface Design\n\nThere are a number of good resources on search interface design. The topic features in Designing the Search Experience and both Luke Wroblewski and Greg Nudelman have written books on how to build search into the mobile experience. The most comprehensive resource on intranet search is Volume 6 of the Intranet Usability Guidelines from the Nielsen Norman Group. This 181-page report contains 113 design recommendations based on their usability research and there are 140 screenshot illustrations to supplement the findings. The report also covers employee search.\u00a0A word of caution about this report: Some of the 42 case studies date back several years (there is one screen shot from 2005) and search technology has moved on quite significantly over the last few years.\n\nStart Searching!\n\nAlthough these resources will be of value, nothing replaces allocating time to conduct a range of searches and taking a user perspective on the outcomes.\n\nEditor's Note: Read more from Martin on the importance of search in Search as a Decision Support System",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.6426998377,
        "format_confidence":0.888972342
    },
    {
        "url":"http:\/\/www.dezzain.com\/search-engines\/the-future-of-web-surfing-seo-trends-from-expert-insiders\/",
        "text":"the-future-of-web-surfing-seo-trends-from-expert-insidersKeeping track in a constantly changing world of SEO is difficult. One moment you might be a master at one SEO strategy, the next moment that same technique will be obsolete. Experts and SEO practitioners are all predicting and practicing the new forms to best take advantage of new SEO methods.\n\nNew Mediums of Search\n\nTerms such as technical SEO have been being thrown around lately. As the rise of AI has conjoined with search it\u2019s important to know what content has been generated by machines.\u00a0 New forms of search are being put out there. YouTube is the second largest search engine, and that means video is one of the main avenues of search now.\n\nSavvy insiders are looking at these trends and noticing the different areas search is coming from now. It\u2019s no longer a simple text search, but much more as well. Video was first mentioned; another avenue is voice to text search. There are multiple digital assistants such as Siri, Cortana, and Alexa.\u00a0 People tend to speak differently into a search rather than type it.\n\nFor example if they were to type \u201clocal bars Chicago,\u201d they might instead search with their voice by saying, \u201cWhere\u2019s the closest Chicago bars?\u201d Businesses and other associated website owners need to take this change in semantics into account. Google has picked up on direct questions as well. If you were to ask a straightforward question it might respond with a dictionary link, or even a how-to guide on something.\n\nExperts\u2019 Take\n\nA few experts had a lot to say. One of the most important takeaways from all of them was content creation. They said this is an ever-lasting part of the trends that will never go away. Having stellar content is the number one way to go. Hiring quality writers and publishing regular blog posts, guides and videos are important.\n\nVideos need to post on YouTube that generates the second largest amount of search \u2013 this point was made multiple times. Initial search on video is huge and some people would rather watch video than read.\n\nOptimization is a word most are familiar with. The question is how to optimize. Keywords are still an important deal in SEO, but it\u2019s about optimizing for user intent so they are not disappointed when they get to your website. User experience and usability of your content is key. Links are important for building up a network. Google checks out the linkage back and forth and won\u2019t penalize a website for it.\n\nGaining an Edge\n\nOn top of running a business or taking care of a website, SEO can be another added duty that takes up a lot of time. But it is an important part of a website. That\u2019s why certain companies make it their business to assist. Chicago SEO experts keep their sights on an overall goal. It\u2019s not about just gaining traffic, but relevant traffic that is going to turn into something.\n\nTheir real time reporting shows keyword position, in addition to that with real time analytics to show all kinds of statistics. By gaining the data they also interpret it for the website as well. On top of hiring a specialist, something to take note of is page load times. These are becoming more important as mobile search continues to increase, page loading time will factor into a website.\n\nAhead of the Curve\n\nIn order to stay ahead of the game and on top of trends it\u2019s good to know if people are Googling things related to your order of business. The best way to do this is through publishing content that makes you a top authority on a website. Google is taking that and applying the top people to the first pages of a search.\n\nInterrelated app SEO is also becoming important. Between Google\u2019s app streaming and exclusive content, these need to be factored into a multi-faceted SEO approach. This streaming content functionality helps users view content without having to download an app. This is another way to reach a larger audience.\n\nRiding the Trends\n\nOverall, to put the SEO trend in perspective is to look at first off creating amazing content that can reach a wide but targeted audience. Also looking out for new trends in the video sphere and applications is important to stay competitive. As new technologies come out, they shouldn\u2019t be missed or you risk falling behind. Instead ride each ever changing trend to keep your SEO on top.\n\n\nLilly Wilson works as a SEO consultant and is a geeky tech girl at heart. At home Lilly enjoys DIY and pottering in her garden. Her articles appear on digital marketing, SEO and general business websites.\n\nRelated Articles:\n\nSearch Engines\n10 SEO Tricks You Will Need to Use in 2017\nSearch Engines\nThe End of Google Penguin 4.0\nSearch Engines\nTop 5 Benefits of Search Engine Optimization\nSearch Engines\nSEO After Penguin: How Have Things Changed?",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.8459903598,
        "format_confidence":0.8356562853
    },
    {
        "url":"https:\/\/baylorlariat.com\/2011\/11\/30\/viewpoint-the-lost-honour-of-dying-google\/",
        "text":"Viewpoint: \u201cThe Lost Honour\u201d of dying Google+\n\nBy Joshua Madden\nA&E Editor\n\nThink back to this past summer for a minute. I know it\u2019s hard to do with the weather finally starting to get chilly here in Waco, but reflect on those sunny days. The time spent lounging by the pool. The time when Google+ was supposed to be the Facebook killer.\n\nDoesn\u2019t that seem like forever ago?\n\nGoogle+, Google\u2019s most recent attempt at creating a viable social network, appears to be dead in the water. I was only even reminded of its existence because of an e-mail I got informing me that Google Wave, Google\u2019s previous half-hearted attempt at creating some kind of social experience online, was being shut down. The question I had to ask myself was this: How long until Google+ follows suit?\n\nWhen Google+ was first released, I\u2019ll admit it intrigued me. The Android app was solid. The invite-only feature made me feel special. The fact that it wasn\u2019t blocked everywhere with a proxy gave it a leg up over Facebook.\n\nThe problem, of course, was that it still wasn\u2019t Facebook. All of my friends were on Facebook. Even with Google+, 1,083 of my friends are still there. I don\u2019t even have 100 friends on Google+.\n\nNobel laureate Heinrich B\u00f6ll\u2019s \u201cThe Lost Honour of Katharina Blum\u201d follows the beautiful Katharina. Everything appears to be going her way until she falls in love with a murderer and quickly finds herself under investigation from the authorities.\n\nThe book is one of my all-time favorites, partially because of the fact that it speaks to something larger than the story contained within the pages. Who hasn\u2019t seen something beautiful (like Katharina) find itself in trouble because of a few careless mistakes?\n\nObviously there is some debate about whether or not Ms. Blum is actually to blame for her downfall \u2014 you\u2019ll have to read the book so you can decide for yourself \u00ad\u2014 but I personally thought that part of her problem is that she was too arrogant and thought too highly of herself. Could there be a more direct parallel to Google+ and its rapid decline in popularity?\n\nThe reason that no one shifted from Facebook to Google+ was that there was no reason to shift from Facebook to Google+. Google+ had no notable features that Facebook did not have and Google executives seemed to think that we should all move over simply because we love Google, even though many of those executives did not appear to embrace the site themselves.\n\nJust so we\u2019re clear, I do love Google. I think it\u2019s clearly the best search engine on the Internet and it makes it so easy to bring disparate things together. This was, however, the exact reason that I was never sold on Google+. All of the characteristic ease of use of Google Non-Plus didn\u2019t seem to make its way over to Google+. That\u2019s too bad.\n\nI write this column not to rejoice in the downfall of Google+, but to mourn it. There are serious privacy issues with Facebook that I\u2019m not convinced will ever be adequately addressed.\n\nI want Google+ to succeed and I think there is still a chance to make that happen. But it\u2019s going to take some real effort. They might start by taking their own \u201c+1\u201d concept to heart. Why can\u2019t Google+ one-up Facebook?\n\nFacebook is often slow to address issues that people would like to see addressed. Why is it that years after its initial implementation, the \u201cLike\u201d button on Facebook remains alone? Why is it that we don\u2019t have \u201cDislike\u201d buttons or \u201cLove\u201d buttons? Even Xanga had varied amounts of \u201ceProps\u201d that could be left for users.\n\nThese are easy areas that Google+ could have capitalized on, but instead, they chose only to give us a slightly more muddled version of the \u201cLike\u201d button.\n\nIf we\u2019re going to shift over to Google+, we need a reason to do so. The Google brand will get me to sign up, but it won\u2019t get me to stay there, especially when my 1,083 friends are staying on Facebook.\n\nThinking that you\u2019re more important than my 1,083 friends makes me think you and Katharina Blum might have something in common.\n\nPlease send comments to",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.9317442179,
        "format_confidence":0.7783569098
    },
    {
        "url":"http:\/\/stayclassyinternet.com\/articles\/so-lets-talk-about-multi-cloud.1\/",
        "text":"So Let's Talk About Multi Cloud\n\nBy Thomas Vachon\n\nMulti-Cloud: A myth or a practical reality?\n\nFor many businesses, the desire of being on one, two, or even three Cloud vendors is attractive to mitigate financial risks due to untenable price increases and business risks around availability of their critical systems. For other businesses, they want to provide a global reach to their customers where some providers excel in those regions while others do not perform within acceptable parameters.\n\nWhat is a Cloud?\n\nThe more pressing question, and frankly the harder one to answer, is \u201cWhat is a Cloud?\u201d. The Cloud can be many things to many people, from the simple systems administrator version of \u201csomeone else\u2019s computers in someone else\u2019s data center\u201d. If you ask a business analyst, they may tell you that Salesforce is a Cloud or possible Google\u2019s gSuite is a Cloud. You would be amiss if you argued that either of those points of view is either invalid or even misguided.\n\nSo, for the brevity of such an article, I will be talking about IaaS vendors going forward. Some of this will also apply to PaaS vendors, but we will not be focusing on SaaS vendors at all.\n\nExamining the Risks\n\nThe problem with most enterprises is they have largely decided to build systems in the IaaS Clouds much as they do on-premises. In some vendor offerings, such as vCloud Air or VMware on AWS, if you have the overlay networks and virtual SAN\u2019s in place on-premises today, that largely works. The problems start when you move to offerings which are not like for like from what you did in the past.\n\nFinancial Risk\n\nThis risk is by far the least understood risk by most technologists but one of the most important to the business. As part of the Cloud transition, costs largely are moved from Capital Expenditures (CapEx) to Operational Expenditures (OpEx). This important to understand as CapEx can be seen as a transfer of assets from cash to equipment under the Generally Accepted Accounting Practices (GAAP). As such, the company is not expending money, its transferring money into non-monetary instruments which devalue over time, a process known as amortization. As such, one day one, a million dollar mainframe didn\u2019t impact the balance sheet as a million dollars of cash gone out the door.\n\nThis all changes in the Cloud with some exceptions such as reservations. When companies spend money in the Cloud, its generally considered an OpEx usage by GAAP which means a dollar spent is a dollar not in the company\u2019s balance sheet.\n\nWhat does it mean to be Multi-Cloud\n\nWith the risks well understood, the question is now what does it actually mean to be Multi-Cloud. There are several camps of thought around this topic and I would summarize them as:\n\n  1. Syncing your static backups to another provider\n  2. Using vendor agnostic provisioning systems (e.g. Terraform, Puppet, Ansible, etc.) and having a copy of warm data in another provider\n  3. Actively running your app(s) across multiple Cloud vendors at once\n\nClearly these are cumulative, you cannot do #3 without #1 and #2. These are also presented in levels of increasing difficulty and complexity.\n\nI would argue, for about 70% of companies, you should just do #3 and have a runbook on how to use the second provider by hand if required mitigating a potential financial risk due to increased single provider costs. For most of that 70%, to ensure you have durability of your application, using multiple regions in your primary provider is the correct way to reduce business risk.\n\nIn part 2 of this series, I will investigate what you need, when you should do it, what to avoid, and what\u2019s the first step",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.7130066752,
        "format_confidence":0.8638093472
    },
    {
        "url":"http:\/\/www.cgw.com\/Print.aspx?Page=%2FPublications%2FCGW%2F2002%2FVolume-25-Issue-11-November-2002-%2FSpeaking-for-Scotland.aspx",
        "text":"Issue: Volume: 25 Issue: 11 (November 2002)\n\nSpeaking for Scotland\n\nJenny Donelan\nIn 1999, the year Scotland's Parliament met for the first time in three centuries, the Scottish Executive was created to run the country's day-to-day operations. The new government body\u2014technically Scotland's \"devolved\" government\u2014handles issues such as education, transport, law, and health (the UK Parliament in Westminster, London, still oversees defense and foreign affairs).\n\nA Web site helped the newly formed Scottish Executive share information on domestic issues, but the government also sought some more cutting-edge way to communicate. Taking inspiration from the well-known virtual newscaster Ananova, the Scottish Executive decided to create a digital spokesperson who would provide both a friendly, accessible interface and a progressive image that would reflect its pro-technology attitude.\n\nThat real-time animated spokesperson, Seonaid (pronounced SHOW-na), premiered on Scotland's Junior Executive site (www.scotland.gov.uk\/pages\/news\/junior). Says William Paul, the Scottish Executive's editor of Executive News Online, \"It was decided to do a youth site, which wasn't there before, with the idea being that young people are notoriously apathetic about government. We could have had just a straightforward list of stories, but we decided to do something a wee bit more exciting and have a virtual character read the news rather than try and get the kids to read it.\"\n\nSeonaid, the Scottish Executive's virtual spokesperson, is the first real-time digital character to appear on a government Web site. She was created by Digital Animations Group, who also developed virtual newsreader Ananova.\n\nThe Scottish Executive asked Glasgow-based Digital Animations Group (DAG), creator of Ananova, to develop a character who would appeal to the children of Scotland. \"We built a personality profile,\" says Mike Antliff, CEO of Digital Animations Group, \"that included a young but experienced individual who was well-educated, independent, and outgoing.\"\n\n\"She's female,\" says Paul, \"because the research shows that for reasons best known to themselves, children tend to trust female broadcasters more than they do males. She's also got a bit of a Celtic look to her.\" Although Seonaid is an acronym for Scottish Executive Online News and Information Distributor, it is also a familiar Gaelic name, he notes.\n\nLike Ananova (see \"Animated Anchors,\" pg. 27, June 2000), Seonaid was developed with a mix of commercial and proprietary technologies. Once artists at DAG had created her personality profile and developed an idea of how she should look, they modeled Seonaid in NewTek's LightWave. Her animation libraries are derived from LightWave, and also from Softimage XSI. For example, \"We created the dynamic animation libraries in XSI,\" says Antliff, \"and took them through our own pipeline tool, DA:Hugo, into our core engine.\" DAG's own text-driven animation editor, DAT:T2, creates content for Seonaid.\n\nUsing DAT:T2, editors at the Scottish Executive type in scripts for interviews or press releases, and Seonaid voices the words in real time. Editors can tag the scripts with gestures, sound effects, and music, and also drop in logos and background imagery. These sequences can then be edited with video footage, if needed. Seonaid has, for example, interviewed several cabinet ministers.\n\nHer modular design, says Antliff, enables easy incorporation of third-party programs, a capability exploited last summer when DAG used Rhetorical Systems' software to replace Seonaid's Ananova-style British accent with a West Coast Scottish one. The accent matches that of Seonaid's \"hometown\" in Scotland.\n\nPositive reaction to Seonaid on the Junior Executive site encouraged the Scottish Executive to employ her on its main site (www.scotland.gov.uk\/) as well, at about the same time that she received her Scottish accent. Wherever small icons of her face appear, users may access streaming video or audio versions of announcements and news stories.\n\nFuture possibilities for Seonaid include multilingual capability (she could also deliver the news in Gaelic, for example) and sign language. Says Antliff, \"There are also more sophisticated systems for a future date that will give her the ability to hear and to see using a combination of voice recognition and vision systems. We will also be able to equip her with an intelligent conversation engine so people can log on and ask questions of her and she'll be able to respond.\"\n\nSeonaid also works off-line occasionally, including a recent appearance at a museum exhibition. \"There's no reason that in the future you couldn't access her on interactive TV, a public access kiosk system, or handheld mobile devices,\" says Antliff.\n\nAt present, according to Paul, Seonaid's greatest obstacle is bandwidth. He estimates that only 14 percent of Scottish users have broadband access, and \"obviously she's better on broadband than over a phone link.\" He is encouraged, however, because more than half of Scottish schools now have broadband access. \"All schools have some kind of Internet connection, and as more people come online and she becomes more easily accessible, I think we'll get more and more visitors to the site.\"\n\nAlong with her new voice, Seonaid recently received some wardrobe upgrades and a pair of glasses. She's more mobile, and can now deliver the news from in front of or behind her desk. Her CV, posted on the Junior Executive Web site, lists her interests (conservation, computer games), marital status (single, one long-term partner, Callum), and other statistics. \"So as well as being a representative of the Scottish Executive, her whole identity is now Scottish, so that people can identify with her as a Scottish person\u2014if you want to call her a person,\" says Paul.\n\n\"We are claiming, and no on has yet countered it, that she's the only real-time virtual character based on a government site. I suppose you could say she's a bit of a gimmick at the moment, but I think she'll become a required part of the site now that she's there.\" ..\n\nJenny Donelan is Managing Editor of Computer Graphics World.\n\nDigital Animations Group www.digimania.com\nNewTek www.newtek.com\nRhetorical Systems www.rhetoricalsystems.com\nSoftimage www.softimage.com\nBack to Top\n\nPrinted from CGW.com\nComputer Graphics World Inc. | www.cgw.com | (800) 280-6446\n\nCopyright 2013",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.6826104522,
        "format_confidence":0.8741140962
    },
    {
        "url":"http:\/\/www.geekpreneur.com\/get-creative-with-linkedin-api",
        "text":"\ufeff Get Creative with LinkedIn\u2019s API | Geekpreneur\nhome\u00a0about\u00a0contact\u00a0internet marketing book\u00a0twitter business book\u00a0archives\u00a0subscribe\n\nGet Creative with LinkedIn\u2019s API\n\nOne of the most obvious differences between LinkedIn and its social media rivals, Facebook and Twitter, is its reach. It\u2019s not easy these days to wander onto a Web page that doesn\u2019t invite you to send a link to your Twitter followers or share the page with your Facebook friends. LinkedIn buttons? Not so much.\n\nThat might seem surprising. LinkedIn is popular both with users, who now number more than 100 million worldwide, and with investors. (The company\u2019s stock finished up 109 percent on its first day of trading recently, the fifth-largest opening rise since the bursting of the dotcom bubble.) But it also has a lot to do with LinkedIn\u2019s own slowness to make easy site integration available.\n\nThat, at least, is now changing. The relaunch of the site\u2019s developer platform with an open set of APIs and the adoption of OAuth has now made integrating LinkedIn easier than ever for developers. \u201cIn Share\u201d buttons are now beginning to appear on Web pages, competing for space and clicks with Facebook\u2019s Like and Twitter\u2019s Tweet. (You can see one in action at the top of this page on a blog about recruitment).\n\nLinkedIn Beats Facebook on Mashable\n\nMashable has been one site quick to make use of the buttons. The tech blog places a Facebook Like button next to its articles, as well as icons for Twitter, StumbleUpon, Tumblr and, on some pages, LinkedIn. A quick look at the frequency with which those buttons are used shows that pages typically pick up more Tweets than LinkedIn Shares, and more Shares than Likes. This post about Groupon, for example, had picked up 1,209 tweets eleven hours after it went up, compared to 55 Likes and 210 LinkedIn Shares. This post about marketing on Facebook, however, generated 256 Shares in less than half an hour, which might suggest that users like to show potential employers that they\u2019re up to date with social media marketing techniques.\n\nThe site\u2019s use of LinkedIn though isn\u2019t limited to sharing buttons. Mashable has also integrated LinkedIn with Mashable Follow, its \u201csocial sharing and content curation platform,\u201d a move only made possible by LinkedIn\u2019s recent adoption of OAuth.\n\n\u201cWe are able to authenticate users using our existing OAuth support framework,\u201d Chris Heald, Follow\u2019s lead developer told readers. \u201cOnce users are authenticated, we can use their authorization tokens to make calls to the LinkedIn API to easily conduct the shares.\u201d\n\nMashable though isn\u2019t the only site to make use of LinkedIn\u2019s new openness. Other sites are lining up to do the same, and in various ways. Behance, a portfolio platform for creative professionals, declares that having uploaded their work, members can \u201cat the touch of a button\u201d broadcast their work on Facebook and Twitter, \u201cas well as sync with LinkedIn.\u201d\n\nLinkedIn Sells Expertise by the Minute\n\nElegant.ly is also aimed at creatives, this time focusing on new designers. The service hopes to match starting designers with new start-up companies in a kind of pauper\u2019s marriage. It uses LinkedIn\u2019s OAuth integration to enable new members to sign in without having to complete pages of registration forms.\n\nBoth of those sites though only make use of LinkedIn\u2019s smart new API to help the networking service do what it\u2019s supposed to do more efficiently. LinkedIn\u2019s main role has always been to help people find work; both Behance and Elegant.ly make job-seeking a little easier by focusing on one niche industry and making the information available on the site readily accessible to people who might be looking to hire professionals in those industries.\n\nMinuteBox is a little more creative. This service also mines LinkedIn\u2019s database to bring professional help to people who need it, but it offers not skills but knowledge. That\u2019s a fairly unique approach and an interesting twist on the usual way in which professionals market themselves. Instead of pitching for full-time or freelance jobs, MinuteBox allows LinkedIn\u2019s members to offer professional advice for which they can charge on a minute-by-minute basis. The information is delivered through video, audio or text chat and the interaction conducted through MinuteBox but the trust is built through an impressive portfolio on LinkedIn.\n\nMove away from LinkedIn\u2019s main function as a way for people with skills and knowledge to sell their expertise, and things start to get a little murkier, even with LinkedIn\u2019s new API. SociallyApp is a smartphone app that tries to integrate a phone\u2019s functions with all of the information streaming through multiple social networks. Those networks include Facebook and Twitter, of course, but also draw on Foursquare and LinkedIn. The app collates the data, adding the latest Facebook picture to contact lists, for example, or placing birthdays on calendars.\n\nBut it\u2019s not easy to see what the benefits might be for users of LinkedIn, beyond adding loose connections to a contact list and forcing the phone user to scroll through more names than he\u2019d like before he can call home.\n\nThe challenge for LinkedIn \u2014 and for developers hoping to make use of its API \u2014 is that the site is not the kind of news streaming service that has made Facebook and Twitter so valuable. (The former for news about friends and family; the latter for updates and chat from industry insiders.) It\u2019s too formal for casual browsing and the reasons for networking on the site are too obviously commercial for the kinds of loose chat that can make even Twitter so much fun. While Facebook is like chatting with friends and Twitter is hanging out at the watercooler, LinkedIn still feels like the kind of professional networking occasion at which everyone wears name tags and tries not to eat with mouth full of canap\u00e9s.\n\nNone of that is to say that LinkedIn isn\u2019t useful. It clearly is as its 100 million users and happy investors will tell you. It\u2019s a valuable service that can help match the career-minded to new opportunities and businesses to talented individuals. Whether it can compete with the flexibility of Twitter and Facebook, even with its new API, remains to be seen.\n\nLeave a Comment",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.7180576921,
        "format_confidence":0.9075990915
    },
    {
        "url":"http:\/\/www.convergedinfrastructure.com\/Windows-Server-2016-Implications-for-Converged-Solutions?asrc=SS_searchsdn_SN-450430926",
        "text":"Windows Server 2016: Implications for Converged Solutions\n\nThe release of any new Microsoft server operating system is always an important milestone for the entire computer industry. Windows Server 2016 is no exception. For many organizations, upgrading to Windows Server 2016 will be a trigger event that defines how they deploy infrastructure today and in the future, ranging from data centers to remote and branch offices (ROBOs) and to the cloud.\n\nThe implications of converged systems as a deployment model for IT infrastructure are profound. Windows Server 2016 is designed to fully embrace convergence, with wide ranging enhancements to compute, networking and storage virtualization\u2014along with systems management\u2014that facilitate the path to the software-defined data center (SDDC) and enable organizations to seamlessly leverage cloud models.\n\nBy embracing the SDDC and the cloud in Windows Server 2016, Microsoft has made it simpler and more cost efficient for IT professionals to leverage converged systems in supporting small and remote offices, whether they deliver IT services locally, from a centralized data center or from a private, public or hybrid cloud. The benefits of converged systems for ROBOs in Windows 2016 environments include:\n\nEnhanced agility: Converged infrastructures enable IT to deploy infrastructure more quickly to support the needs of ROBOs, as well as targeted teams such as software developers. With Windows Server 2016, IT teams can automate and secure ROBO infrastructure from a centralized location using a converged solution, and extend the infrastructure to the cloud on demand when needed. For more flexibility, IT teams can locally deploy a converged infrastructure, a hyperconverged infrastructure or a modular infrastructure solution that is designed to meet the specific requirements of small businesses and branch offices.\n\nSimpler IT: One of the important benefits of converged solutions is that they simplify the processes involved in deploying, maintaining and scaling IT. IT teams can use a unified management platform to orchestrate and automate functions across compute, networking and storage. One of the enhancements in Windows Server 2016 that simplifies IT management for ROBOs is a toolset called Server management tools, which allows IT to manage Windows servers wherever they are, whether physical or virtual machines on premises or VMs hosted in the cloud.\n\n\n\nSoftware-defined architectures: Converged infrastructures already leverage software-defined models, so many of the enhancements Microsoft has developed for Windows Server 2016 are well-suited for such infrastructures. An example is Storage Spaces Direct, a new feature that enables IT teams to build highly available storage systems using local storage. IT teams can support ROBOs by working with Dell EMC, so they can leverage converged infrastructure models rather than trying do-it-yourself approaches that can be complex and time-consuming to manage, particularly where there may not be properly trained local IT personnel on site.\n\nA future-proofed infrastructure: In some ways, Microsoft Windows 2016 is following the path already blazed by companies that have developed the converged and hyperconverged infrastructure market, particularly Dell EMC. The design concepts around Microsoft Windows 2016 are built on the same foundation as the converged infrastructure model, specifically:\n\n  \u2022 Building a software-defined foundation\n\n  \u2022 Automating and securing the infrastructure\n\n  \u2022 Having the capability to expand the infrastructure to the cloud\n\nMust Read\n\nWhen FUD Fizzles\n\nFind out why circulating fear, uncertainty and doubts about the strategies of your competitors doesn't always work.\n\nContinue Reading\u00a0\n\nThe Microsoft model, in fact, is quite similar to the Dell EMC Future-Ready Enterprise model, which is based on converged and tightly integrated infrastructure that is:\n\n  \u2022 Workload ready\n\n  \u2022 Virtual-infrastructure ready\n\n  \u2022 Software defined\n\n  \u2022 Cloud ready\n\n  \u2022 Big data optimized\n\nThe release of Windows Server 2016 is, in many ways, an affirmation of the converged infrastructure model pioneered by Dell EMC because it embraces all of its key concepts, including virtualization as well as automation and orchestration across compute, networks and storage to support the evolution to a software-defined data center.\n\nAs IT decision-makers consider migrating to Windows Server 2016, one of the benefits to keep in mind is the ability to use Windows Server 2016 in combination with converged infrastructures to deliver IT services to ROBOs more efficiently. Advantages include simpler IT, enhanced agility and a future-proofed model that offers the flexibility to leverage infrastructure resources on premises, from a centralized data center or from the cloud.\n\nDell EMC Solutions are powered by Intel\u00ae",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.772834599,
        "format_confidence":0.5738512278
    },
    {
        "url":"https:\/\/virtualizationreview.com\/articles\/2014\/11\/05\/death-of-virtual-machines.aspx",
        "text":"Dan's Take\n\nThe Death of Virtual Machines? Not so Fast\n\nDan argues that containers are great, but not always the right choice.\n\nI was reading \"Are Containers the Beginning of the End of Virtual Machines?\" In this article, Jeffrey Schwartz asks if containers, an implementation of OS virtualization and partitioning (OSVP), will replace virtual machine (VM) technology. While the analysis is certainly worth reading, he's speculating whether one type of virtualization technology will replace the other over time. I believe that both types have a good future, because they address different requirements, and both types of requirements are likely to continue to be found in enterprise datacenters.\n\nVirtual Processing Software: A Definition\nBoth OSVP software and VM software are types of virtual processing software. My book, \"Virtualization: A Manager's Guide\" (O'Reilly, 2011), offers the following definition for processing virtualization:\n\nProcessing virtualization does one of three things: encapsulates the operating system so that many virtual systems can run on a single system, links multiple systems together so that workloads will fail over if a system fails, or links systems together so an application or data can be spread across all of them for performance or scalability. This means that, depending upon the type of processing virtualization, the application can be run on multiple systems simultaneously or run under a hypervisor. A hypervisor can run as a process under another operating system or can run directly on the system hardware. Hypervisors can support one or more complete virtual systems at the same time.\n\nThis technology, as with other virtualization technologies, was originally developed for mainframe systems in the late 1960s, was recreated on minicomputers (now called midrange machines) in the 1980s, and started appearing on industry-standard systems (x86-based) in the early 1990s.\n\nThis type of virtualization is often used in conjunction with several other types.\n\nThere are five different types of virtualization technology in this layer (see \"The 7-Layer Virtualization Model\" or my book for a deeper look at the layer cake I call virtualization). Workload managers, cluster managers, parallel processing managers, OSVP and VM software are all found at this layer.\n\nOperating System Virtualization and Partitioning\nOSVP virtualization technology makes it possible for different workloads to reside on the same physical host and operate independently, in near complete isolation, on the same OS. This capability has been a standard part of mainframe systems since the 1960s. It was made available as partitions, LPARs, VPARs or containers in Unix environments in the 1980s. It has reappeared in Linux environments, and several companies are offering this type of technology for Windows.\n\nThe benefits of this approach are performance and efficiency. Because one OS is supporting all these virtual environments, less memory and storage is required. Furthermore, switching from one partition to another is very fast. Also, a single physical host may be able to support several times the number of independent workloads than if VM software was being used.\n\nWhile using this approach, all of the workloads work with the same OS and the same version of that OS. So mixtures of Windows, Unix and Linux workloads on the same physical host aren't part of the repertoire of this type of processing virtualization. The tradeoff is between performance and flexibility.\n\nVirtual Machine Software\nVM software, also known as a \"hypervisor,\" works differently than OSVP, even though some attributes are the same. Rather than sitting inside a single OS and allowing it to partition and isolate workloads, VM software typically sits below the OS. It encapsulates all of the software, from the OS all the way up to the applications running on that OS into a virtual system. Several VMs are allowed to execute simultaneously on the same physical host.\n\nThe benefits of this approach are somewhat similar to the use of OSVP. VMs can execute on the same system, but be isolated from one another. The major difference, however, is that VM software makes it possible for many different OSes or OS versions to happily work at the same time on the same physical host.\n\nThere's a cost for that flexibility, however: VM files can be big, and take up quite a bit of memory and storage. Moving them into and out of physical systems can take quite a while. Moving them from one physical machine to another can consume a lot of network bandwidth, as well. This also means the process of switching from one VM to another can take significantly more time than switching from one partition to another.\n\nIf the enterprise wants the flexibility of having Windows, Linux and Unix workloads coexisting and executing on the same physical machine at the same time, VM software is the best choice.\n\nDan's Take: Not One or the Other\nWhat does all this add up to? Both types of processing virtualization technology are likely to have a strong future in the datacenter. When organizations need to run multiple workloads on the same physical system simultaneously, and they're happy to have them all run on the same OS, OSVP is the best choice. Smaller system configurations could be used to get the same or better performance. This is why Docker is getting so much attention today.\n\nIf support of workloads hosted by a number of different OSes or OS versions is required, VM software is the answer.\n\nAbout the Author\n\nDaniel Kusnetzky, a reformed software engineer and product manager, founded Kusnetzky Group LLC in 2006. He's literally written the book on virtualization and often comments on cloud computing, mobility and systems software. He has been a business unit manager at a hardware company and head of corporate marketing and strategy at a software company.",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.6187654138,
        "format_confidence":0.8761869073
    },
    {
        "url":"https:\/\/www.sitepoint.com\/analytics-bliss-quantitative-data-with-qualitative-research\/",
        "text":"Analytics Bliss: Quantitative Data with Qualitative Research\n\nThis article was originally published on the Fullstory blog, and is republished here with permission.\n\nKnowing what happens on your website is hard. You can have all the user data in the world but no idea what to do with it, no process to manage it, and no way to filter the signal from the noise. Where do you even start? And once you begin, where do you go? What you need is a systemized approach \u2014 and the right tools.\n\nWhen it comes to knowing, traditionally there have been two approaches you can take. The first is the quantitative approach. Quantitative analytics puts everything in terms of numbers. Number of users doing whatever \u2014 bounce rates, conversion rates, time on site \u2026 that kinda thing. Quantitative research is a squeaky clean way to crunch numbers and look at things at a high level.\n\nThe drawback to quantitative data is that much can be lost in translation. Important nuances are lost. (What determines a functional analytics tool is a subject for another day, but no matter what, the analytics tools you choose should serve your needs and budget.) And in the absence of information, the quantitative can play tricks on our pattern-recognizing brains. We can see things that just aren\u2019t there \u2014 just like how we can see faces in objects.\n\nThe first few results for the Google Image Search for \u201cfaces in objects\u201d do not disappoint.\n\nTo really know what happens on your website or web application, you need a way to get at that nuance. And that\u2019s where the second approach comes in \u2014 the qualitative. Qualitative analysis focuses on the subjective qualities of your users \u2014 the nuanced actions that drive the numbers. It\u2019s descriptive research. The qualitative approach is subjective, too. The user paused here. Raged there. They seem confused with this UI: what\u2019s going on? Qualitative data is messy. But it is rich in insights.\n\nThe major problem with qualitative research methods is they occur at the individual level. Qualitatively researching every user who visits your site is impossible. You could never do it, nor would it be useful.\n\nSo what do you do? Use both.\n\nSo what do you do? Use both.\n\nThe best analyses combine quantitative and qualitative research to create a flywheel of continuous product improvement. You use quantitative information to focus attention on the biggest problems (and opportunities) and then \"zoom in\" to the level of the user through qualitative research to round out your understanding \u2014 and solve the problem.\n\nThis one-two research method spins out insights you can turn into actions. It\u2019s a quantitative-qualitative flywheel that can be used for continuous improvement of your website or app.\n\nHere\u2019s how it works \u2026\n\nQuantitative data alerts you to problems. Start here.\n\nThe most basic web analytical tools available are quantitative \u2014 and free. It comes as no surprise that one of the first things website owners do on launching a new site is install Google Analytics (GA).\n\nQuantitative analytical tools like GA do a reliable job providing high-level metrics \u2014 e.g. user info, page visits, events, conversion rates, bounce rates, time on site, etc. Most importantly, they allow you to see the status of different aspects of your site and help you focus on problems or opportunities in aggregate.\n\nWith a functional quantitative analytics tool up and running, you\u2019re ready to identify issues and opportunities for improvement. This is the first step in building the quantitative-qualitative flywheel. (Also see our discussion on if user metrics make you heartless.)\n\nThe Quantitative-Qualitative Flywheel starts with identifying problems through quantitative analysis.\n\nThe Quantitative-Qualitative Flywheel starts with identifying problems through quantitative analysis.\n\nOnce you have a quantitative tool tracking your customer data, you can quickly be dazzled by the illusion of insights. Fight the tendency to be hypnotized by the show.\n\nCome at your newfound quantitative data powers with an objective. Otherwise, you can easily be overwhelmed.\n\nConsider these changes in quantitative metrics that left teams struggling to understand the why behind the quantitative what:\n\nRemember: behind every quantitative metric is a customer trying to get some job done. All you need to figure out is what that job is. And to do that, build out the next step in your flywheel.\n\nQualitative data gives you the why\n\nHaving used quantitative analysis to identify problems that require additional analysis, you need a way to \"zoom in\" on individual user behavior. That\u2019s why you need a qualitative analytics tool.\n\nQualitative data can provide you with a firehose of nuanced information about individual user behaviors. It\u2019s the most dense level of data, and it\u2019s what you get through session replay. Using a qualitative analytics tool, you can see how individual users interact with your product.\n\nThe second step in the Quantitative-Qualitative Flywheel is to use qualitative analysis to understand behaviors at the individual level.\n\nThe second step in the Quantitative-Qualitative Flywheel is to use qualitative analysis to understand behaviors at the individual level.\n\nHow does it work? How do you go from quantitative data to qualitative research? Using a tool like FullStory, you can search the entire catalog of user behaviors on your site for just those that would affect whatever quantitative data you\u2019re researching.\n\nContinuing from our company examples above, qualitative data was used to uncover the why behind the change in quantitative data:\n\n  \u2022 Rage clicking indicated to Wistia that much of their customer churn was caused by confusion about how to use the product.\n  \u2022 Observing that most users skipped onboarding shed a light on why Kayako users didn\u2019t know about certain features or couldn\u2019t figure out how to use them.\n  \u2022 Seeing the site layout from the user\u2019s perspective helped SpanishDict see that users couldn\u2019t figure out how to get started because a banner ad was blocking the search bar.\n\nIn each of the above cases, qualitative research (completed using FullStory) provided the additional insights necessary to make sense of the problems identified through other quantitative tools.\n\nHaving used qualitative research to understand why you\u2019re seeing changes in quantitative data, it\u2019s time for action.\n\nExperiment and check for improvements\n\nArmed with a complete view of the user from the individual to the aggregate, you now have the insight you need to do something, and that something is to make edits and run experiments. This is the last step in the quantitative-qualitative flywheel.\n\nOnce you thoroughly understand the problem through quantitative and qualitative analysis, make the needed edits or run experiments.\n\nOnce you thoroughly understand the problem through quantitative and qualitative analysis, make the needed edits or run experiments.\n\nJust what edits are required will vary greatly depending on the problem you\u2019re solving. Make the edits, run the experiments (A\/B testing is helpful here), and then measure your results.\n\nYou\u2019re back at the top of the flywheel, and on and on we go.\n\nFull circle\n\nRather than qualitative vs quantitative, perhaps analysts should use the two together.\n\nQuantitatively identifying problems in aggregate and then qualitatively understanding the underlying behaviors behind those problems has become a critical workflow that drives product improvements:\n\n  \u2022 For Wistia, user confusion about how to use the product was reduced by through allocating additional resources to customer support.\n  \u2022 Kayako learned that they needed to overhaul their user onboarding, and designed a completely new onboarding flow to guide users through the part of the product they were most interested in.\n  \u2022 SpanishDict increased engagement by moving the ad banner that was blocking the search bar to optimize for mobile users.\n\nThe quantitative-qualitative flywheel ensures your brain will never have to scramble to fill in gaps. The best qualitative and quantitative analytics tools work together to reveal the full story of user behavior on your site.\n\nPut the flywheel to work and reap the rewards of an improved customer experience on the web.\n\nPut the flywheel to work",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.6059131622,
        "format_confidence":0.6944210529
    },
    {
        "url":"https:\/\/www.cso.com.au\/article\/640497\/pre-emptive-action-vital-battle-against-cyber-attacks\/?utm_medium=leapfrog&utm_source=www.computerworld.com.au",
        "text":"Pre-emptive action is vital in the battle against cyber attacks\n\nby Simon Howe, ANZ Sales Director, LogRhythm\n\nOnce the subject of alarmist science fiction movies, cybersecurity breaches have now become an all-too-common feature of modern life.\n\nFrom attacks on power grids and nuclear reactors to the disabling of business IT systems and theft of data stores, the activities of cyber criminals are becoming ever more sophisticated. As a result, the damage and disruption they can cause is growing by the day.\u00a0\u00a0\n\nWith the number of threats continuing to increase, there are four key action areas in which Australian organisations should be focusing. Taking steps in each of these will reduce the likelihood of falling victim to an attack and help with mitigation should one occur. The action areas are:\u00a0\n\n1. Constantly monitor vulnerabilities\nThankfully, most organisations now have basic IT security measures in place, but it must be remembered that this is far from being a set-and-forget exercise. IT teams need to constantly update operating systems and applications on a regular basis to ensure they can withstand new threats.\n\nFailure to do this can have big ramifications. For example, the WannaCry ransomware strain that hit victims in 2017 succeeded because it exploited a vulnerability in Windows that had been identified weeks earlier. Microsoft had released a patch for the bug, but many organisations failed to apply it, leaving them open to the exploit. The Petya\/NotPetya attacks soon followed and used another known vulnerability to target Ukrainian infrastructure and huge companies such as shipping giant Maersk.\n\nIT departments must have a robust process in place to become aware of security vulnerabilities as they are disclosed \u2013 and to apply patches as soon as they\u2019re available.\n\n2. Focus on employee training\n\nIt\u2019s often said that the weakest link in any IT infrastructure is the users, and this is backed by the fact that Phishing remains one of the most common techniques used by cybercriminals.\n\nResearch\u00a0 has found that two thirds of organisations have fallen victim to social engineering attacks, while whaling\/CEO fraud is becoming increasingly common as a way to dupe individuals into making payments into the accounts of cybercriminals.\n\nFor this reason, all staff need to be trained effectively to spot the warning signs of attacks. To make this more effective, organisations are turning to gamification techniques\u00a0 to make training more engaging for staff.\nDefined by Gartner as \u201cthe use of game mechanics and experience design to digitally engage and motivate people to achieve their goals\u201d, gamification is being successfully used by the likes of Ford, Deloitte and PwC to make their workforces more security-savvy.\n\n3. Don\u2019t pretend nothing has happened\n\nDuring 2017, Uber acknowledged a major data leak had compromised 57 million user accounts. The company also admitted it had paid hackers $100,000 to conceal and destroy the stolen data.\n\nThe way the company dealt with the breach led to its chief security officer leaving his post and resulted in an investor consortium offering 30 per cent less than share value for a stake in the company.\n\nA similar incident occurred when credit-rating agency Equifax waited until September 2017 to report a hack of 143 million personal information records that had occurred in July of that year. The company\u2019s CEO, CIO and CSO all resigned once it became clear that there had been a delay in reporting the incident.\nIt is likely the impact of these breaches wouldn\u2019t have been so severe if the organisations had been upfront about them. Businesses should therefore be transparent when they suffer a security breach.\u00a0\n\n4. Get more sophisticated\n\nIn general, recent cybersecurity breaches suggest that many organisations need to become more sophisticated in their security capabilities.\n\nFor example, when it comes to phishing, two-factor authentication can severely reduce risks, as it makes stolen credentials extremely difficult to use. Also, mobile devices linked to corporate networks should be encrypted to ensure confidential data that\u2019s contained, communicated and accessed by them is difficult for third parties to obtain.\n\nThe segmentation of corporate networks, with authentication required to move between segments, also makes it more difficult for attackers to get at the data they\u2019re seeking, making it too costly and time-consuming for them to pursue.\n\nIt\u2019s also a good move to put in place a security incident and event management (SIEM) together with relevant network monitoring and user and entity behaviour analytics (UEBA). Such systems will ensure an organisation is much better prepared to rapidly detect and respond to compromises.\n\nAt the very least IT teams need to be diligent about applying software patches and providing compelling and effective training for users. They should also have detailed plans in place for how to report and respond to successful breaches.\n\nConsidering these four action areas now will put your organisation in a much better position to withstand cyberattacks and deal with any that might succeed.\n\nJoin the newsletter!\n\n\nSign up to gain exclusive access to email subscriptions, event invitations, competitions, giveaways, and much more.\n\nMembership is free, and your security and privacy remain protected. View our privacy policy before signing up.\n\nError: Please check your email address.\nHave an opinion on security? Want to have your articles published on CSO? Please contact CSO Content Manager for our guidelines.\n\nTags cyber attacksIT systems\n\nMore about CSODeloitteEquifaxGartnerMicrosoftUber\n\nShow Comments\n\nFeatured Whitepapers\n\nEditor's Recommendations\n\nSolution Centres\n\nStories by Simon Howe\n\nLatest Videos\n\nMore videos\n\nBlog Posts",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.7858923674,
        "format_confidence":0.5362436771
    },
    {
        "url":"https:\/\/www.smallbusinesscomputing.com\/buyersguide\/article.php\/3686446\/The-Outlook-is-Good.htm",
        "text":"The Outlook is Good\n\nBy Gerry Blackwell\n  \u2022 Print Article\n  \u2022 Email Article\n\nFor many of us, Outlook, the e-mail, contact, calendar and task manager module in Microsoft Office, is a crucially important organizing tool without which we\u2019d be lost. Mess with our Outlook and you mess with something very personal to us, a virtual extension of our brain.\n\nIf you\u2019ve been following this ongoing series of reviews of the major components in Office 2007 you\u2019ve probably already guessed it \u2013 Microsoft messed with Outlook. The good news: not only are the downsides not as bad as we feared, the new version is actually a major win for Outlook fans, with significant new and useful functionality and some nice minor improvements.\n\nOutlook screenshot\nInstant Search, an Office-wide feature, replaces the painfully slow search function of past Outlook versions.\n(Click for larger image)\n\nWe're still not entirely over our snit about Microsoft betraying long-time devotees by forcing us to learn a new interface (when the old one seemed perfectly adequate), and by removing some of the ability to personalize and customize the Office programs. But the new version of Outlook definitely puts us in a kindlier mood.\n\nFor starters, Outlook 2007 does not use the full Microsoft Office Fluent interface, which in the other programs replaces traditional menu and tool bars and vertical text menus with a ribbon and tabbed panels that drop down and stretch across the screen, graphically displaying groups of functions. In the main Outlook 2007 windows \u2013 Mail, Calendar, Contacts, Tasks \u2013 the menus and task bars remain much as they were, with only minor and entirely welcome modifications.\n\nWe\u2019re assuming this decision was made because the ribbon would take too much screen real estate away from display of calendar and e-mails. But as soon as you call up a subsidiary window \u2013 any of the dialogs for adding new items, for example \u2013 you get the full Office 2007 interface with ribbon and graphical display of functions.\n\nOutlook does also remove some ability to customize, and the changes under the hood may render some macros inoperable. Is this terribly important? Probably not for the vast majority of people, but here is a tiny example to illustrate our frustration at these changes.\n\nOne of our pet peeves about Outlook 2003 (and previous versions) \u2013 and of others, too, as we know from trolling Internet forums in search of a solution \u2013 is that when you view the calendar in month mode and hit the Today button (or select Go To\/Today), Outlook displays the first week of the month at the top of the page, the last week at the bottom, with today\u2019s date highlighted somewhere in between. If you\u2019re at or near the end of the month, you see mostly dates that have passed, which isn\u2019t terribly useful.\n\nSo big deal. Just scroll down, right?\n\nNot us. We took considerable effort to write a macro to display the monthly view the way we wanted, with the current week at the top of the page so we could see upcoming dates below it \u2013 much more sensible. Outlook 2007, we discovered, displays the month view the same illogical way as past versions \u2013 but the macro we had labored over no longer works. Argh!\n\nIt took us more than an hour to figure out how to change the macro so it would perform the same task.\n\nNew and Better\nBut we're nit-picking. The good in Outlook 2007 outweighs the bad. Some of the good new features require you to be running a Microsoft Exchange server and\/or Microsoft Sharepoint Services, but here\u2019s a list of some of the best of what\u2019s new that anyone can use:\n\n  \u2022 Instant Search \u2013 fast desktop searching, an Office-wide feature, replaces the painfully slow search function of past Outlook versions\n  \u2022 To-Do Bar \u2013 a side panel (optional) in Mail that provides a consolidated view of daily tasks, flagged e-mails, upcoming appointments, etc\n  \u2022 Attachment Preview \u2013 view attachments within Outlook without launching another program\n  \u2022 Flagging Mail as Tasks \u2013 create a task from an e-mail in one step\n  \u2022 Task integration on calendar \u2013 drag tasks and flagged mail onto a calendar to schedule time to complete them\n  \u2022 Navigate E-mail Threads \u2013 jump from one message to another within a thread\n  \u2022 Color Categories \u2013 a new way to group and quickly find information using color cues\n  \u2022 RSS Feed Integration \u2013 have Really Simple Syndication (RSS) subscriptions delivered directly to your inbox\n  \u2022 Electronic Business Cards \u2013 a new way to personalize and share contact information in a business card format\n  \u2022 Auto Account Setup \u2013 Outlook 2007 can set up most e-mail accounts using only address and password\n  \u2022 Office InfoPath Integration \u2013 embed a form in an e-mail message for the recipient to fill in and return\n  \u2022 Anti-Phishing and Junk e-mail \u2013 improved prevention for junk and malicious e-mail\n\nMicrosoft\u2019s Instant Search utility, integrated with Office 2007 but also available separately as a free download, is a general-purpose desktop search tool. It\u2019s not as good, in our opinion, as some other desktop search tools, including the one from Yahoo, but with Outlook integration, Instant Search makes searching for messages about three times as fast as it used to be. A very welcome improvement.\n\nMuch To-Do\nOne of the first things you\u2019ll notice when you launch the new version of Outlook and go to Inbox is the To-Do bar, which appears by default at the right of the screen. You can also display it in Calendar, Contacts and Tasks, but you may have to go to the View menu and select the To-Do bar to tell it to display.\n\nThe bar includes a thumbnail view of the month, a list of upcoming calendar appointments, color coded as they are in Calendar, and a list of tasks arranged by date. You can still open Outlook Today -- to get a quick view of your day and week -- by clicking the Personal Folders icon in the left-hand panel in Mail. Outlook Today does give more detail \u2013 more appointments, for example. But the To-Do bar will give most people all the task information they need while viewing e-mail, and in a much more convenient way. And you can have it appear in other Outlook windows too.\n\nYou can minimize the bar to make more room for messages \u2013 it appears as a thin border along the side of the screen, with sideways lettering showing your next appointment and telling you how many tasks you have. You can also close it completely. And you can reconfigure it to show less of some types of items or more of others.\n\nThe ability to view Office and other standard format attachments within Outlook, Inbox without having to open another program, is a nice convenience that saves time when all you need is a quick look.\n\nAn unobtrusive bar near the top of the message shows labeled icons for Message \u2013 selected by default \u2013 and for each attachment. To view an attachment, simply click its icon. A disclaimer pops up pointing out that the version of the attachment you\u2019ll see may not be exactly as it appears when viewed using the program that created it, but you can turn this off for future messages. A 1MB PowerPoint file took a few seconds to load in attachment preview on my 1.6 MHz dual-core laptop. To return to the message after viewing the attachment, just click the message icon.\n\nOutlook screenshot\nThe To-Do Bar offers a consolidated view of daily tasks, flagged e-mails, upcoming appointments, etc\n(Click for larger image)\n\nFlagging e-mail messages as tasks is another nice convenience for some people, though not all. Many workers get instructions from bosses and clients by e-mail. In the past, if they wanted to create an Outlook task to schedule and track an assigned piece of work, they had to retype or clip and paste information from the message into a New Task form.\n\nNow you can simply right click on the flag beside a message in the Inbox to schedule the task. In previous Outlook versions, right clicking brought up a menu that let you select a different colored flag so you could set up a personal color coding system for messages. In Outlook 2007, the menu lets you choose a time frame for completing the task or following up on the message \u2013 Today, Tomorrow, This Week and so on. The task now shows up in the To-Do bar and in Tasks and when you double click it, the original message opens.\n\nMaking Time for Tasks\nThe integration of Tasks with Calendar is the kind of feature that makes you scratch your head and wonder why they didn\u2019t do this long ago. You have tasks and you have appointments, but until now, Outlook only allowed you to block time on your calendar for the latter. If you wanted to block time to complete a task, you had to create a \u201cfake\u201d appointment. Now, you can simply drag tasks from the To-Do bar onto a day in the calendar, open the item and set the time you want to devote to it.\n\nOne new feature that barely rates a mention in Outlook 2007 literature but that we think is very useful is the capability to jump through a long e-mail thread message by message, without having to scroll to find the message you want. When you mouse over a spot near the top right of the message window, navigation buttons appear that let you jump to the next or previous or last or first message in a thread. It\u2019s not a big thing, but it will make reviewing long threads a little easier.\n\nThe new Color Categories is really just a visual extension of the existing categories function. As well as an alphanumeric name for each category, you can now assign a color, which theoretically makes it easier to quickly see relevant items in lists of messages, contacts and tasks. If you\u2019re particularly visually oriented and you don\u2019t need too many categories, it might be a help. If you have a lot of categories, you\u2019ll have to use subtler shades to differentiate them which could defeat the purpose \u2013 categories with similar colors won\u2019t stand out from each other.\n\nWe\u2019ve really only scratched the surface here. Outlook offers possibly the richest set of new features of all the Office 2007 modules. As with most of the others, it includes great new featu... (truncated)",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.9895336032,
        "format_confidence":0.9397521019
    },
    {
        "url":"https:\/\/www.extremetech.com\/extreme\/53298-could-we-be-winning-the-spam-wars",
        "text":"\n\nWe were getting spammed to death! We hadn\u2019t practiced good e-mail address practices in the past and now some users were getting blasted with 20 or more spams a day and over 50 during the weekends. Spam became an issue at management and IT meetings. We had to do something. Many users\u2019 Inboxes became almost unmanageable. Pressing the Delete key is easy but wading through 20 spams to find two good e-mails is a total waste of time.\n\nSpam became the bane of my existence.\n\nWe tried everything at the desktop level using Outlook and Exchange filters, rules and anti-spam add-on programs. They just didn\u2019t work well for us. Good mail got rejected and spam still got through.\n\nLuckily for us, our Raptor\/Symantec Enterprise firewall supported the use of one Real Time Black Hole List (RBL). The default service was MAPS; however, they wanted money for their service and wouldn\u2019t let us test the service first.\n\nNot knowing much about RBLs and after being spoiled by our Apache\/Linux web server cost (that is, zero), we decided to pursue other options. Our company is going through a \u201cbelt-tightening\u201d process.\n\nWe then shifted gears and looked into creating a mail proxy server using Spam Assassin and Linux. Our research for Spam Assassin pointed us in Spamcop\u2019s direction; Spamcop has a free RBL (\n\nWe configured our firewall to use Spamcop and wham! We started blocking hundreds of spam attempts per day.\n\nNow we were onto something. We started blocking a large majority of our spam using just one RBL and we were rejecting it at the perimeter, so it didn\u2019t clog up our mail servers or users\u2019 folders. Most of the desktop-based programs redirect incoming spam to a folder on the mail server.",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.9743475318,
        "format_confidence":0.6934404373
    },
    {
        "url":"https:\/\/bdtechtalks.com\/2016\/04\/11\/how-mobile-technology-is-helping-improve-authentication\/",
        "text":"How mobile technology is helping improve authentication\n\n3 min read\n\nsmart lock\n\nFor many years, the issues with password-based authentication have been riddling the cybersecurity industry. Passwords are being stolen, bruteforced and circumvented in a myriad of ways by hackers that are seeking to find their way into the accounts of their unfortunate victims. The main mechanism to counter this problem has historically been known as two-factor and multifactor authentication (2FA\/MFA).\n\nBasically, two-factor authentication involve the combination of something you know (the password) and something you have (physical token, fingerprint, retina scan, voice recognition\u2026) in order to authenticate users.\n\nBut the problem is that 2FA\/MFA have had their own set of complexities in implementation and integration, to say nothing of the vulnerabilities. For instance, in older days, the implementation of fingerprint scanners and authentication was an arduous and costly process, requiring expensive hardware and complex SDKs and APIs. And aside from that, fingerprint scans can easily be stolen off databases or picked up from physical locations touched by the user, and later used to circumvent second factor authentication.\n\nPhysical tokens also have their own flaws. They can be lost or stolen, and they\u2019re an extra burden, something that the user has to carefully look after, and since they are tailored for a very specific purpose, users are prone to forget about them, leave them on their office desk and only remember them once they get home and try to access their account only to find that they don\u2019t have the key on them.\n\nAll of these challenges and complexities have become major hurdles in the implementation of 2FA\/MFA.\n\nBut that is gradually changing as advances in mobile technology are providing many new opportunities for strengthening security in online accounts. For one thing, smartphones are devices that are extremely personal, and they\u2019ve become inherent parts of our lives, a digital extension of the self that is never detached from the person and is seldom forgotten on counters and desks. And their connectivity makes them the perfect device to identify us on the internet.\n\nAs has been proven in recent debates that have pitted tech companies against law enforcement, the recent generations of mobile devices are also extremely secure, which makes them perfectly suitable as a physical second token to confirm authentication.\n\nIn fact a number of companies, including Google, are exploring the efficiency of using mobile devices as the main medium for login and authentication. Instead of entering a passcode, users receive notifications on their phone, to which they can respond in order to grant access to the account. Naturally, only the person with the phone\u2019s secure entry code will be able to confirm the notification. Therefore, by just remembering a simple 4- or 6-digit PIN code, users will be able to control all of their accounts and won\u2019t need to remember multiple sets of complex passwords for each of their mobile-authenticated accounts.\n\nBut the progress of mobile goes well beyond enabling simple physical token access. The features that are embedded in today\u2019s phones make them perfect for many other scenarios and forms of authentication. For instance, the fingerprint scanners on most new phones are enabling service providers to implement fingerprint authentication more easily. Puerto Rican startup company Qondado LLC has launched an easy-to-implement service called KodeKey, which enables developers to integrate fingerprint authentication into their web apps in a few easy steps. The platform is also available as a WordPress plugin, which users only need to install and register on their website.\n\nInstead of using passwords, KodeKey sends a fingerprint scan request to the target user\u2019s phone when they try to log in their account. No fingerprint scans are sent over the internet and the authentication takes place inside the secure confines of the phone, and only the result of the challenge is sent back to the service provider which enables it to authenticate the user.\n\nBeyond fingerprint scans, smartphones offer a plethora of other goodies that can all be used for MFA. Hi-res cameras can actually help in retina scan and facial recognition. High quality microphones can be used for voice recognition. GPS can be used for geolocation- and geofencing-based authentication. And all of them can be blended together in order to improve security and prevent identity theft. LaunchKey offers a decentralized authentication platform that enables developers to leverage the full power of mobile devices to set-up multifactor authentication for their services without the use of passwords. The platform takes full advantage of all features available on mobile devices and enables users to secure their accounts with different rules that will require one or more authentication process based on the sensitivity of the account or that of the type of action being taken.\n\nPasswords are far from dead, but mobile is surely changing the way we interact with our internet profiles. As internet usage moves more and more toward the mobile, there might come a day where passwords become history.\n\n\nLeave a Reply\n\nThis site uses Akismet to reduce spam. Learn how your comment data is processed.",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.6117507815,
        "format_confidence":0.8928638101
    },
    {
        "url":"https:\/\/www.theregister.co.uk\/2015\/05\/18\/its_the_end_of_life_as_we_know_it_for_windows_server_2003\/",
        "text":"\n\nCan you survive without support?\n\n\nWindows Server 2003 will pass out of Microsoft support on July 14, 2015. Different organisations report different numbers, but all agree that there are millions of Server 2003 servers still running in the wild.\n\nMicrosoft says there are 11 million Server 2003 servers still running. Gartner says eight million. Several internet searches bring up various other numbers, but I think it is safe to say somewhere between five and 15 million Server 2003 servers are still out there.\n\nMy hunch is that Gartner is under-estimating here. The analyst focuses on enterprises and on the whole wouldn\u2019t care if small businesses were all to get flushed into the sun. A Spiceworks poll of workplaces reports that 57 per cent of respondents have at least one Server 2003 instance still running.\n\nOne study looking at un-migrated numbers specific to the UK finds 400,000 units yet to receive TLC there. I have as yet been unable to find numbers for my own country, Canada, but my straw polls say it will be a lot more than people think.\n\nAnother poll from late last year says one fifth of businesses will miss the mark.\n\nAmong my own client base there are probably 40 or so units left in the wild, and only about 20 of those can be migrated. Once upon a time Server 2003 was the last gasp for applications that should have been ported when Windows NT met its maker. Now, years later, the time has come again for some of my clients, and despite repeated warnings they are not ready.\n\nThere are a number of reasons why people don't want to migrate: familiarity with the older operating system; money; and in many cases the complexity of the workloads running on those Server 2003 instances.\n\nMany of those servers probably have not had much attention paid to them in years aside from periodic patching. Administrators responsible for the workloads running on them may well have moved on, and their replacements may be theoretically aware of how to migrate data and settings but have have never had the time to practice doing it.\n\nTales of the unexpected\n\nOf course, if you don't migrate you have two options: choose to run your operating system without security patches, or cough up for emergency support.\n\nThe cost of support for Server 2003 will be higher than it was for dragging instances of XP past their due date. Think $600 per server for the first year, and rising for every year after that, though you an probably bank a discount if you are a big customer.\n\nWeird things can happen during migrations. Take this domain controller bug. Even if things are mostly straightforward, there is really no substitute for experience.\n\nOver the past several years many sysadmins have done Server 2003 migrations. Unfortunately, there are not that many of us who can legitimately say we have done hundreds or even thousands of them. This has led to fear in the channel that one of the things holding migrations back is simply a lack of systems administrators with the skills and experience to do the job.\n\nFortunately the internet is full of practical guides to migration, so if you end up having to do this on your own you won't be left out in the cold.\n\nTo July and beyond\n\nMicrosoft's official Support Lifecycle Policy FAQ is here. If you need to talk to Microsoft about extending support past the Server 2003 deadline, that's the place to start.\n\nWhen planning your migration away from Server 2003 it is worth bearing in mind the end of support dates for its successor operating systems.\n\nWindows Server 2003 in most flavours meets its end on July 14, 2015. Windows Server 2008 support ends January 14, 2020, while Windows Server 2012 support ends January 10, 2023.\n\nWindows Server 2016 is coming out soon, presumably in 2016. It will come with a new nano version. For some it is rather frustrating to have this new version coming out next year and yet have Windows Server 2003 expiring this year. So close, and yet so far.\n\nFor those who want to hop on Server 2016 as soon as it lands, Microsoft has Software Assurance as an option. Pay more, buy into the whole subscription concept for licensing and get access to upgrades as soon as they appear.\n\nBad vibrations\n\nWhat everyone needs to remember is that Server 2003 doesn't suddenly stop working when support expires. It will continue doing its thing \u2013 but there will be an increased risk of security incidents past that date.\n\nEvery bad guy who has been sitting on hidden vulnerabilities is going to pull them out and go to town. Unlike Windows XP Server 2003 won\u2019t cling around, zombie-like, at high numbers for years.\n\nCompanies will upgrade their servers. Those that don't will lock them down behind so many defences that nobody outside those organisations will ever know that a Server 2003 unit exists.\n\nRight after support ends there will be a surge of attack attempts, malware and other trouble. This will last only so long and then it will simply taper off as the bad guys move on to more lucrative targets.\n\nBut let\u2019s not get too doom-laden. Contrary to what you will be told by those with a vested interest in selling you new severs, it is entirely possible to defend an out-of-support operating system.\n\nThere is no path by which you save money by not upgrading your Server 2003 box and yet remain secure\n\nWhat you need to know before you even consider trying to do so is that if you do it properly you will spend a heck of a lot more defending the operating system than you would on the cost of a new licence.\n\nThere is no path by which you save money by not upgrading your Server 2003 box and yet remain secure. If you think you can defend such a box with free tools you are deluding yourself and anyone who listens to you.\n\nOf course, the other thing to consider is that in many cases the cost of migration is more than simply the cost of a new server and new operating system licence. Where the workload in question is on the complex side, administrators probably don't know how to migrate it or as migrations delve deep into the inner workings of Windows additional costs will appear.\n\nThese can be as simple as the cost of calling the application developer's support line and paying it to help you migrate. It can also evolve into multi-vendor support fiascos that can really run the meter, or require bringing in outside consultants charging some quite exorbitant fees.\n\nMost migrations away from Server 2003 will be easy. Some will be hard. A few simply won't be possible at all.\n\nCry for help\n\nIf you are a small business needing help migrating away from Server 2003 IT network Spiceworks is perhaps the best place to find a local service provider. It has a list which will help you to find a provider operating in your area \u2013 one that charges you rates you can afford.\n\nIt is a place to start, but it covers anyone who chooses to list themselves as a service provider and there is no vetting of any kind. But although that may sound like a dire warning, there are great companies to be found on the Spiceworks IT providers list.\n\nI strongly recommend cross checking these companies against the Microsoft partner list. This will give you a decent idea of how much a company is committed to the Microsoft ecosystem. A search on Spiceworks will also give you a good idea of the competence (and personality traits) of the people who work there.\n\nMy local managed provider of choice, Optrics, is on both lists. And no, this is not just a plug for my mates; these folks have certainly done well by me, but even skilled and experienced systems administrators need help from time to time.\n\nThere are plenty of aspects of IT that I don't have the experience to handle, or the time to learn. None of us can do it all, and none of us should be afraid to seek help if we feel our Server 2003 migrations involve a little more than we can handle.\n\nLarger companies are not likely to find what they need in the Spiceworks or Microsoft partner lists. The bigger you get the more you need to start engaging name brand managed providers.\n\nBefore hitting Google I recommend having a conversation with your application's developers first. Most problems in migrating away from Server 2003 are related to specific workload incompatibilities. Your application developer may know of a company with experience of handling migrations of its software. If there is such a team out there, go with the pros.\n\nIf what you need is help migrating the more in-depth Windows features and coping with esoteric issues, then any old managed IT service provider will not do. What you want is a Microsoft MVP.\n\nJuly 14 is coming up fast. Will you be ready in time?\n\nBiting the hand that feeds IT \u00a9 1998\u20132018",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.928453505,
        "format_confidence":0.8251049519
    },
    {
        "url":"http:\/\/www.zdnet.com\/article\/vormittag-erp-conference-highlightscustomer-insights\/",
        "text":"Vormittag ERP - Conference Highlights\/Customer Insights\n\nVAI may not be the best known ERP vendor out there but their customers sure like them. Read on for the highlights of their recent user conference.\n\nI recently attended the VAI (Vormittag Associates, Inc.)\u00a0 user conference in Las Vegas. It was my first opportunity to attend their show and to speak with their customers.\n\nVormittag VAI 120\nCopyright 2012 - TechVentive, Inc. - All Rights Reserved\n\nBy way of background, VAI makes a line of ERP software that is built and optimized for the IBM iSeries and related platforms. They\u2019ve been around a long time and their customer base has been quite loyal to the firm. The software is clearly marketed to the SMB\/Mid-Market space.\n\nSome of the bigger things to note:\n\n  \u2022 VAI customers have some of the smallest IT departments that mid-market firms would ever possess. This is a serendipitous result of the hardware this solution utilizes. Some of these firms have 1 person as their IT department (out of a total workforce of 115 employees). A $500 million revenue firm, for example, has only 10 IT staff. \u00a0It would be hard for any software vendor to displace VAI in these accounts using an economic or IT savings use case. Only a full multi-tenant cloud solution could stand a chance of replacing VAI (and even then, a vendor would have to mollify any conversion risk concerns of the client). \u00a0\n  \u2022 High customer satisfaction is a rare commodity in established software firms. The older a firm gets, the more common it is to find customers being subjected to frequent usage audits (to generate more inbound revenue for a flagging vendor), fewer net new software releases\/upgrades, etc. VAI has some really happy customers and this would, like the point above, make their customer base harder for competitors to siphon away.\n  \u2022 Cloud penetration into these accounts was scant. One customer reported being a user of salesforce.com. Payroll and some HR functions are often outsourced (the usual big players were indicated). One customer is using Ceridian\u2019s cloud solution (note: this could be a private label version of Ultimate\u2019s HR solution).\n\nIn my discussions with customers and management, I learned that:\n\n  \u2022 Customers appear to be quite pleased with the software. In my informal polling of them, I asked them to give VAI a customer satisfaction score (1-10 score with 10 being most satisfied).\u00a0 Most firms gave VAI a 9 with one firm offering up an 8. \u00a0\n  \u2022 Many users have customized the solution, some materially, and can often be several versions back from the current release.\n  \u2022 While some of the customers acquired VAI software over a decade ago, the competitor names that did surface included: JD Edwards (now part of Oracle), Epicor, Daly (www.daly.com), SAP and some vertical industry solutions.\n  \u2022 Many of the customers have had a long history with IBM hardware. Several of them can date their experience to System 3X, AS\/400 or iSeries gear.\n  \u2022 The reasons these firms moved to VAI included:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - Y2K replacement solution\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - Needed a post merger solution that supported multiple companies and locations\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 - Needed critical warehouse distribution software\n\nI also attended a couple of the vendor briefings and keynotes at the event. VAI announced new sales force functionality, new integration capabilities and several mobile enhancements.\n\nVormittag VAI\nCopyright 2012 - TechVentive, Inc. - All Rights Reserved\n\nIBM also presented. Their spokesman discussed how the new Power series solutions will permit simultaneous virtualized environments for both AIX and Windows environments. This would permit a customer to run their Microsoft applications (e.g., Office) and VAI software on 1 machine. There is a potential for cost savings for some firms in going with this approach.\n\nVormittag VAI 115\nCopyright 2012 - TechVentive, Inc. - All Rights Reserved\n\nI walked away from the event better aware of VAI. I also confirmed something that I\u2019ve known for a long time: sometimes the software companies that delight their customers the best aren\u2019t necessarily the biggest, oldest, newest, hottest, best known or the most bleeding-edge. If you value a positive long-term customer relationship, make sure you add the right questions, characteristics, etc. in your next software selection RFP. Customer references, Net Promoter Scores and other data points speak volumes about the future relationship you'll have with a vendor.\n\n\n\nYou have been successfully signed up. To sign up for more newsletters or to manage your account, visit the Newsletter Subscription Center.\nSee All\nSee All",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.586350143,
        "format_confidence":0.8248220682
    },
    {
        "url":"http:\/\/www.forbes.com\/2010\/02\/01\/ibm-software-security-technology-cio-network-danahy.html",
        "text":"Jack Danahy\n\nJack Danahy\n\n2\/01\/2010 @ 3:00PM\n\nA Security Prescription For 2010\n\nOne thing no one can dispute: the importance of a company\u2019s IT security. In the first half of 2009, more than half of all security vulnerabilities disclosed were related to Web applications. With security and compliance concerns growing every day, 2010 looks to be the year that will force organizations to adopt more proactive security strategies.\n\nThe good news is that many companies are already moving in this direction. Despite current economic conditions, security software budgets are projected to grow in 2010.\n\nIncreased budgets also mean that more decisions must be made. Organizations are under tremendous pressure to spend dollars wisely and achieve real results. While security teams may be overwhelmed by the increasing need to safeguard their information assets, chief executives are thinking of ways to cost-effectively ensure security across organizational, geographic and profit center borders.\n\nWhen offered new budget dollars and a new security initiative, many executives make the mistake of immediately creating a shopping list, simply focusing their efforts on deciding which products to buy. This is almost always the wrong first step to take.\n\nFirst, every organization needs to perform a frank self-evaluation. There are, at a minimum, four key steps every C-level executive must initially take to arrive at a strong security strategy.\n\nIdentify the motivating factors.\n\nA major mistake many business leaders make is to simply purchase products without first understanding why they are implementing a security strategy. Understanding your organization\u2019s motivation is the first step to quickly arriving at a strategy. If your organization\u2019s security has been breached, your strategy will need to address both the means of protecting against re-infection or re-emergence of remnants from the same exploitation, and a lens into the system that will more quickly identify other similar problems earlier in the cycle. Understanding the reasons why your organization is not secure enough is a good launching point.\n\nThere is a different set of steps to be taken if the motivation is derived from corporate, regulatory or board-level interest. If security has become a priority because of a directive and not a disaster, then the outputs will be very different. In this case, the strategy should start with understanding what those groups need from a security strategy. This can range from reporting capabilities to assertions or certification.\n\nDetermine what needs to be secured.\n\nA recurring theme in many unsuccessful security programs tends to be incompleteness. For example, a network segment has been left unprotected, a data element has accidentally revealed or a system has been left unpatched. This is commonly the result of a security program that has been conceived in broad terms, and lacks focus on specific areas of concern. The process of better understanding specifically what should be secured is an important one. For many, the initial reaction to figuring out what you are trying to secure is \u201cmy networks.\u201d However, other factors may eventually lead you to answer \u201cmy data,\u201d \u201cmy business,\u201d \u201cmy reputation\u201d or \u201cmy time.\u201d\n\nOnce this knowledge exists, the relationship between improvement of security and the disruption to the organization becomes much more positive. Better security becomes less expensive, less intrusive and more likely to provide the targeted protection that is required. As a result, gaining supporters and champions for your security implementation becomes easier, and also provides clear ways to measure results.\n\nThe biggest mistake that is made (and it is made quite frequently) is to over apply a new-found dedication to security. Heavy-handed approaches cause projects to get delayed and budgets to break. A more informed strategy can bring all the security that is needed, without seeking to disrupt all the processes and projects already underway.\n\nUnderstand the consequences of not acting.\n\nHuman nature inclines us to want to do the right thing. People ask advice about security, and endeavor to improve it, because they truly want to be better informed and do a better job. These lofty ambitions, however, can be brought to earth quickly by the practical realities and resistance that security improvement will typically meet. Security is not free, and good security is neither cheap nor convenient.\n\nThis collision of aspiration and implementation is made more pronounced by the natural zealotry of security professionals who have witnessed cybersecurity incidents and damages, and therefore advocate security over all other concerns.\n\nIt is very important to ask the question \u201cwhat will happen if we don\u2019t do this right,\u201d because knowing whether it is imperative or just interesting will make all of the difference.\n\nIf failure will mean the loss of jobs, revenue and reputation, there will be robust support for the person who wagers a career on creating an effective means of addressing security issues, even if those means are not easy.\n\nA successful approach cannot be found if the goal of better security is tainted with panic about the ever-present, ever-changing, and ever-increasing likelihood of a breach. That overreaction is probably the most debilitating mistake that can be made, and one that will magnify any weakness in the other steps.\n\nBe vigilant\u2013change is constant, and you must be willing to adapt.\n\nThe best security plan today will be an out-of-date embarrassment a couple of years from now if it is not designed to monitor the environment that it is meant to serve. For many organizations, this translates directly into a constant watch on the security landscape, looking for new threats, new breaches and new technologies. The most critical area, however, is the understanding of what is going on within the organization\u2019s own walls.\n\nNew applications, networks and the internal adoption of new technologies are much more likely to have a negative effect on a security posture than any external catalyst. The creation of new inroads into existing networks, the exposure of previously protected data and the introduction of Internet working applications and businesses, each offer up the opportunity to invalidate the best planned but brittle approach to security.\n\nAs areas of security planning are completed, they must be accompanied by the process through which they will be updated as IT evolves.\n\nThe Bottom Line\n\nSecurity is not strictly defined, so it must always be approached from a situational perspective. These four steps are clearly not the most specific, nor the most technical, but they are fundamental and too often completely ignored. Following this prescription as plans are made for 2010 and beyond will help to keep organizations in a healthy balance of security, profit, growth and accessibility.\n\nJack Danahy is a security executive in the Office of the CTO for IBM Rational software. He is the former chief technology officer and founder of Ounce Labs, which was acquired by IBM in July 2009. He is one of the industry\u2019s most prominent advocates for data privacy and application security. Danahy holds five patents in a variety of security technologies and is a contributor to industry and national security working groups on data privacy, security, cybersecurity and cyber threats. He also writes a blog called Smart Grid Security.\n\nSee Also:\n\nThe Not-All-Bad Security News\n\nHow Security Is Changing\n\nProfiting From The Google Hack\n\nComments are turned off for this post.",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.7170659304,
        "format_confidence":0.9775096774
    },
    {
        "url":"https:\/\/hackercombat.com\/understanding-business-intelligence-and-data-security\/",
        "text":"Understanding Business Intelligence and Data Security\n\nBI & Data Security\n\nBusiness leaders across the world are enjoying reduced costs, and increased efficiency, which is helping them in a better decision process. The new era of business intelligence strategy has ushered, that focus on data analysis. While most business leaders are expecting this to help them enhance their business operation, but very few of them consider the full implication of Business intelligence.\n\nVulnerabilities in exploring BI Security\n\nThe basic of security is that if you leave any data unattended it is going to cause you enough damage. When you churn out enough data and produce them at large scale, you need to have a system in place to keep them secured. Business Intelligence and data analysis are no longer related to the 1s and 0s, but sensitive data which have been always vulnerable to attacks by cybercriminals. If analyzed data can be stolen by the hackers, imagine when they have the key to the whole process, where they could just hijack the whole system. It\u2019s like a thief who enters a car garage to steal a few spare parts, he swindles of the whole car from the dealership\u2019s rack.\n\nEnsure the best Cloud Options\n\nThe first thing to protect is the very Business Intelligence tool that is in operation, and it requires a tight security. Nearly every organization considers cloud as an important part of the business and analytical strategy, despite the security concern they still go with it. Business Intelligence along with cloud option is a cost-effective way to use data strategically, but cloud providers need to chalk out a proper method that shows how they safeguard data. If you have an internal audit to look into the aspect of safety and if possible provide a secondary layer of security will be an added advantage.\n\nEncrypting Data\n\nKnowing the fact that hacker is always eyeing Business Intelligence more than anything, one should ensure that data is secure once it is unraveled. The data must be encrypted before it is stored or sent across to someone else, and doing so will put the hacker to look elsewhere. The IBM Z Machine is one such thing that allows you to encrypt massive data quickly, nearly 12 billion transactions a day. If this is ignored, you will pay a heavy price for the breach.\n\nIf you are able to segment your data storage and network, it could be a strategic move, because solid data can be safe. This way, if a hacker is able to get into one section, they will not be able to jump to the next since the medium to access is not available.\n\nAuthenticate Permission and access\n\nBusiness Intelligence is a powerful tool, and with power comes responsibility, so if the power falls into the wrong hand it can cause catastrophe. So it is always good to keep the access to a limited few, even if you feel it is safe to share with all employees or stakeholders. Ensure the items are shared on supervisory or management purpose. Audit the role of each and every user who is going to operate on a module or section of BI.\n\nData Security and BI\n\nMost of the business leaders think that business intelligence is about sourcing insight about their business, but most don\u2019t anticipate that BI can be used for security. Business Intelligence can use to analyze data which can discover network inconsistencies, network flaws, technical and other issues. This can be used as a real-time problem solver, and you can monitor data as it comes into the system and leaves your organization. So any dubious activity can be tracked and fixed thereupon. To quote we can say Equifax\u2019s breach, which the principles couldn\u2019t trace and it resulted in such a grave results, where data and personal information of millions of customers got compromised.\n\nOnce the core objective of having the business intelligence is achieved and it starts showing results, never go back to match it with your old pre-BI strategies. Not necessarily you are going to bring in better results with a legacy system, and it might open your data to threats. At the end of the day, companies sustain and becomes successful by keeping a tight shield between business intelligence and security.\n\n\n\nJulia Sowells165 Posts\n\nJulia Sowells has been a technology and security professional. For a decade of experience in technology, she has worked on dozens of large-scale enterprise security projects, and even writing technical articles and has worked as a technical editor for Rural Press Magazine. She now lives and works in New York, where she maintains her own consulting firm with her role as security consultant while continuing to write for Hacker Combat in her limited spare time.\n\n\nLeave a Comment\n\n\nWelcome! Login in to your account\n\nRemember me Lost your password?\n\nDon't have account. Register\n\nLost Password",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.5619956851,
        "format_confidence":0.5259463191
    },
    {
        "url":"http:\/\/www.ds410.com\/cybersecurity-a-serious-concern-for-businesses-both-big-and-small\/",
        "text":"Cybersecurity: A Serious Concern for Businesses Both Big and Small\n\nMany businesses believe that online security breaches are only a real problem for big businesses.\n\nCyber Crime\n\nYou only ever hear about cybercrime on the news when it happens to a business as big as Target or Home Depot.\n\nThis way of thinking seems logical on the surface: When international corporations like Target are apparently vulnerable to unauthorized access, why would any cybercriminal bother with a business that isn\u2019t worth the assets of a single Target store, or (as is the case for many smaller businesses and one-man operations) even just a single department within a single Target store?\n\nContrary to this belief, smaller organizations are actually more likely than large corporations to be the victim of a cyberattack. In 2014, 60% of cyberattacks targeted small to medium-sized businesses.\n\nDon\u2019t be surprised. While businesses the size of Target do have more assets for cybercriminals to take advantage of, they also have the resources to invest in the best security tools available and hire top-tier IT talent to secure their network. On the other hand, smaller businesses have fewer resources and are more likely to skimp on their IT budget and neglect their network.\n\nOver half of businesses that sustain significant data loss from a cyberattack go out of business within 6 months of their breach. Make yourself an easy target with a weak security solution and you\u2019ll eventually get hit too. All it takes is one successful attack to close you down for good.\n\n\nRead any article about the current threat landscape and it will boil down to this basic point: there are a lot of web-based threats out there, so many that it\u2019s stupid not to worry about network security.\n\nYou may be wondering, just how many threats is \u201ca lot\u201d?\n\nWell, allow us to quantify the problem for you: in 2014 alone over 300 million new pieces of malware were created. If that figure intimidates you, it should. This isn\u2019t a problem that antivirus software can solve alone. You need an expert that can help you navigate this crowded minefield and keep the integrity of your data intact.\n\nThe Dark Side of Mobilization\n\nMore and more businesses are mobilizing their workforces. The efficiency and convenience benefits of mobilization are clear, but there are also security concerns to consider.\n\n17% of all Android apps will infect your phone with malware. Putting that stat in context makes it even scarier: there are about 1.5 million apps available to Android users, which means about 255,000 Android apps, any one of which might be downloaded by one of your employees, are malicious.\n\nThere\u2019s a lot of potential for an employee\u2019s personal mobile device to be compromised, and depending on how much you\u2019ve invested in the cloud, this mobile device may be used to access business data and share files with co-workers, taking a virus that started as an employee\u2019s problem and making it your problem.\n\nIt isn\u2019t enough to just enable your employees to work remotely. You need to make sure to secure those remote communications, too.\n\nMore access points through the popularity of cloud computing and mobile devices make your network more vulnerable. Contact {company} at {phone} or {email} and take the first steps towards truly securing your network.\n\nConnect With Your New York City IT Team\n\n  \u2022 59 Franklin Street, Suite 6B,\n    New York, NY 10013\n  \u2022 (646) 583-0410",
        "topic_id":20,
        "format_id":11,
        "topic_confidence":0.8467761278,
        "format_confidence":0.5764611363
    }
]