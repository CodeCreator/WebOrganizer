[
    {
        "url":"http:\/\/2013.igem.org\/wiki\/index.php?title=Team:Wellesley_Desyne\/Eugenie&printable=yes",
        "text":"Team:Wellesley Desyne\/Eugenie\n\nFrom 2013.igem.org\n\nWellesley HCI iGEM Team: Welcome\n\n\n\nEugenie is a multi-touch application for designing biological circuits using the programming language Eugene. The visual language we created for Eugenie allows synthetic biologists to harness the power of Eugene without having to write Eugene code. The application consists of three representations of the circuit: \u201cTreeView,\u201d \u201cFlowView,\u201d and \u201cEugeneView.\u201d The user can search through Clotho and iGEM databases to find the biological parts desired and can drag them to the workspace, graphically create Eugene rules, and view live-updating Eugene code for the circuit. After the user is done specifying a circuit, she or he can then view the search results, which display the possible permutations of a circuit based on a collection of parts. This process is meant to be iterative, so users can go back and add more properties and behavior specifications if they would like to prune the results.\n\n\nThe design process is divided into three main views: TreeView, FlowView, and EugeneView. A user starts by searching for parts from the Part Registry or their Clotho database and bringing them into their worskspace. In the TreeView, users can specify the order and structure of their design. The behavior of parts in the device is specified using the FlowView. Users specify structure and behavior using a visual language that we developed specifically for the application; the language utilizes SBOL symbols, an open-source standard for in silico representation of genetic designs. The EugeneView updates automatically throughout the design process, and users may click on individual parts in order to see the corresponding code highlighted in the EugeneView. Finally, users may view and export their results to a CSV or Pigeon files in the ResultsView.\n\nCreate the structure of the circuit in the TreeView.\n\nDefine rules between parts in the FlowView.\n\nView live-updating Eugene code in the EugeneView.\n\nGenerate and export results in the ResultsView.\n\n\nWe implemented Eugenie using C#, XAML and the Surface SDK 2.0. We also used the MVC framework, Eugene for the creation of synthetic biology circuits, and Pigeon to display the circuits visually.\n\n\nWe had four goals in creating Eugenie:\n  \u2022 Implementing top-down design\n    We utilized the functionality of Eugene, a programming language that is used to implement top-down design of novel genetic circuits\u2014design that begins by specifying devices and then instantiating parts.\n  \u2022 Enhancing sensemaking\n    We implemented sliding panel views, which allow users the option to view multiple sets of information simultaneously or one set at a time, helping users understand the connection between the different types of presentation.\n  \u2022 Supporting resource integration\n    We give users the option to load parts from the parts registry, access parts in a Clotho database, or use a local database.\n  \u2022 Fostering collaboration\n    We used the Microsoft PixelSense, which allows multiple people to use Eugenie simultaneously.\n\n\nResults showing users' experience with Eugenie.\n\nWe conducted user studies of Eugenie with members of the BU and MIT iGEM teams as well as with Wellesley College students; the majority of the 15 participants were biology or other science majors. Our study began with a brief introduction to the Eugene programming language, the visual language we had created, and the Eugenie user interface. Users were first tasked with interpreting genetic devices from our visual language and then translating SBOL representations of genetic devices into our visual language.\n\nWe next asked participants to use Eugenie to specify one of the genetic devices they had translated. Post task, participants rated the ease of understanding the visual language and of using the program. We also asked participants for their feedback on how the program could be improved.\n\nThe majority of users found it easier to create a device using Eugenie than to draw one by hand. Some participants also remarked that the multiple panels were helpful for understanding the device. Overall, users' response to Eugenie was positive, with many testers believing that Eugenie can become a great tool to help other young synthetic biologists with their research work.\n\n\nFuture Work\n\nOur visual language\n  \u2022 Supplement part information with additional information from previous iGEM projects and other synthetic biology databases\n  \u2022 Allow users to access more parts from other synthetic biology databases\n  \u2022 Expedite the process for synthetic biologists to create genetic circuits\n  \u2022 Further incorporate the visual language we created into Eugenie's interface\n  \u2022 Develop a companion web application to enable widespread access to Eugenie",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.8620836139,
        "format_confidence":0.8247609138
    },
    {
        "url":"http:\/\/jmlr.csail.mit.edu\/papers\/v5\/lewis04a.html",
        "text":"RCV1: A New Benchmark Collection for Text Categorization Research\n\nDavid D. Lewis, Yiming Yang, Tony G. Rose, Fan Li; 5(Apr):361--397, 2004.\n\n\nReuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.\n\n\nHome Page\n\n\n\n\nEditorial Board\n\n\n\nOpen Source Software\n\n\n\n\nContact Us\n\nRSS Feed",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.589913249,
        "format_confidence":0.9820636511
    },
    {
        "url":"http:\/\/www.exploratoryprogramming.org\/",
        "text":"In any design or learning activity, exploration is a key component. Significant research and conventional wisdom show that the best way to achieve a high-quality design is to explore multiple variations and iteratively evaluate them. When novices learn a new skill or system, they must explore and practice the available options. Similarly, when experts try to understand and improve an existing design, they must explore different approaches to modifying its behavior. Unfortunately, exploration is risky, error-prone, and cumbersome using today\u2019s tools. For instance, when users decide their current design is not effective, the only mechanisms available for selectively backtracking out of changes are linear undo and version control, which make it difficult to isolate backtracking to specific edits, or else users must manually remove undesired edits, which is slow and fallible. Further, today\u2019s tools do not support comparing two variants of a design or combining elements from multiple variants. Research is showing that these manual processes inhibit exploration, making users and designs less effective.\n\nTo address these problems PIs from four partner institutions have come together to undertake a research program that is both broad and deep, focusing on the creation and management of variations during a system\u2019s implementation and evolution. The goal is to discover new theories, algorithms, visualizations, and tools that support variations in code. The team will evaluate all of their approaches through lab and field studies, and they will investigate how users can be educated in more effective ways to work with variations. Based on a choice calculus for representing variations in software, they will develop a theory for formally defining and reasoning about variations. They will leverage theories of human behavior such as Minimalist Learning, Attention Investment, and Information Foraging, to develop a theory of Variation Foraging. They will develop an infrastructure including multiple levels of transcripts of users\u2019 editing operations that will support a novel form of selective undo and enable users to investigate their existing variants, return to any previous variant, and mix and match elements from multiple variants. They will develop algorithms to enable recording of interactions with variants so they can be explored and reused to explore and test new variants; these recordings will be augmented with automatically created data to help users understand behaviors they have not explicitly explored. Using this infrastructure the PIs will invent visualizations, search facilities, and interaction techniques that provide effective ways for users to find, understand, explore, reuse and create variants, and be able to ask \u201cwhy\u201d questions to understand the differences among variations of a system. For novices, an \u201cIdea Garden\u201d will help them explore new strategies for identifying which variations can help solve a problem and how to implement them.\n\nBroader Impacts: This research will enhance infrastructure for research and education by producing an integrated, open source web development environment for use by researchers and the world. The work will therefore benefit society by empowering the tens of millions of end-user programmers to creatively build content and applications for the web. The PIs will advance discovery while promoting learning by integrating their research into undergraduate courses on creativity and software engineering, and by supporting summer camps for at least 300 high school students per year. Project outcomes will be disseminated to researchers through publications and presentations, to computing educators through the above-mentioned camps and the National Girls Collaborative Project, and through public deployment. The PIs expect high interest because the work will be based on JavaScript, which is today\u2019s most popular programming language and for which there is a high demand for better tools. The research will address underrepresentation via its focus on investigating how to support both male and female end-user programmers, by involving high-school members of underrepresented groups, and by engaging many of the PI\u2019s female students.",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.9733943939,
        "format_confidence":0.5128951073
    },
    {
        "url":"http:\/\/www.techrepublic.com\/resource-library\/whitepapers\/msapriori-using-total-support-tree-data-structure\/",
        "text":"Big Data\n\nMSApriori Using Total Support Tree Data Structure\n\nDate Added: Apr 2012\nFormat: PDF\n\nAssociation rule mining is one of the important problems of data mining. Single minimum support based approaches of association rule mining suffers from \"rare item problem\". An improved approach MSApriori uses multiple supports to generate association rules that consider rare item sets. Necessity to first identify the \"Large\" set of items contained in the input dataset to generate association rules results in high storage and processing time requirement. The proposed work overcomes this drawback by storing items and their support values as total support tree data structure, resulting in an algorithm that is more efficient than existing algorithm both in terms of memory requirement as well as in processing time.",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.7596805096,
        "format_confidence":0.6951265335
    },
    {
        "url":"https:\/\/credential.eu\/publications\/d4-4-guidelines-for-secure-authentication-to-the-cloud\/",
        "text":"D4.4 Guidelines for Secure Authentication to the Cloud\n\nContributing Partners\n\n\nExecutive Summary\nDuring its development, the CREDENTIAL project had to face important issues related to privacy-preserving data, identity provision, authentication data, and their contextualization within a cloud environment. Studying the most successful technologies in the field helped the team in taking the sound decisions. The present document shows the in-depth study carried out throughout the project, with a particular focus on the \u201cauthentication to the cloud\u201d topic, leveraging the lesson learned during the CREDENTIAL initiative.\nFirstly, requirements related to the authentication-to-the-cloud are analysed from a general perspective, studying security and privacy topics, and the cloud requirements more particularly. A drill down of the CREDENTIAL requirements follows, with the aim of exposing what aspects could be improved.\nFurther on, the document focuses on the two most important strong authentication\/authorization frameworks, namely, FIDO and OATH. Both of them can provide a second strong authentication factor, which was missing in the original. FIDO resulted being our favourite methodology.\nThe document then shows a review of state of the art authentication techniques, such as biometric mechanisms, hardware solutions, and privacy-enhancing technologies.\nA technical assessment on how to accomplish a secure authentication-to-the-cloud is then carried out, conforming to FIDO Alliance guidelines, followed by a list of improvements to the cloud authentication process, which takes into account all the previous research work.\nFinally, depict enhanced CREDENTIAL scenarios where those improvements could be used.",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.5931373239,
        "format_confidence":0.8341113925
    },
    {
        "url":"https:\/\/elib.dlr.de\/119971\/",
        "text":"DLR-Logo -> http:\/\/www.dlr.de\nDLR Portal Home | Imprint | Privacy Policy | Contact | Deutsch\nFontsize: [-] Text [+]\n\nAn Eclipse Based Software Solution for Space Mission Design\n\nFischer, Philipp M. (2011) An Eclipse Based Software Solution for Space Mission Design. EclipseCon Europe 2011, 2.-4. Nov. 2011, Ludwigsburg, Germany.\n\n[img] PDF\n\nOfficial URL: https:\/\/www.eclipsecon.org\/europe2011\/sessions\/eclipse-based-software-solution-space-mission-design.html\n\n\nMany experts are needed to design a spacecraft. Each of them is responsible for a specific sub system, such as power, propulsion or configuration. These subsystems have strong dependencies among each other. For example, just a small change in the spacecraft's mass can lead to a change in its size. Changing the size, however, may require a different rocket to launch the spacecraft, which in turn may not be able to reach the designated orbit. In the classical, serial design approach, all experts worked on their specific part of the problem on their own. When finished, they passed on their results to the next expert until everyone had contributed to the overall design. As a consequence, several months were needed to complete a design. Today, spacecraft design is based on concurrent engineering (CE). All subsystem experts work on their part of the problem in parallel. Additionally, they continuously verify that their work does not conflict with the design choices by others. For this kind of cooperative work, the German Aerospace Center (DLR) operates a Concurrent Engineering Facility (CEF). At the CEF the experts meet for design sessions of typically two weeks' time. Each workplace is equipped with a computer that runs the design tools required by the corresponding subsystem. All computers share their results via a central data repository. So far, Excel used to be the standard tool for data collection and exchange. This choice, however, proved to be unsatisfactory in various ways. For example, Excel did not support the design data exchange between disciplines well. For this reason, DLR started a software development based on the Eclipse Rich Client Platform. Although this platform provides little support for the specific engineering tasks of the experts, many open source tools for concurrent and distributed work have been developed over the years, and Eclipse integrates many of them into a smooth workflow. This makes Eclipse a perfect match and baseline for the DLR design tool for space missions. The core of the new software is an EMF-based data model that serves two purposes: First, it is used as starting point for Model-Based Software Engineering, where a range of EMF-related features are very useful, such as the EMF Edit Framework and XML serialization. Second, the data model supports Model-Based System Engineering by providing semantics and consistency to the expert's data. Thus it can be reused for parameterizing further engineering tools such as simulations of the subsystems or the whole spacecraft. In a CE session all experts use local instances of the design software. The output of each instance contributes to the common, distributed data model which is shared by a Subversion server in the background. The details of the data exchange are hidden from the experts. They only see two functions in the user interface for \"update\" and \"commit\", which they use whenever necessary to contribute changes to the study, or to receive updates from others. As compared to the Excel-based tool, the new CEF software proved to be superior in terms of functionality, reliability and extensibility. In this talk, we give an overview of our software development, with emphasis on the data exchange among the experts. We comment on the strengths and weaknesses of Eclipse in respect of our work.\n\nItem URL in elib:https:\/\/elib.dlr.de\/119971\/\nDocument Type:Conference or Workshop Item (Speech)\nTitle:An Eclipse Based Software Solution for Space Mission Design\nAuthorsInstitution or Email of AuthorsAuthor's ORCID iD\nFischer, Philipp M.Philipp.Fischer (at) dlr.dehttps:\/\/orcid.org\/0000-0003-2918-5195\nRefereed publication:No\nOpen Access:Yes\nGold Open Access:No\nIn ISI Web of Science:No\nKeywords:Eclipse, CEF, Virtual Satellite\nEvent Title:EclipseCon Europe 2011\nEvent Location:Ludwigsburg, Germany\nEvent Type:international Conference\nEvent Dates:2.-4. Nov. 2011\nOrganizer:The Eclipse Foundation\nHGF - Research field:Aeronautics, Space and Transport\nHGF - Program:Space\nHGF - Program Themes:Space System Technology\nDLR - Research area:Raumfahrt\nDLR - Program:R SY - Space System Technology\nDLR - Research theme (Project):R - Virtueller Satellit (old)\nLocation: Braunschweig\nInstitutes and Institutions:Institut of Simulation and Software Technology > Software for Space Systems and Interactive Visualisation\nDeposited By: Fischer, Philipp M.\nDeposited On:20 Jul 2018 09:59\nLast Modified:31 Jul 2019 20:17\n\nRepository Staff Only: item control page\n\nHelp & Contact\nelectronic library is running on EPrints 3.3.12\nCopyright \u00a9 2008-2017 German Aerospace Center (DLR). All rights reserved.",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.8083748221,
        "format_confidence":0.9579894543
    },
    {
        "url":"https:\/\/repository.lib.ncsu.edu\/handle\/1840.16\/2772",
        "text":"Caching Strategies for More Efficient Generational Garbage Collection\n\nShow full item record\n\nTitle: Caching Strategies for More Efficient Generational Garbage Collection\nAuthor: Krishnakumar, Sree Vidhya Lakshmi\nAdvisors: Yan Solihin, Committee Member\nVincent W. Freeh, Committee Member\nEdward F. Gehringer, Committee Chair\nAbstract: Until the advent of generational garbage collection, page faults caused by garbage collection were a major source of bottleneck. Generational garbage collectors, by collecting smaller regions of the heap called generations, reduce the memory footprint of the collector and therefore the number of page faults caused by it. With page faults out of the way, the focus now is on cache misses due to garbage collection. The gap between the processor and memory cycle time is widening each year. Projections indicate that this increase is likely to continue. This makes cache performance an attractive area to study in order to tune the performance of a program. In one such study, a strategy has been proposed to improve cache performance of generational garbage collectors by pinning the youngest generation in the L2 cache. In this thesis, we study an anomaly associated with this strategy, and propose a new realization of the pinning strategy that removes this anomaly, thereby making it more attractive. We apply the idea of an SNT (selectively non-temporal) cache to garbage collection. This helps reduce cache pollution and conflict misses in a direct mapped cache due to non-temporal accesses during garbage collection. Simulation results show an average miss-rate reduction of 10% for 16 KB and 32 KB direct mapped L1 caches with SNT support. The improvement is greater in benchmarks with a large amount of live data.\nDate: 2004-01-08\nDegree: MS\nDiscipline: Computer Science\n\nFiles in this item\n\nFiles Size Format View\netd.pdf 335.9Kb PDF View\/Open\n\nThis item appears in the following Collection(s)\n\nShow full item record",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.6341122985,
        "format_confidence":0.9259245396
    },
    {
        "url":"http:\/\/research.cs.wisc.edu\/techreports\/viewreport.php?report=160",
        "text":"Computer Sciences Dept.\n\nThe Evolution of a Compiler: PUFFT 1964-1972\n\nE.J. Desautels, M.D. Shapiro\n\nThe evolution of a compiler: PUFFT 1964-1972. The Purdue University Fast Fortran Translator PUFFT went into operation on a 7090 in 1964. It has since then been used in many environments and subject to many extensions and improvements. These changes and improvements are described, along with the reasons for making them. The use of PUFFT demonstrates that second-generation equipment may find its useful life prolonged, particularly at educational institutions seeking low cost processing of large numbers of jobs.\n\nDownload this report (PDF)\n\nReturn to tech report index\n\nComputer Science | UW Home",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.8519564867,
        "format_confidence":0.9146071672
    },
    {
        "url":"http:\/\/aaai.org\/Library\/ICAPS\/2008\/icaps08-038.php",
        "text":"Realizing Multiple Autonomous Agents through Scheduling of Shared Devices\n\nSebastian Sardina, Giuseppe De Giacomo\n\nImagine a collection of available devices, such as a camera, a vacuum cleaner, or robotic arm, each of which is able to act (that is, perform actions) according to a given behavior specification, expressed as a finite transition system. Imagine next a set of virtual independent and autonomous agents, such as a surveillance agent or a cleaning agent, which are meant to operate concurrently, each within a given specification of its capabilities, again expressed as a finite transition system. The question then is: can we guarantee the realization of every agent by intelligently scheduling the available devices while fully preserving the agents' autonomy? In this paper, we define the problem formally, and propose a technique to actually generate a solution by appealing to recent results in LTL-based synthesis of reactive systems. We also show that the proposed technique is optimal with respect to computational complexity.\n\n\nSubjects: 7.1 Multi-Agent Systems; 11. Knowledge Representation\n\nSubmitted: Jun 27, 2008\n\nThis page is copyrighted by AAAI. All rights reserved. Your use of this site constitutes acceptance of all of AAAI's terms and conditions and privacy policy.",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.6063557267,
        "format_confidence":0.9837995768
    },
    {
        "url":"https:\/\/link.springer.com\/book\/10.1007%2Fb96106",
        "text":"Integrated Formal Methods\n\n4th International Conference, IFM 2004, Cnaterbury, UK, April 4-7, 2004. Proceedings\n\n  \u2022 Eerke\u00a0A.\u00a0Boiten\n  \u2022 John\u00a0Derrick\n  \u2022 Graeme\u00a0Smith\nPart of the Lecture Notes in Computer Science book series (LNCS, volume 2999)\n\nTable of contents\n\n  1. Front Matter\n  2. Invited Talks\n\n    1. Thomas Ball, Byron Cook, Vladimir Levin, Sriram K. Rajamani\n      Pages 1-20\n    2. Richard J. Boulton, Hanne Gottliebsen, Ruth Hardy, Tom Kelsey, Ursula Martin\n      Pages 21-35\n  3. Tutorial\n\n  4. Contributed Papers\n\n    1. Bill J. Ellis, Andrew Ireland\n      Pages 67-86\n    2. Steve Schneider, Helen Treharne\n      Pages 87-107\n    3. Adalberto Farias, Alexandre Mota, Augusto Sampaio\n      Pages 108-127\n    4. Sagar Chaki, Edmund M. Clarke, Jo\u00ebl Ouaknine, Natasha Sharygina, Nishant Sinha\n      Pages 128-147\n    5. Kirsten Winter\n      Pages 148-167\n    6. Jin Song Dong, Shengchao Qin, Jun Sun\n      Pages 168-186\n    7. K. Lano, D. Clark, K. Androutsopoulos\n      Pages 187-206\n    8. Tiberiu Seceleanu, Juha Plosila\n      Pages 227-246\n    9. Michael M\u00f6ller, Ernst-R\u00fcdiger Olderog, Holger Rasch, Heike Wehrheim\n      Pages 267-286\n    10. C\u00e9cile Bui Thanh, Hanna Klaudel\n      Pages 287-306\n    11. Gabriel Ciobanu, Dorel Lucanu\n      Pages 307-327\n    12. Tim A. C. Willemse\n      Pages 343-362\n\nAbout these proceedings\n\n\nThe fourth conference in the series of international meetings on Integrated F- mal Methods, IFM, was held in Canterbury, UK, 4\u20137 April 2004. The conference was organized by the Computing Laboratory at the University of Kent, whose main campus is just outside the ancient town of Canterbury, part of the county of Kent. Kent is situated in the southeast of England, and the university sits on a hill overlooking the city of Canterbury and its world-renowned cathedral. The UniversityofKentwasgranteditsRoyalCharterin1965.Todaytherearealmost 10,000 full-time and part-time students, with over 110 nationalities represented. The IFM meetings have proven to be particularly successful. The ?rst m- ting was held in York in 1999, and subsequently we held events in Germany in 2000, and then Finland in 2002. The conferences are held every 18 months or so, and attract a wide range of participants from Europe, the Americas, Asia and Australia. The conference is now ?rmly part of the formal methods conference calendar. The conference has also evolved in terms of themes and subjects - presented, and this year, in line with the subject as a whole, we saw more work on veri?cation as some of the challenges in this subject are being met. The work reported at IFM conferences can be seen as part of the attempt to manage complexity by combining paradigms of speci?cation and design, so that the most appropriate design tools are used at di?erent points in the life-cycle.\n\n\nUnified Modeling Language (UML) automata calculus component systems formal method formal methods logic model checking modeling object-oriented modeling process algebra program analysis programming refinement verification\n\nEditors and affiliations\n\n  \u2022 Eerke\u00a0A.\u00a0Boiten\n    \u2022 1\n  \u2022 John\u00a0Derrick\n    \u2022 2\n  \u2022 Graeme\u00a0Smith\n    \u2022 3\n  1. 1.Computing LaboratoryUniversity of KentCanterbury, KentUK\n  2. 2.Department of ComputingUniversity of SheffieldSheffieldUK\n  3. 3.School of Information Technology and Electrical EngineeringThe University of QueenslandAustralia\n\nBibliographic information\n\n  \u2022 DOI\n  \u2022 Copyright Information Springer-Verlag Berlin Heidelberg 2004\n  \u2022 Publisher Name Springer, Berlin, Heidelberg\n  \u2022 eBook Packages Springer Book Archive\n  \u2022 Print ISBN 978-3-540-21377-2\n  \u2022 Online ISBN 978-3-540-24756-2\n  \u2022 Series Print ISSN 0302-9743\n  \u2022 Series Online ISSN 1611-3349\n  \u2022 About this book",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.8892767429,
        "format_confidence":0.7449761629
    },
    {
        "url":"http:\/\/www.actapress.com\/Abstract.aspx?paperId=37917",
        "text":"Constructing Non-Dominated m-Group Coteries for Group Mutual Exclusion\n\nT. Harada and M. Yamashita (Japan)\n\n\nm-group coteries, Non-dominatedness\n\n\nA typical group mutual exclusion algorithm among m groups makes use of an m-group coterie, which determines the performance of the algorithm. There are two main performance measures: The availability is the probability that an algorithm tolerates process crash failures, and the concurrency is the number of processes that it allows simultaneous access to the resources. Since non-dominated (ND, for short) m-group coteries (locally) maximize the availability and their degrees roughly correspond to the concurrency, methods to construct ND m-group coteries with large degrees are looked for. Nevertheless, only a few naive methods have been proposed. This paper presents three methods to construct desirable m-group coteries. The \ufb01rst method constructs an ND m-group coterie from a dominated one using the transversal composition. The second one constructs an ND (m \u2212 1)-group coterie from an ND m-group coterie. The last one uses the coterie join operation to produce an ND m-group coterie from an ND coterie and another ND m group coterie. These methods preserve the degrees of the original m-group coteries.\n\nImportant Links:\n\nGo Back",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.7006387711,
        "format_confidence":0.9807474017
    },
    {
        "url":"https:\/\/segarage.org\/papers\/64",
        "text":"Stochastic optimization of program obfuscation\n\nAuthor email: jy1989@mails.tsinghua.edu.cn\nTool name: Closure\nDescription: Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques. This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = \u3008t2, t2, ... , tn\u3009 (\u2200i \u03b5 [1, n]. ti \u03b5 T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness\/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq. We have realized the framework in a tool Closure* for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure* outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure* is practical and can reduce the human attack success rate by 30%.\nBibtex: @inproceedings{10.1109\/ICSE.2017.28, author = {Liu, Han and Sun, Chengnian and Su, Zhendong and Jiang, Yu and Gu, Ming and Sun, Jiaguang}, title = {Stochastic Optimization of Program Obfuscation}, year = {2017}, isbn = {9781538638682}, publisher = {IEEE Press}, url = {https:\/\/doi.org\/10.1109\/ICSE.2017.28}, doi = {10.1109\/ICSE.2017.28}, booktitle = {Proceedings of the 39th International Conference on Software Engineering}, pages = {221\u2013231}, numpages = {11}, keywords = {program obfuscation, obscurity language model, markov chain monte carlo methods}, location = {Buenos Aires, Argentina}, series = {ICSE \u201917} }\nLink to public pdf: https:\/\/dl.acm.org\/doi\/abs\/10.1109\/ICSE.2017.28\nLink to tool webpage: https:\/\/bitbucket.org\/njaliu\/closure-star-tool\nLink to demo: Not provided by authors\nCategory: None\nYear and Conference: 2017, ICSE\nTerms of use",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.9803661704,
        "format_confidence":0.7522243261
    },
    {
        "url":"https:\/\/rycolab.io\/publication\/meisteral-tacl-20\/",
        "text":"Best-First Beam Search\n\n\nDecoding for many NLP tasks requires a heuristic algorithm for approximating exact search since the full search space is often intractable if not simply too large to traverse efficiently. The default algorithm for this job is beam search\u2014a pruned version of breadth-first search\u2014which in practice, returns better results than exact inference due to beneficial search bias. In this work, we show that standard beam search is a computationally inefficient choice for many decoding tasks; specifically, when the scoring function is a monotonic function in sequence length, other search algorithms can be used to reduce the number of calls to the scoring function (e.g., a neural network), which is often the bottleneck computation. We propose best-first beam search, an algorithm that provably returns the same set of results as standard beam search, albeit in the minimum number of scoring function calls to guarantee optimality (modulo beam size). We show that best-first beam search can be used with length normalization and mutual information decoding, among other rescoring functions. Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar search bias in terms of downstream performance, but runs in a fraction of the time.\n\nTransactions of the Association for Computational Linguistics",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.689227879,
        "format_confidence":0.9869913459
    },
    {
        "url":"http:\/\/shodhganga.inflibnet.ac.in:8080\/jspui\/handle\/10603\/17461?mode=full",
        "text":"\nFull metadata record\nDC FieldValueLanguage\ndc.coverage.spatialComputer Scienceen_US\ndc.description.abstractLogic Programming languages use a uniform formulation to represent data, program, queries and newline- newlineintegrity constraint. Answering a query in a such systems is more of computation than a mere newlineretrieval of information. Only positive information can be logical consequences of a program, newlinetherefore special rules are needed to deduce negative information. Logic programs lack sufficient newlineexpressiveness for many situations. Explicit representation of negative information in logic newlineprogramming has not been feasible in many applications. The techniques for knowledge newlinerepresentation in logic programming systems can not be isolated from semantics for Logic newlinePrograms. newlineTraditionally, negation is discussed in the context of databases. However, there are several ways in newlinewhich negation can be added to clauses in first order logic. In order of increasing complexity, we newlinecould allow negated atoms in the queries, the body of the clauses or the head of clauses. In order to newlineaddress these problems to some extent we extend a class of Hom clauses that include a form of newlinenegation close to CW A called Generally Hom Logic Programs (GHLP). Covering is the basis of newlineprocedural semantics of extended program clauses. With the use of classical negation we attempt newlineto encode negati ve information for a common sense reasoning system. The operational semantics of newlinethis extension depends on the SLDNFS-resolution and Intentional Negation newlineIt is clear that some recent attempts to formulate semantics for programs with negation have newlineunderlying intuitions inspired by nonmonotonic reasoning. The use of negation or dealing with newlinenegative conditions cause nonmonotonicity in the system that rules out fixed point semantics newlineapproaches. We select rule based formulations to incorporate negative information in logic newlineprogramming system. The Censored Production Rules (CPR) and Hierarchical Censored newlineProduction Rules (HCPR) are the two formulations that inculcate features of Variable Precision newlineLogic (VPL) in a logic system.en_US\ndc.titleNegation in logic programming systemsen_US\ndc.creator.researcherMinz, Sonajhariaen_US\ndc.subject.keywordComputer Scienceen_US\ndc.subject.keywordSystem Scienceen_US\ndc.subject.keywordprogramming systemsen_US\ndc.description.noteBibliography p.134-141en_US\ndc.contributor.guideBharadwaj, K Ken_US\ndc.publisher.universityJawaharlal Nehru Universityen_US\ndc.publisher.institutionSchool of Computer and System Scienceen_US\nAppears in Departments:School of Computer and System Science\n\nFiles in This Item:\nFile Description SizeFormat\u00a0\n01_title.pdfAttached File100.57 kBAdobe PDFView\/Open\n02_certificate.pdf106.62 kBAdobe PDFView\/Open\n03_dedication.pdf50.8 kBAdobe PDFView\/Open\n04_abstract.pdf131.06 kBAdobe PDFView\/Open\n05_acknowledgements.pdf148.7 kBAdobe PDFView\/Open\n06_contents.pdf183.55 kBAdobe PDFView\/Open\n07_list of figures.pdf178.53 kBAdobe PDFView\/Open\n08_list of tables.pdf18.92 kBAdobe PDFView\/Open\n09_chapter 1.pdf499.95 kBAdobe PDFView\/Open\n10_chapter 2.pdf824.6 kBAdobe PDFView\/Open\n11_chapter 3.pdf1.04 MBAdobe PDFView\/Open\n12_chapter 4.pdf1.41 MBAdobe PDFView\/Open\n13_chapter 5.pdf666.77 kBAdobe PDFView\/Open\n14_chapter 6.pdf350.91 kBAdobe PDFView\/Open\n15_bibliography.pdf350.42 kBAdobe PDFView\/Open\n\nItems in Shodhganga are protected by copyright, with all rights reserved, unless otherwise indicated.\n\nAltmetric Badge:",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.9100905657,
        "format_confidence":0.8546619415
    },
    {
        "url":"http:\/\/reports-archive.adm.cs.cmu.edu\/anon\/isr2010\/abstracts\/10-125R2.html",
        "text":"Institute for Software Research\nSchool of Computer Science, Carnegie Mellon University\n\n\n\u00b5\u00c6minium Language Specification\n\nSven Stork, Jonathan Aldrich, Paulo Marques\n\nFebruary 2012\n\n\nThis report updates CMU-ISR-10-125R to\nreflect the renaming of share blocks to split blocks.\n\nKeywords: Programming languages, concurrency, access permissions\n\nWriting concurrent applications is extremely challenging, not only in terms of producing bugfree and maintainable software, but also for enabling developer productivity. In this paper we present \u00b5\u00c6minium: a core calculus for the \u00b5\u00c6minium concurrent-by-default programming language. Using \u00b5\u00c6minium programmers express data dependencies rather than control flow between instructions. Dependencies are expressed using permissions, which are used by the type system to automatically parallelize the application. The \u00b5\u00c6minium approach provides a modular and composable mechanism for writing concurrent applications, provably preventing data races. This allows programmers to shift their attention from low-level, error-prone reasoning about thread interleaving and synchronization to focus on the core functionality of their applications.\n\n59 pages\n\nReturn to: SCS Technical Report Collection\nSchool of Computer Science homepage\n\nThis page maintained by",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.9832547903,
        "format_confidence":0.9695890546
    },
    {
        "url":"https:\/\/ru.scribd.com\/document\/370650531\/A-Review-of-Elastic-Search-Performance-Metrics-and-challenges",
        "text":"\u0412\u044b \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0435\u0441\u044c \u043d\u0430 \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u0435: 1\u0438\u0437 8\n\nInternational Journal on Recent and Innovation Trends in Computing and Communication ISSN: 2321-8169\n\nVolume: 5 Issue: 11 222 \u2013 229\n\n\nA Review of Elastic Search: Performance Metrics and challenges\n\nSubhani Shaik1 Nallamothu Naga Malleswara Rao2\n\nResearch scholar, Department of CSE, Professor, Department of IT,\nAcharya Nagarjuna University ,, RVR & JC College of Engineering,\nGuntur, A.P, India. Chowdavaram, Guntur, A.P,India.\n\nAbstract: The most important aspect of a search engine is the search. Elastic search is a highly scalable search engine that stores data in a\nstructure, optimized for language based searches. When it comes to using Elastic search, there are lots of metrics engendered. By using Elastic\nsearch to index millions of code repositories as well as indexing critical event data, you can satisfy the search needs of millions of users while\ninstantaneously providing strategic operational visions that help you iteratively improve customer service. In this paper we are going to study\nabout Elastic searchperformance metrics to watch, important Elastic search challenges, and how to deal with them. This should be helpful to\nanyone new to Elastic search, and also to experienced users who want a quick start into performance monitoring of Elastic search.\n\nKeywords: Elastic search, Query latency, Index flush, Garbage collection, JVM metrics, Cache metrics.\nnode is also able to function as a data node. In order to\n1. INTRODUCTION: improve reliability in larger clusters, users may launch\ndedicated master-eligible nodes that do not store any data.\nElastic search is a highly scalable, distributed, open source\nRESTful search and analytics engine. It is multitenant-capable a. Data nodes\nwith an HTTP web interface and schema-free JSON Every node that stores data in the form of index and performs\ndocuments. Based on Apache Lucene, Elastic search is one of actions related to indexing, searching, and aggregating data is\nthe most popular enterprise search engines today and is a data node. In larger clusters, you may choose to create\ncapable of solving a growing number of use cases like log dedicated data nodes by adding node.master: false to the\nanalytics, real-time application monitoring, and click stream config file, ensuring that these nodes have enough resources to\nanalytics. Developed by Shay Banon and released in 2010, it handle data-related requests without the additional workload\nrelies heavily on Apache Lucene, a full-text search engine of cluster-related administrative tasks.\nwritten in Java.Elastic search represents data in the form of\nstructured JSON documents, and makes full-text search b. Client nodes\naccessible via RESTful API and web clients for languages like Client nodeis designed to act as a load balancer that helps\nPHP, Python, and Ruby. It\u2019s also elastic in the sense that it\u2019s route indexing and search requests. Client nodes help to bear\neasy to scale horizontally\u2014simply add more nodes to some of the search workload so that data and master-eligible\ndistribute the load. Today, many companies, including nodes can focus on their core tasks.\nWikipedia, eBay, GitHub, and Datadog, use it to store, search,\nand analyze large amounts of data on the fly.\n\nIn Elastic search, a cluster is made up of one or more\nnodes.Each node is a single running instance of Elastic search,\nand its elasticsearch.yml configuration file designates which\ncluster it belongs to (cluster.name) and what type of node it\ncan be. Any property, including cluster name set in the\nconfiguration file can also be specified via command line\nargument. The three most common types of nodes in Elastic\nsearch are:\n\n2.1 Master-eligible nodes\n\nEvery node in Elastic search is master-eligible by default\nunless otherwise specified. Each cluster automatically elects a\nmaster node from all of the master-eligible nodes. The master\nnode is responsible for coordinating cluster tasks like\ndistributing shards across nodes, and creating and deleting\nindices. If the current master node experiences a failure\nmaster-eligible nodes elect a new master. Any master-eligible Fig:1 Elastic Search Cluster\nIJRITCC | November 2017, Available @ http:\/\/www.ijritcc.org\nInternational Journal on Recent and Innovation Trends in Computing and Communication ISSN: 2321-8169\nVolume: 5 Issue: 11 222 \u2013 229\nIn Elasticsearch, interrelated data is stored in the same index\nthatcontains a set of related documents in JSON format.\nElasticsearch\u2019s secret sauce for full-text search is Lucene\u2019s\ninverted index. When a document is indexed, Elasticsearch\nautomatically creates an inverted index for each field; the\ninverted index maps terms to the documents that contain those\nterms.An index is stored across one or more primary shards,\nand zero or more replica shards, and each shard is a complete\ninstance of Lucene, like a mini search engine.\nWhen creating an index, we can specify the number of primary\nshards, as well as the number of replicas per primary. The\ndefaults are five primary shards per index, and one replica per\nprimary. The number of primary shards cannot be changed\nonce an index has been created. The number of replicas can be\nFigure 3: Processing of Search Request\nupdated later on as needed. To protect against data loss, the\nmaster node ensures that each replica shard is not allocated to\nIf search is a customer-facing feature you should monitor\nthe same node as its primary shard.\nquery latency and take action if it surpasses a threshold. It\u2019s\nimportant to monitor relevant metrics about queries and\nfetches that can help you determine how your searches\nperform over time. For example, you may want to track spikes\nand long-term increases in query requests, so that you can be\nprepared to tweak your configuration to optimize for better\nperformance and reliability.\n\nSearch performance metrics\n\n\uf0b7 Query load: Monitoring the number of queries currently in\nprogress can give you a rough idea of how many requests your\ncluster is dealing with at any particular moment in time.\nConsider alerting on unusual spikes or dips that may point to\nFig:2 Elastic search data rganization underlying problems. You may also want to monitor the size\n4. of the search thread pool queue.\nMETMETRICS : \uf0b7 Query latency: Though Elasticsearch does not explicitly\nElasticsearch provides plenty of metrics to detect problems provide this metric, monitoring tools can help you use the\nlike unreliable nodes, out-of-memory errors, and long garbage available metrics to calculate the average query latency by\ncollection times. All these metrics are accessible via sampling the total number of queries and the total elapsed time\nElasticsearch\u2019s API as well as single-purpose monitoring tools at regular intervals. Set an alert if latency exceeds a threshold,\nlike Elastic\u2019s Marvel and universal monitoring services like and if it fires, look for potential resource bottlenecks, or\nDatadog. investigate whether you need to optimize your queries.\n\n4.1Search and indexing performance \uf0b7 Fetch latency: The fetch phase, should typically take much\nIn Elasticsearch we have two types of requests, the search less time than the query phase. If this metric isconstantly\nrequests and index requests which aresimilar to read and write increasing, this could indicate a problem with slow\nrequests in a traditional database system. disks, enriching of documents (highlighting relevant text in\nsearch results, etc.), or requesting too many results.\n4.1.1 Search Request:\n\uf0b7 Client sends a search request to Node 2 4.1.2 Index Requests\n\uf0b7 The coordinating node, Node 2 sends the query to a Indexing requests are similar to write requests in a traditional\ncopy of every shard in the index. database system. If your Elasticsearch workload is write-\n\uf0b7 Each shard executes the query locally and delivers heavy, it\u2019s important to monitor and analyze how effectively\nresults to Node 2. Node 2 sorts and compiles them you are able to update indices with new information. When\ninto a global priority queue. new information is added to an index, or existing information\n\uf0b7 Node 2 finds out which documents need to be fetched is updated or deleted, each shard in the index is updated via\nand sends a multi GET request to the relevant two processes: refresh and flush.\n\uf0b7 Each shard loads the documents and returns them to \uf0b7 Index refresh\nNode 2. Newly indexed documents are not immediately made available\n\uf0b7 Node 2 delivers the search results to the client. for search. First they are written to an in-memory buffer where\nthey await the next index refresh, which occurs once per\nIJRITCC | November 2017, Available @ http:\/\/www.ijritcc.org\nInternational Journal on Recent and Innovation Trends in Computing and Communication ISSN: 2321-8169\nVolume: 5 Issue: 11 222 \u2013 229\nsecond by default. The refresh process creates a new in-\nmemory segment from the contents of the in-memory buffer\n(making the newly indexed documents searchable), then\nempties the buffer, as shown below.\n\nFigure: 5The index flush process\n\nIndexing Performance Metrics\n\nElasticsearch provides a number of metrics to assess indexing\nperformance and to optimize update your indices.\n\uf0b7 Indexing latency: Monitoring tools can help to\ncalculate the average indexing latency from the\nFigure 4. The index refresh process\navailable index_total and index_time_in_millis metric\ns. If the latency is increasing, user is trying to index\nShards of an index are composed of multiple segments. The\ntoo many documents at one time.To index a lot of\ncore data structure from Lucene, a segment is essentially a\ndocuments without new information to be\nchange set for the index. These segments are created with\nimmediately available for search, you can optimize\nevery refresh and subsequently merged together over time in\nfor indexing performance over search performance by\nthe background to ensure efficient use of resources. Each\ndecreasing refresh frequency until you are done\nsegment uses file handles, memory, and CPU. Segments are\nmini-inverted ind... (truncated)",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.5618035793,
        "format_confidence":0.9724909663
    },
    {
        "url":"http:\/\/swat.cse.lehigh.edu\/pubs\/data\/song11a.rdf",
        "text":"]> Automatically Generating Data Linkages Using a Domain-Independent Candidate Selection Approach One challenge for Linked Data is scalably establishing high-quality owl:sameAs links between instances (e.g., people, geographical locations, publications, etc.) in different data sources. Traditional approaches to this entity coreference problem do not scale because they exhaustively compare every pair of instances. In this paper, we propose a candidate selection algorithm for pruning the search space for entity coreference. We select candidate instance pairs by computing a character-level similarity on discriminating litera values that are chosen using domain-independent unsupervised learning. We index the instances on the chosen predicates\u2019 literal values to efficiently look up similar instances. We evaluate our approach on two RDF and three structured datasets. We show that the traditional metrics don\u2019t always accurately reflect the relative benefits of candidate selection, and propose additional metrics. We show that our algorithm frequently outperforms alternatives and is able to process 1 million instances in under one hour on a single Sun Workstation. Furthermore, on the RDF datasets, we show that the entire entity coreference process scales well by applying our technique. Surprisingly, this high recall, low precision filtering mechanism frequently leads to higher F-scores in the overall system. 2011-09-28 LNCS 7031 10th International Semantic Web Conference Bonn, Germany Springer 2011 November",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.7589449286,
        "format_confidence":0.9825927019
    },
    {
        "url":"http:\/\/www.cs.cmu.edu\/~tcm\/thesis\/subsection2_9_3_3.html",
        "text":"Next: Exclusive-Mode Prefetching Up: Experimental Results Previous: Scheduling Algorithm\n\nPrefetching Indirect References\n\nThe prefetching algorithm used so far in this chapter attempts to prefetch both dense and indirect array references. Indirect references are prefetched as described in Section . Only one of the multiprocessor applications contained a significant number of indirect array references: MP3D. Figure breaks down how much of the prefetching benefit came from the dense versus the indirect prefetches.\n\nAs we see in Figure , the overwhelming majority of the benefit was from prefetching the indirect references. This is in contrast with the results we saw earlier in Figure for the uniprocessor version of MP3D, where there was very little advantage to prefetching the indirect references. The difference between these two cases is that the indirect references are to objects that are very actively shared and modified amongst the processors (the ``space cells''), whereas the dense references are to objects that are rarely shared and reside in a processor's local memory (the ``particles''). Therefore the miss latency tends to be substantially larger for the indirect references, since they are often found dirty in a remote processor's cache, in contrast with the dense references, which are found locally.\n\nThis application illustrates several aspects of our prefetching compiler algorithm: (i) locality analysis to reduce the overhead of prefetching dense matrix references (as shown in Figure ), (ii) prefetching indirect references (as shown in Figure ), and (iii) non-binding prefetching for multiprocessors (as evidenced by the size of the ``pf-miss: invalidated'' category in Figure ). We now consider the final aspect of our multiprocessor prefetching algorithm, which is using exclusive-mode prefetches.\n\nSat Jun 25 15:13:04 PDT 1994",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.9480996132,
        "format_confidence":0.9838702679
    },
    {
        "url":"http:\/\/cs.gmu.edu\/~ygingold\/rigmesh\/",
        "text":"RigMesh: Automatic Rigging for Part-Based Shape Modeling and Deformation\nPeter Borosan, Ming Jin, Doug DeCarlo, Yotam Gingold, Andrew Nealen\nACM Transactions on Graphics (TOG) 31(6):198:1\u2013198:9. Also in Proceedings of SIGGRAPH Asia 2012.\n\nPaper: PDF (21 MB) | PDF (4 MB)\n\nPresentation (SIGGRAPH Asia 2012): PDF (10 MB) | PDF with notes (10 MB) | Keynote '09 (18 MB) | PowerPoint with demo (97 MB)\n\n\nThe creation of a 3D model is only the first stage of the 3D character animation pipeline. Once a model has been created, and before it can be animated, it must be rigged. Manual rigging is laborious, and automatic rigging approaches are far from real-time and do not allow for incremental updates. This is a hindrance in the real world, where the shape of a model is often revised after rigging has been performed. In this paper, we introduce algorithms and a user-interface for sketch-based 3D modeling that unify the modeling and rigging stages of the 3D character animation pipeline. Our algorithms create a rig for each sketched part in real-time, and update the rig as parts are merged or cut. As a result, users can freely pose and animate their shapes and characters while rapidly iterating on the base shape. The rigs are compatible with the state-of-the-art character animation pipeline; they consist of a low-dimensional skeleton along with skin weights identifying the surface with bones of the skeleton.\n\nVideo (MP4, 18 MB):\n\nModeling the camel (MP4, 6 MB):\n\nBibTeX (or see the ACM Digital Library entry):\n\n author    = {Peter Borosan and Ming Jin and Doug DeCarlo and Yotam Gingold and Andrew Nealen},\n title     = {Rig{M}esh: Automatic Rigging for Part-Based Shape Modeling and Deformation},\n journal   = {ACM Transactions on Graphics (TOG)},\n volume    = {31},\n number    = {6},\n pages     = {198:1--198:9},\n articleno = {198},\n numpages  = {9},\n url       = {http:\/\/doi.acm.org\/10.1145\/2366145.2366217},\n doi       = {10.1145\/2366145.2366217},\n year      = {2012},\n month     = nov,\n publisher = {ACM Press},\n address   = {New York, NY, USA}",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.7916551828,
        "format_confidence":0.9435508847
    },
    {
        "url":"https:\/\/pennstate.pure.elsevier.com\/en\/publications\/a-software-engineering-team-research-mapping-study",
        "text":"A software engineering team research mapping study\n\nResearch output: Contribution to journalReview articlepeer-review\n\n3 Scopus citations\n\n\nPurpose: The purpose of this mapping study has been performed to identify, critically analyze and synthesize research performed in the area of software engineering teams. Teams, in a general sense, have been studied extensively. But the distinctive processes that need to be executed effectively and efficiently in software engineering require a better understanding of current software engineering team research. Design\/methodology\/approach: In this work, software engineering team publications were analyzed and the key findings of each paper that met our search inclusion criteria were synthesized. In addition, a keyword content analysis was performed to create a taxonomy to categorize each paper and evaluate the state of software engineering team research. Findings: In software engineering team research, the resulting areas that are the most active are teamwork\/collaboration, process\/design and coordination. Clear themes of analysis have been determined to help understand how team members collaborate, factors affecting their success and interactions among all project stakeholders. In addition, themes related to tools to support team collaboration, improve the effectiveness of software engineering processes and support team coordination have been found. However, the research gaps determined from the content analysis point toward a need for more research in the area of communication and tools. Originality\/value: The goal of this work is to define the span of previous research in this area, create a taxonomy to categorize such research and identify open research areas to provide a clear road map for future research in the area of software engineering teams. These results, along with the key finding themes presented, will help guide future research in an area that touches all parts of the software engineering and development processes.\n\nOriginal languageEnglish (US)\nPages (from-to)203-248\nNumber of pages46\nJournalTeam Performance Management\nIssue number3-4\nStatePublished - Jun 7 2018\n\nAll Science Journal Classification (ASJC) codes\n\n  \u2022 Management Information Systems\n  \u2022 Organizational Behavior and Human Resource Management\n  \u2022 Management of Technology and Innovation\n\n\nDive into the research topics of 'A software engineering team research mapping study'. Together they form a unique fingerprint.\n\nCite this",
        "topic_id":2,
        "format_id":0,
        "topic_confidence":0.9825379252,
        "format_confidence":0.9816447496
    }
]