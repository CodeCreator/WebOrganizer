[
    {
        "url":"http:\/\/eprints.utm.my\/3335\/",
        "text":"Universiti Teknologi Malaysia Institutional Repository\n\nAcademic computing components in Malaysian Higher Education\n\nMokhtar, Shamsul Anuar and Alias, Rose Alinda (2005) Academic computing components in Malaysian Higher Education. In: Postgraduate Annual Research Seminar 2005.\n\n[img] PDF\n\n\nIn the information age, higher education institutions in Malaysia are implementing academic computing to provide better education services and remain competitive in a global knowledge industry. It encompasses the utilisation of staff, infrastructure (hardware and software) and services (technology, information content and human resources) which enable and support the management and delivery of academic programmes in teaching, learning and research. Research by UNESCO (2004) shows that many Asia-Pacific countries including Malaysia lack the proper framework to assess and evaluate IT implementation in higher education. Seven main areas of academic computing are identified. They include teaching and learning, research and publication, infrastructure, information services, institutional support, plan and policy and assessment. These areas encompass a total of thirty-four components. To determine the importance of these components within the Malaysian context, a pilot survey was conducted on a selected higher education institution. This research discusses the results.\n\nItem Type:Conference or Workshop Item (Paper)\nUncontrolled Keywords:academic computing, assessment, framework, higher education\nSubjects:H Social Sciences > H Social Sciences (General)\nQ Science > QA Mathematics > QA75 Electronic computers. Computer science\nDivisions:Computer Science and Information System (Formerly known)\nID Code:3335\nDeposited By: Ms Haslina Hashim\nDeposited On:23 May 2007 03:30\nLast Modified:06 Jun 2012 01:16\n\nRepository Staff Only: item control page",
        "topic_id":4,
        "format_id":0,
        "topic_confidence":0.986843884,
        "format_confidence":0.9140293598,
        "weight":0.0074293127
    },
    {
        "url":"https:\/\/www.bugwood.org\/intensive\/98-019.html",
        "text":"The Bugwood Network\n\nEvaluating Pine Regeneration Economic Opportunities: Natural Regeneration,\u00a0 Cut-over Planted, and\u00a0 Oldfield Afforestation Pine Stands\n\n\nDavid J. Moorhead - Professor of Forestry, The University of Georgia\nColeman W. Dangerfield - Associate Professor of Forestry, The University of Georgia.\n\nAugust 15, 1997, Georgia Cooperative Extension Service, College of Agricultural and Environmental Sciences, The University of Georgia, Athens, GA\u00a0 30602 U. S. A.\n\n\nThe U.S. South, including Alabama, Arkansas, Florida, Georgia, Kentucky, Louisiana, Mississippi, North Carolina, Oklahoma, Puerto Rico, South Carolina, Tennessee, Texas, and Virginia, holds about two-fifths of the timberland in the U.S., with 23 percent of the nation's softwood growing stock and 44 percent of the hardwood growing stock (Haynes et al. 1995). Over 660,000 employees are working in forest products manufacturing firms in the South, with a combined annual payroll in excess of $14.5 billion. Total economic impact of forestry and forest product industries on the Southern economy was in excess of $90 billion in 1994.\n\nOver the period 1990 to 2040, softwood harvests from U.S. forests are projected to rise 35 percent. Most of this increase will come from Southern forests. Southern softwood removals comprise 53 percent of the U.S. total timber removal value. Hardwood removals are 60 percent of U.S. totals. Southern hardwood annual growth exceeds harvest by 51 percent. Whereas, total softwood growth averages only 88 percent of harvest.\n\nThe growth of southern timber inventories has leveled off due to increased removals, lack of adequate regeneration, as well as environmental, and urban constraints that limit access to timber growing stocks. Enhancing the growth of existing timber stands, insuring adequate regeneration following harvest, plus afforestation of marginal rowcrop land, are important forest management activities that southern timber producers can implement to increase total timber supply and profits.\n\nIn 1988, there were 182 million acres of commercial forest in the South. There were 20.9 million acres of pine plantations, 40.9 million acres of naturally regenerated pines, and 26.9 million acres of mixed pine-hardwood stands from 25 to 50 percent pine. Projections indicate an increase in acres of planted pines with a corresponding decrease in the forest acres naturally regenerated after tree harvest. In addition, over 8 million acres of former row crop land in the South has been afforested under the Agricultural Conservation Program (1936-92, 2.4 million acres), Soil Conservation Reserve Program (Soil Bank) (1956-60, 2.2 million acres), the Forestry Incentives Program (FIP) (1974-92, 2.1 million acres), and the Conservation Reserve Program (CRP) (1985-92, 2.0 million acres) (Kurtz et al. 1994). Total tree planting in the South in 1996 was over 1.8 million acres (Moulton and Snellgrove 1997).\n\nAt present, hardwood stands are most commonly regenerated naturally after harvest. On the other hand, pine stands are increasingly regenerated by planting after harvest. Common pine regeneration methods include natural regeneration or planting on cut-over sites following timber harvests and, afforestation of former agricultural cropland.\n\n\nTo provide landowners with information about regeneration options we examined the management and economic opportunities of three pine regeneration methods: natural regeneration, plant cut over sites, and afforestation of rowcrop land.\n\n\nThe natural regeneration scenario was computer-modeled using WINYIELD 1.11 (Hepp 1996), while the cut over and oldfield scenarios were modeled using GaPPS Version 4.0 (Zhou and Bailey 1996). Each scenario was examined using common assumptions, where possible, to compare the scenarios. Site productivity, indicated site index (SI), averaged 68 feet at 25 years. This SI can be described as highly productive and would be expected where additional inputs such as site preparation, weed competition control, and fertilizer are added.\n\nFinancial parameters were set as: a 28 percent marginal federal tax bracket; 4.0 percent, uninflated, before tax discount rate to provide a conservative alternative investment return; South-wide 1996 prices of $35 per cord for pulpwood, $62 per cord for chip and saw (CNS), and $243 per thousand board feet Scribner for sawtimber. Stumpage prices were projected uninflated. Total harvest expenses were computed at 12.5 percent of the harvest value, including 10 percent for marketing and 2.5 percent for ad valorem property taxes on timber harvested. Planting costs were charged at $50 per acre. Management was charged at $1 per acre per year. All results are reported uninflated, before taxes. Other variables such as hunting leases, and pine straw harvests were omitted from the assumptions because they would be common to each scenario and would add no real new information to this comparative study.\n\nThree measures of financial performance are presented: Soil Expectation Value (SEV), Internal Rate of Return (IRR), and Annual Equivalent Value (AEV). Soil expectation value is calculated as the net present worth (revenues discounted to present year less costs discounted to present year at the discount rate) of perpetual repetitions of the investment. SEV is useful for comparing investments of unequal time length and for determining bare land value. Internal rate of return is the interest rate at which discounted revenues equal discounted costs. It assumes that all intermediate revenues are reinvested into the project. A project is considered profitable if the internal rate of return exceeds the discount rate. Annual equivalent value is the net present worth expressed as an annuity over the planning horizon, computed at the discount rate. Annual equivalent value is a useful measure for comparing investments over unequal time periods.\n\nNatural Regeneration\n\nNatural regeneration of loblolly pine is a common practice, both planned and unplanned, across the South. Landowners may harvest pine from their lands with the goal of allowing natural regeneration to establish the new stand. Typically a seed tree or shelterwood method is employed leaving mature seed producing pines on each acre after harvest to provide seed for the new crop. Other options include seed, or seedlings-in-place, or seeding from adjacent stands as a natural regeneration source.\n\nWhile natural regeneration methods can provide low cost and effective means to establish new stands, overstocking is common when favorable weather and seedbed conditions occur. Mechanical strip thinning is a recommended practice usually by age 3 to 5 years. Costs associated with precommercial thinning increase as stands age but can still provide good economic returns as is shown in this case where $140 is spent in year 13.\n\nThis scenario utilizes findings from an earlier study by Moorhead, Dangerfield, and Edwards (1997). The naturally regenerated stands were established using seed tree method. Following harvest, all hardwood stems one inch in DBH and larger were treated by herbicide. At age 13 years, the stand was precommercially thinned (PCT) by hand crews using chainsaws to an approximate 12 x 12 foot spacing leaving 302 trees per acre that averaged 3.85 inches DBH and 23.24 feet in height.\n\nPer acre management costs included $5 for site preparation burning, $40 for herbicide treatment, and $140 for the PCT. Beginning in 1997, per acre charges for prescribed burns\/fire breaks at three year intervals were assessed at $8 for the initial burn, $6 for the second burn, and $5 for the subsequent burns.\n\nThe natural regeneration scenario was examined on a 35 year rotation with a thinning at age 28. The thinning treatment was a low thinning to a residual basal area (BA) of 65 square feet per acre. Timing of the thinning was set to maintain medium to low stand risk to southern pine beetle infestations, improve residual tree growth, and move cash-flows forward to improve financial performance. Volume removed had to meet a minimum 5 cords per acre to be considered commercially feasible.\n\nAt the thinning at age 28, the trees averaged 67 feet in height with a BA of 98. An average of 83 pulpwood stems per acre were harvested yielding 10.34 total cords per acre, Table 1. At final harvest at age 35, trees averaged 77 feet tall. The stand had a BA of 91 in 107 stems. A total of 32.90 cords were projected per acre. The product mix shifted to CNS and sawtimber with 25.13 cords and 7.77 cords, respectively. The 35 year rotation produced a total of 43.24 cords per acre in the two harvests. The uninflated IRR equaled 9.7 percent with an AEV of $24.83 per acre, and a SEV of $621 per acre, Table 2.\n\nPlanting Cut-over Stands\n\nA study by Glover and Zutter (1993), examining alternative site preparation methods, found that small increases in the density of hardwood early in the life of the stand had a considerable negative effect on pine survival and basal area. The amount of hardwood also had a negative effect on mean pine diameter and total height, but these effects decreased at later ages with increasing pine mortality and intra-species competitive pressure when the pines were left unthinned.\n\nThis analysis is modeled on data from a study in the Georgia Piedmont (Dangerfield and Edwards 1995). The original stand of loblolly pine was harvested and replanted with improved loblolly pine seedlings on a spacing of 6 x 10 feet (726 trees per acre). The site preparation treatment was shear, root rake, burn, disk, fertilize, and herbicide at a cost of $210 per acre. A thirty-three year rotation was chosen with thinnings at ages 18 and 25 years.\n\nAt the first thinning at age 18, average dominant height is projected at 50 feet with a BA of 137 square feet. An average of 215 stems per acre were harvested yielding 12.37 total cords per acre, Table 1. At the second thinning at age 25 years, average dominant height is projected at 64 feet with a BA of 136 sq... (truncated)",
        "topic_id":15,
        "format_id":0,
        "topic_confidence":0.7740926743,
        "format_confidence":0.9822486639,
        "weight":0.0018063228
    },
    {
        "url":"http:\/\/www.cs.cmu.edu\/~avrim\/ML06\/lect0206.txt",
        "text":"15-859(B) Machine Learning Theory 02\/06\/06 * Recap * Kalai-Vempala algorithm ========================================================================= Recap from last time ==================== \"Combining expert advice\": Have n prediction rules and want to perform nearly as well as best of them. Or, in game-theoretic setting, have n strategies and want to do nearly as well of best of them in repeated play. Randomized weighted majority algorithm: - Give each a weight (starting at 1). - Choose rule at random with probability proportional to its weight. - When you are told the correct label, penalize rules that made a mistake by multiplying weight by 1-epsilon. (If costs in [0,1] then multiply by (1-epsilon)^cost). Get: E[Alg cost] <= (1+epsilon)*OPT + (1\/epsilon)*log(n). \"Sleeping experts\"\/\"specialists\": at each time step, only some rules fire. Goal: for all i, E[cost_i(Alg)] <= (1+epsilon)*cost(i) + (1\/epsilon)*log(n). where cost_i(Alg) = cost of algorithm in the times when rule i fires. - Generalized version of randomized WM. - Initialize all rules to have weight 1. - At each time step, of the rules i that fire, select one with probability p_i proportional to w_i To update weights: - If didn't fire, leave weight alone. - If did fire, raise or lower depending on performance compared to weighted average: * R_i = [sum_j p_j cost(j)]\/(1+epsilon) - cost(i) * w_i <- w_i(1+epsilon)^{R_i} So, if rule i does exactly as well as weighted average, its weight drops a little. Weight increases if does better than weighted average by more than a (1+epsilon) factor. Can then prove that total sum of weights never goes up. Now notice that one way to look at weights is: * w_i = (1+epsilon)^{E[cost_i(alg)]\/(1+epsilon) - cost_i(i)} I.e., we are explicitly giving large weights to rules for which we have large regret. Since sum of weights <= n, exponent must be at most log_{1+epsilon}(n ) ========================================================================== Today: Notice that RWM bounds are good even if \"n\" is exponential in the natual parameters of the problem. But the running time in that case is bad because we have to explicitly maintain a probability distribution over experts. Are there interesting settings where we can get these kinds of bounds *efficiently*? Today we will discuss an algorithm that can do this in many such cases. Kalai-Vempala Algorithm ======================== The Kalai-Vempala algorithm applies to the following setting. - We have a set S of feasible points in R^n. Each time step t, we (probabilistically) pick x_t in S. We are then given a cost vector c_t and we pay the dot-product c_t . x_t. - The set S may have exponentially-many points, so we don't have time even to list its elements. However, we have a magic offline algorithm M that, given any cost vector c, finds the x in S that minimizes c.x. Want to use to produce an *online* algorithm so that our overall expected cost is not too much worse than for best x in S in hindsight. Specifically, we want for any sequence c_1,...,c_T to get: E[Alg's cost] <= min_{x in S} (c_1 + ... + c_T).x + [regret term] - To have any hope, we will need to assume that S is bounded and so are the cost vectors c_t. What are some things we can model this way? =========================================== - Consider the following adaptive route-choosing problem. Imagine each day you have to drive between two points s and t. You have a map (a graph G) but you don't know what traffic will be like (what the cost of each edge will be that day). Say the costs are only revealed to you after you get to t (on the evening news). We can model this by having one dimension per edge, and each path is represented by the indicator vector listing the edges in the path. Then the cost vector c is just the vector with the costs of each edge that day. Notice that you *could* represent this as an experts problem also, with one expert per path, but the number of s-t paths can easily be exponential in the number of edges in the graph (e.g., think of the case of a grid). However, given any set of edge lengths, we can efficiently compute the best path for that cost vector, since that is just a shortest-path problem. You don't have to explicitly list all possible paths. - We can also model the standard experts problem this way by having each expert be its own coordinate, and c_t is the vector of costs at time t. (But bounds as a fn of n might be worse). - More generally, this is like online linear programming. - Can also model the problem of performing search-tree lookups in a way that is competitive with the best binary search tree in hindsight. For this problem, there is a cost of \"movement\" too (choosing x_{t+1} that is different from x_t), but that can be handled by running the algorithm at a very slow learning rate. For this problem, we have one dimension per element, and a given search tree is represented as the vector of depths of its elements. Requests are indicator vectors. Even though there are exponentially-many search trees on n elements, if we are given a vector c (which can be viewed as listing how many times each element is requested), we can compute the optimal tree for c using dynamic programming. So, we can plug into this framework. The KV Algorithm ================ One natural thing to try is just run M to pick the best x in hindsight, argmin_{x in S} (c_1 + ... + c_{t-1}).x, and use that on day t. But this doesn't work. E.g., imagine there are two routes to work, one fast and one slow, but they alternate which is fast and which is slow from day to day. This procedure will always choose the wrong one. Instead, we will \"hallucinate\" a day 0 in which we have random costs, from an appropriate distribution. Then we will run M to pick the x that minimizes (c_0 + c_1 + ... + c_{t-1}).x. To analyze this, we will argue in stages. Step 1: First, we will show that picking argmin_{x in S} (c_1 + ... + c_t).x *does* work. I.e., if you find the best in hindsight tomorrow and use that today. This is maybe not surprising but also not obvious. Note this is *not* the same as the x that minimizes c_t . x, which obviously would be super-optimal. Now, let algorithm A be the one that minimizes (c_1 + ... + c_{t-1}).x. Let B be the one that minimizes (c_1 + ... + c_t).x. They both go to the bar and start drinking. As they drink, their objective functions get fuzzier and fuzzier. We end up getting: - A' that minimizes (c_0A + c_1 + ... + c_{t-1}).x. - B' that minimizes (c_0B + c_1 + ... + c_t).x. where c_0A, c_0B are picked at random in [-1\/epsilon,1\/epsilon]^n. (Could use a ball instead of a cube and get somewhat different bounds. Cube is simpler anyway). As epsilon gets smaller, A and B start behaving more and more alike. Step 2 is to show that for an appropriate value of epsilon, B' is not too much worse than B, and A' is close to B'. This then shows that A' is a good algorithm. Preliminaries ============= - When we analyzed the experts algorithm, we got bounds like: E[Alg's cost] < OPT(1+epsilon) + (1\/epsilon)*(log n). If you then set epsilon = sqrt(log(n)\/T), where T is the number of time steps you plan to run the algorithm, you get: E[Alg's cost] < OPT + OPT*sqrt(log(n)\/T) + sqrt(T*log(n)). If you then use the fact that OPT <= T, you get: E[Alg's cost] < OPT + 2*sqrt(T log n). (This is the way a game-theory person would usually phrase it: in terms of additive regret.) - In our case, we will assume that the maximum L_1 length (sum of absolute values) of any cost vector is 1. If D is the L_1 diameter of S, the bound we will show is: E[Alg's cost] < OPT + D(epsilon*T\/2 + 1\/epsilon) So, setting epsilon = sqrt(2\/T), we get: E[Alg's cost] < OPT + D*sqrt(2T) If you convert to the experts setting, where c \\in [0,1]^n, the result you will get is an \"n\" inside the square root. This is not as good as the standard experts bounds. They give a different distribution for hallucinating c_0 that fixes that problem. Step 1 ====== Define x_t = argmin_{x in S} (c_1 + ... + c_t) . x. We need to show that: c_1 . x_1 + ... + c_T . x_T <= (c_1 + ... + c_T) . x_T In other words, we do at least as well using algorithm B as the optimal fixed x in hindsight does. We can do this by induction: - By induction, we can assume: c_1.x_1 + ... + c_{T-1}.x_{T-1} <= (c_1+...+c_{T-1}).x_{T-1}. And we know, by definition of x_{T-1}, that: (c_1 + ... + c_{T-1}) . x_{T-1} <= (c_1 + ... + c_{T-1}) . x_T So, putting these together, and adding c_T . x_T to both sides, we get what we want. Step 2: ======= Let's start with the easier part, showing that A' and B' are close. - A' is picking a random objective function from a box of side-length 2\/epsilon centered at c_1 + ... + c_{t-1}. B' is picking a random objective function from a box of side-length 2\/epsilon centered at c_1 + ... c_t. Let's call the boxes box(A) and box(B). - These boxes each have volume V = (2\/epsilon)^n. Since their centers differ by a vector of L_1 length at most 1, the intersection I of the boxes has volume at least (2\/epsilon)^n - (2\/epsilon)^{n-1} = V(1 - epsilon\/2). - This means that A' can't be much worse than B'. In particular, say alpha is your expected loss if you were to pick x by feeding to M a random point c in I. We can view B' as with probability 1-epsilon\/2 doing exactly this and getting expected loss alpha, and with probability epsilon\/2 choosing a random c in box(B) - I, and at best getting a loss of 0. We can view A' as with probability 1-epsilon\/2 getting an expected loss of alpha, and with probability epsilon\/2 getting at worst a loss of D. [Actually, we never said the losses had to be non-negative. The point is that the diameter of S is at most D, so the biggest possible gap between the losses of 2 points in S on any given c is D. Actually, for this part of the argument, we only need L_infty diameter <= D and not L_1 diameter.] The difference between the two is D*epsilon\/2. This happens for T time steps, so that counts for the DT*epsilon\/2 part. Now, to fin... (truncated)",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.5479893088,
        "format_confidence":0.90950495,
        "weight":0.0328030646
    },
    {
        "url":"https:\/\/scholars.uab.edu\/display\/pub726618",
        "text":"Rarity of the alzheimer disease-protective APP A673T variant in the United States\n\nAcademic Article\n\n\n  \u2022 Copyright \u00a9 2015 American Medical Association. All rights reserved. IMPORTANCE Recently, a rare variant in the amyloid precursor protein gene (APP) was described in a population from Iceland. This variant, in which alanine is replaced by threonine at position 673 (A673T), appears to protect against late-onset Alzheimer disease (AD). We evaluated the frequency of this variant in AD cases and cognitively normal controls to determine whether this variant will significantly contribute to risk assessment in individuals in the United States.OBJECTIVE To determine the frequency of the APP A673T variant in a large group of elderly cognitively normal controls and AD cases from the United States and in 2 case-control cohorts from Sweden.DESIGN, SETTING, AND PARTICIPANTS Case-control association analysis of variant APP A673T in US and Swedish white individuals comparing AD cases with cognitively intact elderly controls. Participants were ascertained at multiple university-associated medical centers and clinics across the United States and Sweden by study-specific samplingmethods. They were from case-control studies, community-based prospective cohort studies, and studies that ascertained multiplex families from multiple sources.MAIN OUTCOMES AND MEASURES Genotypes for the APP A673T variantwere determined using the Infinium HumanExome V1 Beadchip (Illumina, Inc) and by TaqMan genotyping (Life Technologies).RESULTS The A673T variant genotypes were evaluated in 8943 US AD cases, 10 480 US cognitively normal controls, 862 Swedish AD cases, and 707 Swedish cognitively normal controls.We identified 3 US individuals heterozygous for A673T, including 1 AD case (age at onset, 89 years) and 2 controls (age at last examination, 82 and 77 years). The remaining US samples were homozygous for the alanine (A673) allele. In the Swedish samples, 3 controls were heterozygous for A673T and all AD cases were homozygous for the A673 allele. We also genotyped a US family previously reported to harbor the A673T variant and found a mother-daughter pair, both cognitively normal at ages 72 and 84 years, respectively, who were both heterozygous for A673T; however, all individuals with AD in the family were homozygous for A673.CONCLUSIONS AND RELEVANCE The A673T variant is extremely rare in US cohorts and does not play a substantial role in risk for AD in this population. This variant may be primarily restricted to Icelandic and Scandinavian populations.\n  \u2022 Published In\n\n  \u2022 JAMA Neurology\u00a0 Journal\n  \u2022 Digital Object Identifier (doi)\n\n    Pubmed Id\n\n  \u2022 20137071\n  \u2022 Author List\n\n  \u2022 Wang LS; Naj AC; Graham RR; Crane PK; Kunkle BW; Cruchaga C; Gonzalez Murcia JD; Cannon-Albright L; Baldwin CT; Zetterberg H\n  \u2022 Start Page\n\n  \u2022 209\n  \u2022 End Page\n\n  \u2022 216\n  \u2022 Volume\n\n  \u2022 72\n  \u2022 Issue\n\n  \u2022 2",
        "topic_id":12,
        "format_id":0,
        "topic_confidence":0.9806690812,
        "format_confidence":0.9878380299,
        "weight":0.0244284378
    },
    {
        "url":"https:\/\/thecamel.hypotheses.org\/29",
        "text":"On terminology: A sufi polemic against the \u201cmutafaqqih\u016bn\u201d of Damascus\n\nReading Ignaz Goldziher\u2019s summary (1874) of a sufi\u2019s lament over the corruption of Syrian Islamdom under the Mamluks, I was struck by the sufi\u2019s choice in terminology.\n\nThe Maghrib\u012b born \u02bfAl\u012b b. Maym\u016bn attacks current practices of the jurists and sufis of Syria (and Damascus, in particular) but in the title of his work Bayy\u0101n ghurbat al-isl\u0101m bi-w\u0101si\u1e6da \u1e63infai al-mutafaqqiha wa-l-mutafaqqira min ahl Mi\u1e63r wa-l-Sh\u0101m wa-m\u0101 yal\u012bhim\u0101 min bil\u0101d al-a\u02bdj\u0101m he does not use the common terms faq\u012bh and \u1e63\u016bf\u012b. Rather, he refers to them as mutafaqqih\u016bn and mutafaqqir\u016bn. This grammatical form opens up a wide semantic field.\n\nA quick search on shamila shows that these terms are overall uncommon. They only appear ever so often throughout the different Arabic disciplines. Ibn Maym\u016bn seems an exception in using them throughout his work. So what does that mean? Does he use them in a polemic, perhaps even propagandistic way? The use of the form tafa\u02bf\u02bfala is peculiar in itself. Whereas faq\u012bh commonly refers to an accomplished jurist, this form indicates a more abstract notion, describing those people who follow the way or approach of fiqh (jurisprudence) to gain understanding of God, whereas the mutafaqqir\u016bn, in contrast to the accomplished fuqar\u0101\u02be, only aspire to gain the same knowledge by a different path (but have not achieved it yet). These might respond to the distinction of two categories of knowledge (\u02bfilm al-\u1e93\u0101hir \/ \u02bfilm al-b\u0101\u1e6din), Ibn Maym\u016bn uses in an earlier work (Goldziher 1874, p. 301). The grammatical form can further be interpreted as meaning \u2018those who pretend to be jurists \/ sufis\u2019. In the light of the general argument of the work, this seems most plausible.\n\nThis notion of the state of Syrian scholarship and sufism becomes clearer in the course of Ibn Maym\u016bn\u2019s argument, which, after starts out with plead for a reconciliation of both approaches, proceeds to outline all the faults of Syrians in both areas. The individual points addressed by Ibn Maym\u016bn (and summarized by Goldziher) are all intriguing on their own account. They offer insight into a number of contemporaneous practices, the Maghrib\u012b sufi regarded as wrong and dangerous.\n\nYet, they seem to only illustrate what he outlines already in the title of the work: His choice in terminology already diminishes the status of both Syrian sufis and jurists a priori, casting them down from full fledged representatives of their respective status groups to mere aspirants to the knowledge, these approaches offer.\n\nLeave a Reply\n\nYour email address will not be published. Required fields are marked *",
        "topic_id":18,
        "format_id":0,
        "topic_confidence":0.5919153094,
        "format_confidence":0.8253697753,
        "weight":0.0086087121
    },
    {
        "url":"https:\/\/engagedscholarship.csuohio.edu\/clevstlrev\/vol7\/iss1\/14\/",
        "text":"Article Title\n\nAcute Tax Neuroses\n\n\nThe major cause of acute taxitis is the mental and physical turmoil arising from accusations by the Revenue Service that false and fraudulent returns have been filed with intent to evade taxes. The three most common means by which tax evasion is discovered are: (1) routine audit-spot checks and surveys; (2) tips from informers in hopes of receiving sizeable rewards,or by jealous neighbors or disgruntled employees; (3) newspaper publicity about marriages, trips, home purchases, robberies, real estate and business transactions.",
        "topic_id":9,
        "format_id":0,
        "topic_confidence":0.9000526071,
        "format_confidence":0.766931355,
        "weight":0.0056197026
    },
    {
        "url":"http:\/\/ir.uiowa.edu\/pog\/vol2\/iss2\/7\/",
        "text":"The incidence of obesity has achieved epidemic proportions. The increase in the proportion of women gaining more than 40 pounds during pregnancy and the related increased perinatal morbidity demonstrates how obesity has affected the reproductive outcomes for women. Individualized counseling and preparation for the needs of an obese woman in labor are at the cornerstone of managing obese women in labor and delivery.\n\n\nobesity, delivery, antepartum testing, pregnancy, labor\n\nSubmission Type\n\n\n\nCopyright \u00a9 Mark K. Santillan, 2011.\n\nCreative Commons License\n\nThis work is licensed under a Creative Commons Attribution 3.0 License.",
        "topic_id":12,
        "format_id":0,
        "topic_confidence":0.9980332255,
        "format_confidence":0.938862443,
        "weight":0.023217311
    },
    {
        "url":"http:\/\/eprints.utas.edu.au\/1447\/",
        "text":"Library Open Repository\n\n\n\nDownloads per month over past year\n\nSmith, KH (2002) Innovation. In: The Handbook of Economics. International Encyclopedia of Business & Management . Thomson, London, UK, pp. 98-105. ISBN 1 86152 545 1\n\n[img] PDF\nInnovation.pdf | Request a copy\nFull text restricted\nAvailable under University of Tasmania Standard License.\n\n\nInnovation is novelty - it involves doing new things in new ways. So new products, new processes, new organizational methods, new services, and so on, are all part of innovation. Technological innovation transforms and improves the technical attributes and performance characteristics of products and processes, and through this introduces dynamic change and productivity growth into the economic system. For this reason, all theories of economic growth rest in one way or another on ides concerning innovation and technological change. In studying innovation, most modern research has focused on the sources, nature and characteristics of knowledge and learning. If innovation is novelty, then it must also involve learning (after all, if we already knew how to do something, it would not be an innovation). If we think of technology as forms of knowledge related to productive transformations, then innovative learning is the process that expands the existing knowledge base.\n\nItem Type: Book Section\nKeywords: innovation; learning; technology; knowledge; economic growth\nPublisher: Thomson\nPage Range: pp. 98-105\nDate Deposited: 03 Sep 2007\nLast Modified: 18 Nov 2014 03:19\nItem Statistics: View statistics for this item\n\nRepository Staff Only (login required)\n\nItem Control Page Item Control Page",
        "topic_id":9,
        "format_id":0,
        "topic_confidence":0.7165178061,
        "format_confidence":0.9028233886,
        "weight":0.0066154539
    }
]