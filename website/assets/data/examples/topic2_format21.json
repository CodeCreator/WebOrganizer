[
    {
        "url":"http:\/\/radar.oreilly.com\/2010\/05\/code-for-america-empowering-so.html",
        "text":"Better government through code\n\nCode for America founder Jennifer Pahlka wants to empower developers to become civic coders\n\nCode for AmericaIn his inaugural address in January of 1961, President John F. Kennedy challenged his fellow Americans to \u201cask not what your country can do for you \u2013 ask what you can do for your country.\u201d In 2010, the question has been updated: ask not what your country can code for you \u2013 ask what you can code for your country.\n\nToward that end, Jennifer Pahlka has extended the innovative volunteerism of Teach for America toward software developers with the new organization, Code for America. The non-profit allows the brightest technical minds of this generation to create applications that let government deliver better services to citizens. As Jolie Odell reported at Mashable, five American cities have been selected to receive help building web and\/or mobile applications through Code for America\u2019s fellows program. Boston, Boulder, Philadelphia, Washington, D.C., and Seattle will soon have civic coders looking for ways to make government work better.\n\nWhat follows is an edited and condensed interview with Pahlka conducted shortly before the selected cities were announced. (Note: included emphasis was added by the editor.)\n\nWhat has bootstrapping Code for America been like?\n\nIt\u2019s definitely been \u201ca dive right\u201d in experience. And I think the good news is it\u2019s happening. When ideas hit reality, sometimes there\u2019s sort of a combustion moment. We certainly have had bumps. But it seems as this comes out into the world and gets tested, especially with city staff, that there is a real need and there is an interest in making this happen. There is enough belief in the potential to really drive it forward. We\u2019ve taken it basically from a concept to having \u201creal clients.\u201d\n\nGov 2.0 Expo 2010I finished up work on Web 2.0 in the beginning of December and aggressively launched our first call for applications from cities in the middle of January, which is a really short turnaround. I was happily surprised that we got 11 cities to apply on such short notice, given that we had of educate them about what it was. For some cities, it\u2019s not a long education process. Some of them get it very quickly.\n\nIt\u2019s not just selling them on Code for America. It\u2019s selling them on the idea that there\u2019s something to Web 2.0 in terms of driving transparency, efficiency and participation for cities. That this isn\u2019t just an extra program. It\u2019s really a way to transform government. I think we\u2019ve done really well on that front. Out of those 11, we\u2019ve got five cities that we\u2019ll be working with. They\u2019ve got good projects and we\u2019re happy that cities take a long time to do things, in the sense that our schedule is built such that we\u2019re not going to start building these projects until January. It\u2019s going to take that long to really understand what the opportunity there is.\n\nThis needs to originate inside the cities because they\u2019re the ones that know what will actually save money and make them more efficient and transparent. What we want to do then is take those ideas and bring in experts from the web industry.\n\nHow will Code for America and the selected cities collaborate?\n\nTo clarify, this isn\u2019t being freely offered. We ask the cities to pay for the costs of their Code for America fellows. They each have to cover about $225,000. We don\u2019t pay our fellows much, but we do have to pay them something so that they can survive. That\u2019s been one of the obstacles. We had about 30 more cities that said that they would love to participate but couldn\u2019t come up with the funds. I think that\u2019s not surprising. In each of the five cities, there was someone who really believed that that $225,000 would save them an enormous amount of money in the long run, and they really wanted to do it.\n\nThis project was really birthed last July, when Andrew Greenhill, the Chief of Staff for the Mayor of Tucson, Arizona, was asking me for help bringing in a Web 2.0 team to build a web application that would save the city money. At the time, he was talking about what a dire circumstance Tucson was in and how cities around the country are really struggling. You get these enormous cuts in revenue and it\u2019s not like a business; you don\u2019t have the same flexibility that businesses have. So cities cut services. You can do that only so many times until you\u2019re not providing the services that a city needs to provide.\n\nAs budgets are getting settled in cities across the U.S. right now, you\u2019re actually starting to see the crap hit the fan, so to speak. There are two paths they can take: They can continue to try to shave here and there. Or they can say, \u201cWe have to take a fundamentally new approach\u201d and adopt government as a platform. That includes the principles and values of the web, and outsourcing some functions to cities or using collective intelligence.\n\nThese are very new approaches. They\u2019re very difficult for cities to get their heads around. I think the cities that applied reflect the ones that get that you can\u2019t just keep shaving here and there; you\u2019re going to have to take a new approach, even if it\u2019s going to hurt and even if it\u2019s going to cost a little bit. In the long run, it\u2019s the only way they\u2019re going to survive.\u201d\n\nWhat practical improvements will you bring to municipal government?\n\nWhat we\u2019re trying to do is show that you can save in other departments. One example that I like to use is that if you have a service request to your city, most of the time, you have to call it in. And most of the time, you have to figure out who to call. That\u2019s a burden on citizens. You\u2019re also often contacting a call center that has a pretty primitive way of tracking what you\u2019re calling in on. There\u2019s a bunch of money and time that can be saved in terms of those call centers. What you may not realize is that in most cities, after the call they will then generate a request to send an inspector out to verify whatever it is that\u2019s broken. An inspector will actually have to go visit that pothole, take notes on it, take a picture of it and then put it in the queue. There\u2019s a lot of money spent within cities from stuff like that. That is the perfect kind of thing that you could outsource to citizens. SeeClickFix provides a nice model for that. So does CitySourced.\n\nAnother example is emergency services. What costs a city the most money? Emergency response. It\u2019s incredibly expensive. If you think of neighborhoods as patients, then you would want to do the same thing the health care industry is trying to do: keep your patient out of the emergency room. The way to do that is to give them education and preventative care.\n\nOne project that we\u2019re interested in building is a neighborhood organizing platform that would replace the email lists that many people are on. In many cities, often within the radius of a couple of blocks in a neighborhood, there\u2019s an email list of residents. What gets talked about are subjects like someone\u2019s car was broken into. Trash pickup didn\u2019t happen this week. Library hours are being cut back And those conversations right now are completely citizen-to-citizen. Some person might find a piece of information that may be relevant and share it. That\u2019s very useful.\n\nWe envision a platform whereby the conversation continues to be citizen-to-citizen but the relevant data for a neighborhood is being provided to the citizens by the city. So you\u2019d know the crime stats for exactly those blocks you live on, for instance, instead of guessing that there had been a crime spike in your neighborhood. There would be a trigger set up that if crime jumps by X percent, X neighborhood group gets a message that says, \u201cThis is what\u2019s happening in your neighborhood according to our statistics. Would you like the police department to come do a neighborhood watch training? If so, click here to schedule.\u201d That\u2019s not only more efficient, it probably wouldn\u2019t happen otherwise.\n\nOr consider if there\u2019s a development that\u2019s been proposed for your neighborhood. How will you get information on it? How you will you respond to the city, in terms of your opinion on the project? In Oakland, where I live, they send out an enormous amount of mail for every development. You get actual physical mail sent to you if there\u2019s a proposed development project in your neighborhood, which cannot be cheap. We\u2019re starting to understand the value of urban data now but we really need to show how it can work. Just publishing municipal data is one thing that happened in apps contests like Apps For America, and it\u2019s been hugely valuable. We\u2019d like to get that data in the hands of citizens where they can use it to take control of the direction of their neighborhoods and reduce the number of complaints, reduce the number of emergency calls and save the city money in the long run.\n\nBeyond reducing costs and automating some transactions, what other areas could Code for America fellows help cities with?\n\nI think if you take any city that is using a 311 application, you\u2019re looking at huge savings. That\u2019s not available to a lot of cities right now; the costs are too high. So one area we want to do a lot of work is in 311. We\u2019re looking at building some open source solutions there so that we can cobble together an overall solution that connects with the Open 311 initiatives that San Francisco and D.C. have been driving.\n\n[Editor's note: San Francisco and Washington, D.C. have announced the joint adoption of the Open 311 platform.]\n\nWe need to start to solve a marketing problem for cities. Think about it. Let\u2019s say you build an iPhone app or an Android app that lets you take a picture of a broken streetlight and upload it to your city\u2019s DPW. That\u2019s great. Now you have to get it in the hands of citizens. If every single city is trying to promote their own application, you\u2019ve got huge marketing challenges, especially in metropolitan areas. I live in Oakland, but I\u2019m also frequently in Berkeley and San Francisco. I\u2019m not going to download t... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9127315283,
        "format_confidence":0.9281333089
    },
    {
        "url":"https:\/\/glezos.fedorapeople.org\/fudcon07\/l10n.log.html",
        "text":"FUDCon07 L10n session\n\n16:56 <glezos> \u0397\u03b9 \u03b1\u03bb\u03bb\n\n16:56 <glezos> oops\n\n16:56 <glezos> Hi all\n\n16:56 <glezos> If I'm not mistaken, this should be the time slot for our L10n presentation\n\n16:57 * glezos has changed the topic to: L10n: How our improved infrastructure affects you as a developer or translator\n\n16:58 <glezos> So I'll try to give a brief overview of the Fedora Localization Project (FLP), it's history, what we've been doing the past months and what our plans for the future are\n\n16:58 <glezos> Traditionally we've been handling translations through the internal i18n.redhat.com system\n\n16:58 <glezos> Developers hosted their projects on CVS, and translators registered for an account there getting access to all modules\n\n16:59 <glezos> Fedora is opening up its infrastructure lately\n\n16:59 <glezos> and along these lines, we've been trying to open up our translation infra as well\n\n16:59 <glezos> Why?\n\n16:59 <glezos> Many reasons..\n\n16:59 <glezos> Developers need access to non-CVS repositories\n\n17:00 <glezos> Currently the Fedora Infrastructure project supports CVS, SVN, Mercurial and git... AFAIK bzr is another candidate\n\n17:00 <glezos> the legacy system doesn't support them -- only CVS\n\n17:01 <glezos> With 2000 translators, we need systems to easily manage the accounts -- We already one in the Fedora space, it's the Fedora Account System\n\n17:02 <glezos> And with the project hosting, the FAS could help us out in assigning per-module access control lists, hence scaling better our translations and reaching out for more languages\n\n17:02 <glezos> So, what we've been doing lately to make translations easier, and do more stuff with them?\n\n17:03 <glezos> We've introduced string freezes in the Fedora Schedule, to make sure that the Fedora Releases have 100% translated software\n\n17:04 <glezos> We are focusing in coordinating the L10n Project through fedora-trans-list -- before, we didn't even have a list for the project\n\n17:04 <glezos> We encourage each language contributors to work in teams, just like other projects (great example is GNOME which has a highly respectable Translation Project)\n\n17:05 <glezos> To make sure the collaboration is kept in high levels, we're encouraging each team to have its own maintainer, as a communication point for them team\n\n17:05 <glezos> And we've started having regular IRC meetings.\n\n17:05 <glezos> In the tools area:\n\n17:06 <glezos> We've introduced a new website: http:\/\/translate.fedoraproject.org\/\n\n17:06 <glezos> It can list languages, teams, modules, maintainers. It supports multiple releases, so we can track branches easily\n\n17:07 <glezos> And above all, it supports Version Control Systems (VCS) other than CVS\n\n17:07 <glezos> - To help out developers do what they want, we've talked with people to move their modules away from the internal servers to our open infrastructure\n\n17:08 <glezos> This is an on-going process, and the plan is to completely deprecate the i18n.redhat.com system for Fedora software and bring everything out in the open\n\n17:08 <glezos> Community work -- Higher quality.\n\n17:08 <glezos> - We've created a L10n repository to store our tools\n\n17:09 <glezos> - We've created a bugzilla component so that bug reports for language mistakes and requests are sent to translators and not developers\n\n17:09 <glezos> - And we've created a commit-list, so we can track what's going on in terms of translation files\n\n17:09 <glezos> Finally, we've earned a Google Summer of Code project to create something that no other project has done so far:\n\n17:10 <glezos> a tool to integrate the various Version Control Systems together in the eyes of a translator\n\n17:10 <glezos> This is our main concern now, since it will give us the ability to do more stuff, easier.\n\n17:11 <glezos> It will give developers the option to host their project on their VCS of choice -- thus increasing efficiency, productivity and, potianially, quality.\n\n17:12 <glezos> Also, it will give developers the opportunity to reach out to the fedora translation community in an instance, to translate their software\n\n17:12 <glezos> examples of outside, Fedora-related projects that could benefit are yum, rpm and OLPC\n\n17:13 <glezos> Also, developers won't need to worry about maintaining accounts for the translators\n\n17:14 <glezos> With transifex, the Fedora translation community can contribute *directly upstream*, which of course is in line with the Fedora motto \"upstream, upstream, upstream!\"\n\n17:14 <glezos> Upstream means more projects than Fedora can benefit from each contribution\n\n17:15 <glezos> This is in direct contrast with other efforts like Ubuntu's Rosetta, which hosts translations downstream, with no automatic or semi-automatic way to push them upstream\n\n17:16 <glezos> Here's an overview of the model: http:\/\/glezos.fedorapeople.org\/transifex.png\n\n17:17 <glezos> Through our new submission system, translators will contribute to all our Repositories -- either they are the legacy systems, projects hosted on hosted.fpo, or various local repositories\n\n17:17 <glezos> Also, to projects related to Fedora like smolt, revisor and the OLPC\n\n17:18 <glezos> And finally, we could try to build up an architecture to enable submission directly to upstream projects like GNOME, KDE, Mozilla etc.\n\n17:19 <glezos> At this point we are thinking of introducing editors\/reviewers, that will \"approve\" the submissions, thus increasing the quality control\n\n17:19 <glezos> -\n\n17:19 <glezos> So, if you are a translator:\n\n17:19 <glezos> Keep an eye on fedora-trans-list -- soon we're putting transifex in action\n\n17:20 <glezos> If you are a developer:\n\n17:21 <glezos> Are you hosting your project on elvis (i18n.redhat.com)? Move over to Fedora infrastructure on your VCS of choice and let us know to list your project at http:\/\/translate.fedoraproject.org\/module\/\n\n17:22 <glezos> Do you have a project not hosted on Fedora but want to give the Fedora translation commmunity the ability to contribute translations to it, with language maintainers possibly reviewing submissions? Keep an eye on planet.fedoraproject.org or fedora-announce-list in the following weeks\n\n17:23 <glezos> Same goes for members of upstream projects -- we'd like your feedback for our work (https:\/\/hosted.fedoraproject.org\/projects\/transifex\/) in order to bring our translation communities closer and increase our coverage and quality\n\n17:23 <glezos> -\n\n17:24 <glezos> Any Questions so far? :)\n\n17:25 <Rasther> glezos, do you have a date to transifex to be available?\n\n17:26 <Rasther> or some schedule..\n\n17:26 <glezos> Rasther, in terms of features, we're 100% for Fedora's needs\n\n17:26 <glezos> currently we're testing it\n\n17:26 <glezos> The plan for deployment is available at: http:\/\/fedoraproject.org\/wiki\/SummerOfCode\/2007\/DimitrisGlezos\/Notes#deployment\n\n17:27 <nirik> glezos: so when upstream translators update things, does that get pulled back into transifex so there is no duplication of effort?\n\n17:27 <glezos> We're focusing on making sure security is well-thought -- but I'd say we are in good shape to enable submissions on Fedora repos in the next 7 days.\n\n17:28 <glezos> nirik, definitely -- transifex is just a gateway to submit stuff directly upstream, so with each commit in a decentralized VCS like hg\/git a pull is done before, and a push afterwards\n\n17:29 <joseppc> glezos: what will be available for translation in transifex?\n\n17:29 <glezos> we're not copying translation files downstream at all.\n\n17:29 * ricky wonders about encouraging other large translation teams to use transifex as well.\n\n17:29 <nirik> excellent. Is there a list of apps\/projects that will be supported? Have you talked with the Xfce project about adding them... they have a small but effective translation team...\n\n17:30 <glezos> joseppc, I don't see a reason in limiting what could be translated through transifex. I'd say \"any open source project\".\n\n17:31 <glezos> ricky, nirik, I see it as a 3-step process: 1. Make it work with Fedora's repos (4-5). 2. Expand in Fedora-related projects (yum, rpm, olpc come in mind). 3. Discuss with bigger projects with existing translation communities in finding how we can work together\n\n17:31 <glezos> Some projects might want, for example, to have some editorial control -- which would require some coding :)\n\n17:32 <ricky> Ah, sounds great.\n\n17:32 <glezos> nirik, for start almost all modules registered at http:\/\/translate.fedoraproject.org\/module\/ will be supported.\n\n17:32 <nirik> yeah, sounds nice.\n\n17:32 <glezos> (at least that's what we are hoping for)\n\n17:32 <glezos> hoping\/planning\/aiming\n\n17:33 <Rasther> great!\n\n17:33 <glezos> I believe this is a *great* opportunity for everyone, because it shows what can be achieved if we build bridges between teams and projects\n\n17:33 <ricky> So I guess the last big thing for now is getting some nice documentation for project developers\/translators.\n\n17:33 <joseppc> glezos: my concern was whether it would happen like in rosetta, that is like a fork of every translation project there is (even if they encourage to work upstream), or we'd expect projects to actually use transifex as their translation platform\n\n17:34 <ricky> As a frontend to normal translation tools, I think Transifex would be flexible and work well with projects' existing translation workflow.\n\n17:34 <glezos> joseppc, it is definitely not a fork of the (upstream) translation project, because translations are submitted directly upstream and the upstream translation team has the control of what gets submitted\n\n17:35 <glezos> Transifex could be installed by any project wanting to do the same thing. For example, Debian might want to install it to give its translators the ability to submit to GNOME.\n\n17:36 <glezos> Any other questions?\n\n17:36 <joseppc> glezos: yes, just a sec\n\n17:37 <joseppc> glezos: but say the upstream project is gnome. As a gnome translator I would be a bit annoyed if parallely someone using transifex was doing a translation. I'd rather expect they to work in gnome directly.... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9084848166,
        "format_confidence":0.9661492109
    },
    {
        "url":"https:\/\/www.ministryoftesting.com\/dojo\/series\/the-ministry-of-testing-podcast-2020\/lessons\/mot-podcast-mike-meets-sergio-freire",
        "text":"MoT Podcast - Mike Meets S\u00e9rgio Freire\n\n  \u2022 00:37:56\n\n\nMike talks to S\u00e9rgio Freire from Xray about his career in software development, as a developer, team lead and then a subtle shift in career.\n\nHe discusses his background in Telco and how teams worked back then and his path to becoming a Product Owner for Xray and then a Solution Architect, covering the differences he encountered in each role and reflections on what he wishes he had known when you started out in software development.\n\nHe also covers how Xray is helping teams and what they aim to do in the future.",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9190084338,
        "format_confidence":0.6054413319
    },
    {
        "url":"https:\/\/wiki.lyrasis.org\/display\/cmtygp\/DCAT+Meeting+October+2015",
        "text":"Page tree\nSkip to end of metadata\nGo to start of metadata\n\nshort link to this page:\n\nDate & Time\n\n  \u2022 October 13 15:00\u00a0UTC\/GMT -\u00a011:00\u00a0EDT\n\n\nWe will use the international conference call dial-in. Please follow directions below.\n\n  \u2022 U.S.A\/Canada toll free:\u00a0866-740-1260,\u00a0participant code:\u00a02257295\n  \u2022 International toll free:\n    \u2022 Use the above link and input 2257295 and the country you are calling from to get your country's\u00a0toll-free dial in #\n    \u2022 Once on the call, enter participant code 2257295\n\n\nPreparing for the call\n\n  \u2022 Review the DSpace 6 Testathon Testplan Working group page\n    \u2022 Especially look at the section \"Requirements for\" installation. If we finalize our test plan, we should also put forward a clear set of expectations on how we would want to be configured.\n    \u2022 Review the draft of the XMLUI Test plan (everyone can view, if you want to help out, hit the button on the top right corner to request edit rights)\n  \u2022 Review DS-2741 - Getting issue details... STATUS\n  \u2022 Information on the current status of the UI project is available at: UI Working Group\n\n\nMeeting notes\n\n\nCall Attendees\n\n  \u2022 No labels\n\n\n  1. During the testing I would like to try out uploading big files, in the range 1Gb-10Gb. Would this cause problems?\n\n    1. Since Figshare has just announced they're allowing uploads of files up to 5Gb as part of their big announcement of new features, we'd like to try to stay one step ahead of them in that respect... (smile)\n\n  2. I am really looking forward to testing this new UI and eventually testing installation.\n\n  3. Also, Bram asked whether there were features we'd like to be enabled for the Testathon. DOI generation has become a crucial part of our use of DSpace. I realise that feature is more complicated than others to have enabled because you need to make sure you're minting test DOIs that won't require to be maintained in perpetuity etc, so I'll understand if it's not possible, but I just wanted to flag up that that would be a good thing to test from our point of view, thanks very much.\n\n  4. When will the Testathon begin?\n\n    1. It is currently scheduled for Jan 4-16th\n\n  5. I logged following conclusions in\n\n    The call participants agreed that:\n\n    - it is definitely a common question these days from end users and other stakeholders to support a form of markup. Not necessarily for all metadata fields, but at least for fields such as title and abstract\n    - Security is a more important concern than the support of markup. So any markup support should not come at the cost of risking XSS injection or other associated security risks.\n    - Complying to format requirements of external harvesters can be a more important concern than the support of markup for SOME institutions. This means being able to filter the markup out for harvesting purposes is important.\n\n  6. TODOs for next time:\n\n    \u2022 add your tests to the XMLUI Test Plan. Anyone can apply for, and get edit rights on this plan\n    \u2022 2016 DSpace 6 Testathon Testplan Working group:\u00a0Requirements for installation: Add those config options or features that are necessary for the tests\n    \u2022 Contributors of new features or improvements should be made aware that they can (or even should) add tests for their features to the test plan, once their pull requests are accepted.",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.674680233,
        "format_confidence":0.7483754754
    },
    {
        "url":"https:\/\/old.etherpad-mozilla.org\/UgIGNnOjfG",
        "text":"Web Animations minutes, 5 \/ 6 Dec 2013 Present: Alan, Doug, Shane, Steve, Brian Etherpad: https:\/\/etherpad.mozilla.org\/ep\/pad\/view\/ro.XgDJWjXpD2w\/latest Agenda: 1. Status updates 2. Interpolation of images. 3. Player changes 4. Naming of timing groups 5. Renaming PathAnimationEffect to MotionPathEffect? 6. Factoring out paced timing as separate to easing 1. STATUS UPDATES ================= Brian: - Player changes (see below) - Presentation at html5j: http:\/\/brian.sol1.net\/svg\/2013\/12\/04\/web-animations-html5j-2013\/ 2. INTERPOLATION OF IMAGES ========================== http:\/\/dev.w3.org\/fxtf\/web-animations\/#the--image--type The spec says to use CSS4 Images - http:\/\/dev.w3.org\/csswg\/css-images\/#cross-fade-function - but this requires the intepolation fraction to be limited to [0, 1]. We should add that the fraction is clamped to this range before using the CSS4 Images algorithm. > Actually, we should just replace the section by saying, \"Interpolation is defined in CSS4 Images\" 3. PLAYER CHANGES ================== The spec text is not quite right. Alan has provided some excellent feedback and Brian will try to fix the spec ASAP. Some discussion about whether Element.animate() should return an Animation or a Player? Brian still feels like Animation is right, but others are not sure. Use cases include: i. Building up groups: new TimedGroup( [ elem.animate(...) ] ); vs. new TimedGroup( [ elem.animate(...).source ] );, or allowing Players to be passed into TimedGroup lists.. ii. Adding onend elem.animate(...).onend vs. elem.animate(...).source.onend, or adding events to Players, or Object.observe()? iii. Modifying timing var anim = elem.animate(...) anim.specified.delay = .... vs. var anim = elem.animate(...).source; anim.specified.delay = ... On the other hand: elem.animate(...).pause() \/\/ build a paused animation elem.animate(...).currentTime = ... \/\/ CSS-style \"scrubbing\" > Still not entirely sure what Element.animate should return. Feedback is welcome. Discussed events with regard to where onend lives. If element.animate returns a player we should probably have onend on player as well which means we need to define player events. We will return to this discussion later when we try to solve events once and for all. 4. NAMING OF TIMING GROUPS ========================== (Brian) Original thread: http:\/\/lists.w3.org\/Archives\/Public\/public-fx\/2013JulSep\/0115.html Follow-up threads: http:\/\/lists.w3.org\/Archives\/Public\/public-fx\/2013OctDec\/0124.html http:\/\/lists.w3.org\/Archives\/Public\/public-fx\/2013OctDec\/0123.html Current proposals: A. SyncGroup and SequenceGroup (elements: and ) - Clear but too similar to each other? - 'Group' naming is generative and emphasises that they are the same kind of thing c.f. http:\/\/doc.qt.digia.com\/qq\/qq13-apis.html#theartofnaming \"Identify groups of classes instead of finding the perfect name for each individual class. For example, All the Qt 4 model-aware item view classes are suffixed with View (QListView, QTableView, and QTreeView), and the corresponding item-based classes are suffixed with Widget instead (QListWidget, QTableWidget, and QTreeWidget).\" B. TimedGroup and TimedChain (elements: and ) - Clear and distinctive - Not so generative, not quite so clear that they are the same kind of thing - What do you call them collectively: \"timed groups\" vs \"a regular timed group\" ? - Should it be TimingGroup \/ TimingChain ? TimedGroup \/ TimedChain ? TimeGroup \/ TimeChain ? (Brian: I accidentally got this wrong in my presentation and called it TimingGroup\/TimingChain although the original proposal was TimedGroup\/TimedChain) Other permutations: SyncGroup and ChainGroup ? TimedGroup \/ TimedSequence ? Some concern that TimedSequence is longer than TimedChain and not necessarily significantly clearer. TimedGroup \/ TimedChain seems preferable. Brian somewhat uncomfortable with \"Timed\" particularly in element syntax \"timedgroup\". 5. Renaming PathAnimationEffect to MotionPathEffect? PathAnimationEffect sounds like it animates a path's points, not moves an element \/along\/ a path. Also, unlike KeyframeAnimationEffect, you actually *do* type PathAnimationEffect. Suggested renaming - AnimationEffect remains the same as the common base class - PathAnimationEffect -> MotionPathEffect (based on the common concept of a \"motion path\" as used in, e.g. Office, After Effects etc.) - KeyframeAnimationEffect -> KeyframeEffect - CustomEffect remains the same Disadvantage: Naming no longer distinguish betweens custom effects and animation effects (PathAnimationEffect and KeyframeAnimationEffect inherit from AnimationEffect and are composited according to the prioritisation outlined in the model but custom effects are run later). > Sounds good. 6. Factoring out paced timing as separate to easing ======================================= The idea here is that pacing is a separate property applied to space out the animation values (be they keyframes or points on a path). Easing is applied *after* that. That allows you to pace a path and then ease the entire motion whereas currently you can't do that. For keyframes you'd have three modes: * Keyframes are evenly distributed * Keyframes are distributed so that the rate of change of a given property is constant (for properties where this makes sense) * Keyframes have their positions specified explicitly If the author specifies both \"paced\" and \"explicit offsets\" we need to determine the behaviour. Lots of edge cases to consider. For example, is it useful to be able to pace an effect but specify absolutely the offset of just one value? Decided that pacing should always override any offsets - all offsets are ignored for pacing What do we do when there are no keyframes with the pace property? This seems OK - you just get even distribution, as when any keyframes are missing that property. Need to decide which property to pace on. -- this would be a flag set at the effect level. Is there a default value? The first property in the first keyframe? Probably we should change: [Constructor (OneOrMoreKeyframes frames, optional CompositeOperation composite = \"replace\", optional AccumulateOperation accumulate = \"none\")] To: [Constructor (OneOrMoreKeyframes frames, optional EffectOptions options)] dictionary EffectOptions { ... DOMString distributeMode = \"equal\" ? \/\/ Other options being \"paced(left)\" etc. ? } Still some uncertainty about whether pacing should always win since in the above arrangement \"equal\" would only apply when there are no offsets but \"paced(left)\" would always apply. Needs more thought. Next meeting: same time next week",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9870807528,
        "format_confidence":0.7546049953
    },
    {
        "url":"http:\/\/community.nagios.org\/2009\/08\/17\/meet-the-community-installment-3-mathias-kettner-check_mk\/",
        "text":"Meet the Community Installment 3 \u2013 Mathias Kettner (check_mk)\n\nmathiasIn this installment of the \u201cMeet The Community\u201d series I interview\nMathias Kettner, author of check_mk \u2013 a unique addon for Nagios that greatly simplies monitoring remote system metrics.\n\nRead on for more an informative Q&A session on check_mk and monitoring scalability with Mathias.\n\nQ. Can you tell us a bit about yourself?\n\nA. I\u2019m from Munich in Germany and completed my studies in computer science in 1998. From Autumn 1999 to Spring 2001 I\u2019ve been working as a developer for SuSE in Nurnberg. During that time, amongst other things, I\u2019ve designed the system architecture for YaST2. Since I left\nSuSE I\u2019ve been a self-employed Linux specialist and offer consulting and workshops for Linux and Open Source. The topic \u201cMonitoring with Nagios\u201d is a main focus.\n\nQ. Can you give us a brief overview of what your project (check_mk) is?\n\nA. Check_mk is the product of years of Nagios consulting. Especially when you deal with large installations, the effort for creating and updating the configuration can be great. And of course at about 2000 checks, problems with the performance arise \u2013 sometimes even earlier.\n\nCheck_mk solves these problems by an astonishingly simple scheme. It uses its own very simply built agents. Their specialty: the agent is not called separately for each check but always sends everything it knows about its host. For each host only one active check is triggered by Nagios. That fetches all data from the host at once, interprets it and sends the results of the various services via Nagios\u2019 passive checks.\n\nQ. What are the primary advantages for a Nagios user to begin using check_mk?\n\nA. The most eye-catching advantage is that with check_mk much less work is needed to integrate new hosts into the monitoring. The agent \u2013 regardless whether for Windows, Linux or UNIX \u2013 does not need to be configured. The Linux and UNIX agent is a portable shell script that does not need any compiled programs. The detection and integration of the majority of services and the creation of the configuration for Nagios happens automatically.\n\nAs the number of hosts and services grows, the performance benefit of check_mk gets obvious. Check_mk makes tens of thousands of checks per minute possible \u2013 even if data is written to round robin databases.\n\nQ. check_mk has a unique architectural design compared to other remote monitoring methods. What inspired you to come up with this idea?\n\nA. The idea for that kind of architecture has its origin in the monitoring of UNIX systems. NRPE is especially intricate here since there are no precompiled packages, and compiling NRPE on various UNIX versions overburdens many administrators \u2013 as not only NRPE, but also the plugins have to be compiled.\n\nFor that reason I developed a method based on a shell script and the inetd. The idea of processing all of a host\u2019s services in one single run, and sending the results via the command pipe to Nagios, came out of the blue one year ago.\n\nLater I realized that this method can be extended nicely towards SNMP. When monitoring ports of a switch, check_mk processes the data from all ports in a single run. Furthermore it can detect which ports are in use (and thus should be monitored) and create Nagios services for them.\n\nQ. In your experience, do check_mk users usually replace their existing remote monitoring agents (NRPE, NSClient++, etc) with this addon, or do the use them together?\n\nA. In principal, check_mk can be used in parallel to all other monitoring methods to any degree as required. Once you\u2019ve worked with check_mk for a few days, you probably won\u2019t want to work with the\ndisadvantages of NRPE and NSClient++ any longer. A migration towards check_mk is usually fast. Single checks that are not easy to realise with check_mk, or where the effort of migration does not pay off, can be performed with a classic method in parallel to check_mk.\n\nQ. Are there any shortcomings of using check_mk instead of dedicated agents like NRPE and NSClient++? For instance, is there certain type of information or metrics that can\u2019t easily be monitored using check_mk and its architectural design?\n\nA. The architecture does not impose any restrictions. There are, however, a few cases where checks can be better implemented with the classical method. One of those are checks of network services like check_http, which do not require an agent.\n\nFurthermore, the Windows agent is not yet as flexible as those for Linux and UNIX. Currently it is not scriptable and can only be extended in C. The reason for that is not least my decision to directly program the agent using only the Win32-API. The advantage: the agent does not need .NET, Java or any other runtime environment \u2013 not even a special DLL \u2013 and is therefore perfectly portable.\n\nQ. You have installed check_mk for some of your clients with large IT infrastructures. How does check_mk help Nagios to scale? What is the largest installation of check_mk that you know of?\n\nA. The largest installation currently performs 17,500 checks per minute, and is soon going to be expanded by several hundred Windows servers.\n\nThe load on the 4 CPU machine is currently at 6 \u2013 whereas the majority of CPU time consists of IO wait (Linux includes processes waiting for disk IO in the load calculation). RRD-data is written at a rate of about 5 MB per second. When deploying the RRD cache, I assume 30,000 to 40,000 checks per minute to be achievable.\n\nQ. How long have you been working on check_mk?\n\nA. The current implementation \u2013 in Python \u2013 found its origin about a year and a half ago. Check_mk is available under GPL since the end of April 2009.\n\nQ. You are the primarily developer of check_mk. Are there other developers or contributors to the project?\n\nA. Currently I\u2019m the only one working on the actual programming. A decisive part of the success and maturity of the project has been due to one of my customers \u2013 Karl-Heinz Fiebig. He is the master of the largest installation, produces many good ideas and is the first one taking the blame for my bugs. He fixes many problems in the code himself.\n\nQ. How did you first come to know about Nagios and why did you decide to begin using it?\n\nA. I did my first project with Nagios in 2003. In order to save costs an open source monitoring system was desirable. At that time Nagios was already the most prominent system of its kind. It could already fullfil all of the project\u2019s requirements.\n\nQ. What do you see as being the most advantageous reasons for using Nagios?\n\nA. Nagios is very flexible and allows the implementation of very individual monitoring solutions. Also the price is an important factor, because commercial monitoring systems still involve large licensing costs.\n\nQ. Are there specific changes to Nagios that you\u2019d like to see made in order to make check_mk integration simpler?\n\nA. The integration of Nagios and check_mk leaves nothing much to be desired. All necessary interfaces are present and they are also very simple and efficient.\n\nQ. Are there any resources that you require in order to continue working on or improve the project?\n\nA. Most helpful are, on the one hand, projects for customers where I implement monitoring solutions with Nagios and check_mk, and thus further develop the software. On the other hand feedback from the community is very important \u2013 regardless of whether it\u2019s qualified bug reports, suggestions for important requirements and \u2013 last but not least \u2013 promotion for the project in the form of links, postings in forums, documentation in foreign languages and the like.\n\nQ. What plans do you have for the future of check_mk?\n\nA. The most important issue is surely the expansion of the documentation. Furthermore I\u2019d like to make the Windows agent more easily expandable. For this reason I\u2019m still in search of a really persuasive idea that fits into the current minimalistic concept.\n\nFor more information on check_mk, visit http:\/\/mathias-kettner.de\/check_mk.html\nMathias can be contacted at: checkmk@mathias-kettner.de\n\nBookmark and Share\n\n1 Responses to \u201cMeet the Community Installment 3 \u2013 Mathias Kettner (check_mk)\u201d\n\n  \u2022 Greetings,\n\n    Thanks for the article. I had never heard of check_mk, the idea intrigues me. We are a Windows shop using Nagios with NRPE and NSClient++. We have done this for sometime with good success, however, I love the idea of a more prepackaged option that does NOT rely on other Windows components such as .net, wmi, etc. We will check it out, this may be a good data collector compliment\/replacement with what we are using now.\n\n    Thanks again for the info,\n\n\nLeave a Reply\n\nYou must login to post a comment.",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.7376670241,
        "format_confidence":0.974540174
    },
    {
        "url":"http:\/\/lists.w3.org\/Archives\/Public\/www-ws-desc\/2004Mar\/att-0296\/minutes-wsd-2004-03-25.html",
        "text":"Web Services Description Working Group call\n25 Mar 2004\n\n\nSee also: IRC log\n\n\n Erik Ackerman          Lexmark\n David Booth            W3C\n Allen Brookes          Rogue Wave Software\n Roberto Chinnici       Sun Microsystems\n Paul Downey            British Telecommunications\n Youenn Fablet          Canon\n Yaron Goland           BEA Systems\n Hugo Haas              W3C\n Tom Jordahl            Macromedia\n Jacek Kopecky          Systinet\n Amelia Lewis           TIBCO\n Kevin Canyang Liu      SAP\n Jonathan Marsh         Chair (Microsoft)\n Jeff Mischkinsky       Oracle\n Jean-Jacques Moreau    Canon\n Arthur Ryman           IBM\n Adi Sakala             IONA Technologies\n Jerry Thrasher         Lexmark\n William Vambenepe      Hewlett-Packard\n Asir Vedamuthu         webMethods\n Sanjiva Weerawarana    IBM\n Umit Yalcinalp         Oracle\n Prasad Yendluri        webMethods, Inc.\n Philippe Le Hegaret    W3C\n Glen Daniels           Sonic Software\n Ingo Melzer            DaimlerChrysler \n Bijan Parsia           University of Maryland MIND Lab \n Igor Sedukhin          Computer Associates\n\n\nMinutes approval\n\n<Scribe> Approved\n\nReview of Action items\n\n3.  Review of Action items [.1].\nPENDING   2003-09-18: Marsh to review the QA operational\nPENDING   2004-01-08: Pauld to write up examples of schemas for the\nPENDING   2004-01-28: Philippe and JMarsh will look at the ipr for \n                      test suite.\nPENDING   2004-01-30: DaveO to write up a proposal for augmenting \n                      schema information to enable versioned data.\nPENDING   2004-02-12: DaveO to produce a refined proposal for Asynch \n                      HTTP binding addressing the concerns of folks \n                      that object to leaving replyTo info out of WSDL.\nDONE [.2] 2004-03-04: Editors to add back the WSDL Component \n                      Designators back to the spec as SHOULD.\nDONE [.6] 2004-03-05: Editors to write in the spec the wsdlLocation\n                      global attribute proposal along what has been \n                      done for xsi:schemaLocation.\nPENDING   2004-03-05: DavidO to notify TAG about our decision on safety.\nPENDING   2004-03-05: Editors of media type proposal to give Jonathan a \n                      list of open issues.\nDONE      2004-03-11: Sanjiva to fill in data for May FTF on logistics \nPENDING   2004-03-11: JMarsh, David Booth, David Orchard to form adhoc \n                      group to explore stylistic rendering options.\nDONE [.3] 2004-03-18: Editors to clarify the meaning of 'change' WRT to\n                      optional extensions.\nDONE [.4] 2004-03-18: Editors to implement David Booth's proposal on \n                      making all notes non-normative and moving \n                      the normative text into the regular\nDONE [.5] 2004-03-18: Editors to incorporate Jonathan's proposal for \n                      issues 146 & 150 (with #empty changed to \nDONE [.6] 2004-03-18: Editors to make binding\/operation\/(input|\n                      output)\/@messageLabel optional and have the \n                      same rules as the corresponding thing in\n                      interface\/operation for computing the value.\n\n[.1] http:\/\/www.w3.org\/2002\/ws\/desc\/#actions\n[.2] http:\/\/lists.w3.org\/Archives\/Public\/www-ws-desc\/2004Mar\/0196.html\n[.3] http:\/\/lists.w3.org\/Archives\/Public\/www-ws-desc\/2004Mar\/0193.html\n[.4] http:\/\/lists.w3.org\/Archives\/Public\/www-ws-desc\/2004Mar\/0188.html\n[.5] http:\/\/lists.w3.org\/Archives\/Public\/www-ws-desc\/2004Mar\/0195.html\n[.6] http:\/\/lists.w3.org\/Archives\/Public\/www-ws-desc\/2004Mar\/0284.html\n\nSanjiva: I want to propose to specify the semantic of any as any *one* element\n\nJonathan: Gudge sounds interested in allowing a sequence of elements, but I don't know why\n\n\nJonathan: the week of August 2nd looks best\n\nwe should decide on the length of the meeting; a 4-day meeting is possible\n\nTom: 4 days sounds like a long time\n\nJonathan: I would like to have a Tuesday-Thursday meeting\n\nArthur: are we collocating with another Group?\n\nJonathan: no\n\nWilliam: Is it possible to do a noon to noon meeting on 4 days?\n\nJonathan: if we don't do it in London, I could look into organizing it in Redmond\n\n\n<dbooth> Hugo: P3P allows a privacy policy to be expressed in machine-readable form. The P3P WG is working on P3P 1.1, a generalization of 1.0, to random XML vocabularies.\n\n<dbooth> Hugo: They released first WG in Feb. We wrote a team submission that shows how a generic P3P attribute can be attached to an element in WSDL and what it would mean.\n\n<dbooth> Hugo: The first part shows abstractly what it would mean to attach a privacy policy to a WSDL 2.0 component. The second way shows two concrete ways to do it.\n\n<dbooth> Hugo: More recently I saw email from Glen about Features and Properties that seems to indicate that F&P is more restrictive.\n\nPhilippe: this is an experimentation at this point\n\nJonathan: so the output would be a list of issues again the spec\n\n<asir> http:\/\/www.w3.org\/TeamSubmission\/2004\/SUBM-p3p-wsdl-20040213\/\n\nJonathan: volunteers?\n\n<dbooth> Hugo: (Answering \"Why are there 2 ways to do this?\") To illustrate that it could be done in two ways, and illustrate the differences between them.\n\nTom: it seems that you have found that the use of attributes is more flexible than the use of features and properties\n\n<sanjiva> +1 to using this example to learning about F&P vs. extensions ..\n\n<Scribe> ACTION: Hugo to look at the Handling Privacy in WSDL 2.0 for issues against our spec\n\nJonathan: people are encouraged to read and review this informally\n\nReview of the XML Schema PER\n\nJonathan: Volunteers?\n\nwe have 3 weeks to do this\n\nPaul: I am interested in having a look\n\nAsir: I haven't seen anything which would be a problem to us\n\n<Scribe> ACTION: Paul to review the XML Schema PER\n\nTask force status\n\nJonathan: I have seen some emails by the Media type description\n\ntask force\n\nUmit: I am almost done converting the document\n\nthe plan is to publish as is, and then modify Anish is now officially the co-editor on the XMLP WG side\n\nJonathan: XMLP has more constraints than we do on this\n\nthey should be driving the timing the Schema versioning task force still needs some work on the charter to be started\n\nNew Issues. Issues list\n\n<WilliamV> zakim ??P18 is WilliamV\n\nJonathan: we have Issue 157: f&p at the service level\n\n#any should mean \"any element\" not \"any thing\" has several proposals for it\n\nArthur: somebody said that they wanted to use simple types\n\nYaron: a good test for our feature set: would our declarative model permit to describe any allowable SOAP content?\n\nPaul: security is a good use case; with lots of pointers\n\nJacek: I would like somebody to champion the use cases for this\n\n<sanjiva> I'd like to propose that we limit element=#any to mean *any one element* for right now. If someone wants to generalize it further then can raise an issue and\/or make a proposal.\n\n<Roberto> +1\n\n<TomJordahl> +1 to sajniva -\n\n<alewis> +1 to sanjiva's proposal.\n\nUmit: if we are saying that we have an element-based model, we should just stick to the model\n\n<umit> +1 to Sanjiva.\n\nArthur: do we want a more flexible message model, or have a simpler message model and force the work into the binding?\n\n<prasad> +1 to sanjiva\n\nSanjiva: I'd like to propose that we limit element=#any to mean *any one element* for right now\n\nJonathan: would anybody object to changing our status quo as proposed by Sanjiva?\n\n<Scribe> no objections\n\nRESOLUTION: limit element=#any to mean *any one element*, and open a new issue\n\nJonathan: Jacek proposed some \"paths\" to detail processor conformance\n\nDavidB: I dropped Jacek's proposal accidentally when integrating the text\n\nI think that we need to do more work on wording in the spirit of Jacek's proposal\n\nUmit: if we want to adopt Glen's description, we will be in a place where we'll be arguing about which MEP will be in the conformance set, etc.\n\n<sanjiva> ACTION: editors to update draft to change the semantics of element=#any to mean *any one element*\n\nUmit: I think that we should be concerned only about document conformance and leave out processor conformance\n\nAmy: if we are going to talk about processors at all, then we need to make it clear that a processor that isn't going to go a particular path shouldn't throw certain errors\n\nRESOLUTION: open a new issue about processor conformance \"paths\"\n\nMIME Binding\n\n<dbooth> I think we need to think of this (or define this) in terms of dependencies, but we need to be clear and careful in defining those dependencies.\n\n<sanjiva> +1 to closing non-existant MIME binding issues\n\nJonathan: we don't have a MIME binding in our charter\n\nI would propose that we close those issues\n\nPrasad: how do you describe attachments at the binding level?\n\nJonathan: we haven't started working on MTOM; when we do, we will know the answer to this question\n\nRESOLUTION: issues 58 & 59 without action\n\nAmy: will we be able to describe attachments without SOAP?\n\nJacek: I think that it's out of scope\n\nSanjiva: isn't this related to the media type work?\n\nPhilippe: I believe it's different\n\nmy plan was to support XOP in the HTTP binding\n\nIssue 66: How to represent the equivalent of hypertext links?\n\nJonathan: I believe that this was asking for service references and that it is therefore satisfied\n\nArthur: we can close it on the understanding that you can't address XLink\n\nRESOLUTION: issue 66 closed\n\nSupport for SOAP 1.1\n\nJonathan: this is in our charter now\n\nI have a volunteer: Asir I am appointing him as our editor for the SOAP 1.1 binding editor what is our timeline?\n\nArthur: we just need to have it by the time we get WSDL 2.0 spec out\n\nAsir: I think this can be done in parallel, and that it goes along with part 3\n\nJeff: I think that we need to give priority to SOAP 1.2\n\nJonathan: how long do you think this will take?\n\n<sanjiva> +1 to bring forward the old binding, appropriately subsetted per Jack's suggestion\n\nJacek: I would just subset our SOAP 1.2 binding and change the namespace\n\n<TomJordahl> I agree w... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9780032635,
        "format_confidence":0.9215782285
    },
    {
        "url":"https:\/\/py.checkio.org\/blog\/interview-star-player-cjkjvfnby\/",
        "text":"\u2022 An Interview with Star Player cjkjvfnby\n\nAugust's Star Player is the renowned cjkjvfnby. CheckiO elite coder and founder Alex recently had a conversation with him about what cjkjvfnby likes about being a programmer.\n\nAlex @ CheckiO Tell me about yourself for a little bit. Where are you from? What country or region, what city?\n\ncjkjvfnby Hi, My name is Andrey. I am from Saint-Petersburg Russia.\n\nAlex @ CheckiO Do you attend a University or a are you coding professionally for a company?\n\ncjkjvfnby I am coding professionally in python and java\n\nAlex @ CheckiO What was your first experience with a computer?\n\ncjkjvfnby My first computer was ZX spectrum, which was assembled from parts by my uncle. I drew some circles and lines in BASIC with it. My interest in programming came much later, about 6 years ago.\n\nAlex @ CheckiO What was the decision that lead you to become a programmer?\n\ncjkjvfnby I searched for my path for a long time, changed a lot of jobs, and finally found some IT job and IT courses.\n\nAlex @ CheckiO We've seen that you're pretty active in the community, what do you focus on when looking at the solutions of other players?\n\ncjkjvfnby I am looking at other solutions to see what I might have missed in mine. I often try to convince others to write more readable code.\n\nAlex @ CheckiO When looking for new team members, what are the key factors for you?\n\ncjkjvfnby Positive mind, willingness to learn, desire to do well, and following a project style.\n\nAlex @ CheckiO What advice do you have for new programmers who are just starting to learn?\n\ncjkjvfnby Code a lot. Publish. Read other solutions. Use python in real life to speed up common tasks :)\n\nWelcome to CheckiO - games for coders where you can improve your codings skills.\n\nThe main idea behind these games is to give you the opportunity to learn by exchanging experience with the rest of the community. Every day we are trying to find interesting solutions for you to help you become a better coder.\n\nJoin the Game",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9183784127,
        "format_confidence":0.9845803976
    },
    {
        "url":"https:\/\/origintrail.community\/origintrail-development\/origintrail-beta-program-progress-report-and-discussion\/",
        "text":"August 7th, 2018\n\nToday, OriginTrail developer Vladimir dropped by on the OriginTrail telegram chat and had a short discussion with some community members. He opened his discussion with a short recap of the monthly report:\n\nProtocol Development: Beta Program Progress Report\nThe launch of the OriginTrail Decentralized Network (ODN) testnet represents the beginning of our beta program which will last until the launch of the OriginTrail mainnet. The purpose of the beta program is to gain as much insight as possible into the operation of the nodes and the network in different node environments and network conditions, both in terms of the ODN as well as the blockchain. By participating in the beta program, you help our team enormously by observing and reporting visible issues with the operation and user experience of your node. You are also helping indirectly because automatic logs about your node are sent to our analytics tools, which lets us analyse information on a daily basis. It is important to note that, even when the beta program ends, the testnet will stay live as a free development environment for anyone wanting to build on OriginTrail, much like how the various testnets of Ethereum are used.\n\nSo far, we have released several versions of the node client to test out different scenarios in the network setup. Some of these have introduced significant changes which we have been communicating with active node holders through our beta program channels, mainly Discord. One month into the beta program, we have observed several interesting KPIs, such as the number of nodes present on the network, which peaked at 174. Our team has been running additional tests with spawning significant numbers of our own nodes as well, in order to run simulations at different scales of the network. The testnet is therefore being exposed to periodic data uploads which are used to test out the bidding mechanism, replication and reading capabilities. So far, the success rate of finalized offers has been very high (above 90%), taking into account that several important changes have caused the network to be in a state where all offers can not be completed. Additionally, we have observed a very low number of litigation procedures, which is measured in single digit numbers per day.\n\nOne important note is that the market conditions are still not very insightful, as the network itself is operating with test tokens and test ether. The focus right now is to ensure the proper operation of underlying mechanisms that should enable a stable environment (and thus, market). Therefore, the main targets for the upcoming period of the beta program is to further secure network stability and lower the transaction costs associated with interactions with the blockchain. The team is already working on a solution that will be able to lower the amount of transactions, minimize the amount of transactions that do not result in compensation, optimize smart contracts and introduce further flexibility in the protocol. We will be publishing updates and documentation in the coming weeks which will explain improvements to be introduced.\n\nWe are also working on the existing codebase to make it more flexible, scalable and, above all, failure-resilient, as the stability of the nodes in the production environment (main network) will be a top priority. We released several minor releases during July and we are currently working on a major update of the code, which will introduce a command sourcing pattern\u200a\u2014\u200aa very significant improvement. Click here to read the monthly report.\n\nCommunity member Snapper asked Vladimir some questions:\nSnapper: Thanks Vlad, for those of us less tech savvy. What is a \u201ccommand sourcing pattern\u201d?\n\nVladimir: It is a very cool thing. It means that we are making every single operation of the node as command and then we can track if command is executed, and if not, we can replay it. So for example, you restart the node in the middle of something. When you start it again, it will detect and redo.\n\nSnapper: so that means if there is a problem somewhere you can find the problem more easily?\n\nVladimir: Yes, an easier programming as well. But not only that. It means that you don\u2019t loose stake \ud83d\ude42 Also it prevents all possible problems that may apear if you have corrupted database. Actually it prevents corrupted database. So it is very important. But will also speed up development significantly. There are so many numerous benefits of this approach.\n\nKirk: What is a litigation procedure ?\n\nVladimir: Litigation procedure is when one node is trying to trick another node, then the Smart Contract will judge which one is honest. For example we run offer on one, then start to kill and restart all of the nodes and they completely recover.\n\nSnapper: So how many databases are you using in total then?\n\nVladimir: Just two \u2013 Arango for data and Sqlite for system information.\n\nSnapper: I think someone a while back was asking about level up \/ level down\u2026 is that still present or did it change since then?\n\nVladimir: Yes, it was in the beginning. We replaced it with Sqllite. So no Level up \/ Level down anymore. We needed relational database.\n\nSnapper: Do you intend on removing the dependencies for the other database libraries from the package.json at all?\n\nVladimir: Yes, we do some maintenance once in a while. For next major release for sure!\n\nLeave a Reply\n\nYour email address will not be published. Required fields are marked *\n\n\n\n\nThis site uses Akismet to reduce spam. Learn how your comment data is processed.",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9581384063,
        "format_confidence":0.5007886887
    },
    {
        "url":"https:\/\/www.infoq.com\/interviews\/Bossavit-agile-knowledge",
        "text":"Laurent Bossavit on Creating an Archive of Agile Knowledge and the Art of Being Wrong\nRecorded at:\n\nInterview with Laurent Bossavit by Shane Hastie on Dec 28, 2013 |\n\nBio Laurent Bossavit is mainly known as an Agilist and was a recipient of the 2006 Gordon Pask award from that community. He still likes to code though no longer doing so full-time, dividing his attentions between assisting Agile teams on technical and organizational matters, and the Institut Agile project which collects empirical evidence on the benefits and limitations of Agile practices.\n\nEach year Agile Alliance brings together attendees, speakers, authors, luminaries, and industry analysts from around the world in a one-of-a-kind conference. The Conference is considered the premier Agile event of the year, and provides a week-long opportunity to engage in wide-open interaction, collaboration and sharing of ideas with peers and colleagues within the global Agile community.\n\n\n1. [...] Could we start by talking a little bit about your engagement with the Alliance and what the role of the Archivist is?\n\nShane's full question: Good day, this is Shane Hastie with InfoQ, we are here at Agile 2013 with Laurent Bossavit, welcome. Thank you very much for taking the time to come and talk to us today. You are a board member of the Agile Alliance, a long time Agilista and you currently also have a part time role as the Archivist for the Agile Alliance. Could we start by talking a little bit about your engagement with the Alliance and what the role of the Archivist is?\n\nI served in the board for about 4 years, we were hashing out Policy and Vision back then which actually in retrospect, I don\u2019t think that role really suited me, back when I was decided to get involved I was more interested in actually doing stuff and since I\u2019m a programmer, one of the things I thought I might be doing was writing code for the Agile Alliance and not doing any of that for four years but an opportunity came along after a while to actually help the organization with I think some very unglamorous housekeeping stuff like having a site where we could pull together the session materials and descriptions from all of the conferences. That didn\u2019t exist back then, so we had problem I think still many conferences are having now where as the new conferences comes along and people get excited about that and forget about the previous years and the domain name goes down and maybe someone didn\u2019t keep backups, some stuff that is precious to some people just vanishes basically, so I wanted to prevent that from happening so came up to Phil [Brock] one day and said: \u201dWould you like me to be the Agile Alliance Archivist?\u201d and that is where it\u2019s started.\n\n\n2. So what are we putting on that archive?\n\nEvery session abstract and session types and author names, if you are interested in seeing what talks were given by Bob Martin or Andrea Freeze or me, you could just bring up the search box and have that list, so you could also search within session descriptions and get the slides. It\u2019s very basic stuff but we didn\u2019t have that.\n\n\n3. And you are also building a repository of articles and knowledge within the Agile Alliance website as well, can you tell us a little bit more about that?\n\nOne of the reasons for doing that was people I met, my day job is as a consultant and clients, practitioners, were always asking where can I find some kind of basic reference material, glossary and I need to know what this strange name practice is about in a very short space, I don\u2019t have time to read the book and seems to me like was the Agile Alliance job to put that kind of thing out. So I spent some time working on that, the reason was the guide to Agile Practices, so it\u2019s one thing, and now we are trying to see what kind of supplementary material would be a good complement to that, so we now have a blog where inviting some people to come in as guest bloggers and we are slowly building up that corpus of content.\n\nShane: One of the things that I record seeing on there is a very interesting \u201csubway map\u201d, tell us a little bit about that, what looks like a subway map.\n\nYes, I was looking for some kind of organizing metaphor, some kind of visual representation and one of the things I wanted to be faithful to was the various practices that made up this guide, that was only a first cut, an initial list, it\u2019s kept growing over the years and so a revision is in order to count for that expansion now, I will get around to it someday but the map was a way to pay homage to the communities that the ideas for the practices came from, so one of the main lines was the XP Line because that is the first I learned about, the first community where I learned about Agile Practices. And obviously the other big line was the Scrum Line and so as I start thinking along those lines, pun not intended, I thought well maybe some kind of geographical metaphor would fit and I used, I stole a JavaScript Library for something that look too little like the well known London Tube Map which is a very iconic thing, everyone recognizes that instantly, I thought that is the kind of thing I would like to people to come away with, to have some kind of image of: \u201cThis is the town of Agile\u201d, if you will, \u201coh my, how it\u2019s grown\u201d. I wanted to get that across.\n\n\n4. You are talking at the conferences this year, your talk is titled \u201cThe Art of Being Wrong\u201d; please won\u2019t you tell us a little bit about that?\n\nAll right, so to switch tracks because it\u2019s actually related to the work I\u2019m doing with the Agile Alliance but in very round-about ways, maybe will get to bringing up the relations, but that talk is a session that have been doing in various venues several times this year so that is kind of a change for me because so far I have liked to do different talks at different conferences and mixing things up a bit. This is a topic that I\u2019m really passionate about, it came of my interest in the research of a fellow named Phillip Tetlock who wrote about how experts get things wrong, even people who are professional experts, people who are paid at making predictions. I find that research very interesting and his setup a research project to investigate that with normal people and people who are not paid to act as experts and to make prognostications basically.\n\nSo I enrolled into that program, I learned a lot of things, I\u2019m not going to the details of the sessions but it\u2019s about making predictions about future events. Therefore unknown uncertain events and that turns out to be, it\u2019s a very interesting topic, it\u2019s closely related to a number of things which are important in Software Development; chief among them estimation, estimation is about quantifying some kind of prediction about a future event - specifically how long it\u2019s going to take to develop some piece of software. So that having long being a pain point for me in my practice of Software Development, I wanted to investigate that area. I ended up over a couple years spending many, many hours, countless hours, really invested in that investigation in finding out how you could improve at forecasting things within a fairly specific technical framework. So that took on the aspect of Deliberate Practice and I thought that having invested so much time in that, I might as well at least get a talk of it and so I started putting together some slides, some presentation and I thought it\u2019s of interest to software people.\n\nI was wondering if it was also, if could have broader applicability and I got some interesting opportunity to test that. The first time I actually did the session was at a Scrum Meeting in Paris, back in September last year, and it was one of those Indian summers, weather was still very nice and we had a meeting on a Saturday and we decided to have the meeting in the woods of the Fountain Bleu, and because the weather was nice, one of us said: \u201cCan we bring our kids?\u201d and so a bunch of us decided to come with our kids. Quite unexpectedly I ended that doing that session with an audience which was about ten grownups and ten kids including three of my own. If you think you\u2019ve seen a tough audience, one that is stressful wait until you\u2019ve had that kind of setup. I was completely unsure how they will take it so I took some time at the beginning, to say you won\u2019t hurt my feelings if you walk away or go have fun somewhere but they actually ended up staying the whole hour; from 6 years old to 60 years old, so I had to improvise on motivating the topic of the talk so I started with: \u201cHave you ever seen your parents fight?\u201d and yes, a bunch of them raised their hands, and I addresses the parents. Has that happened? Sure, it happens, why does it happen? Because a lot of the time the culprit is Black and White thinking. If both people in a fight are typically convinced, absolutely convinced that they are each right and the other wrong. So that was the starting point and the session built on that and introduces the Art of Being Wrong.\n\n\n5. So there is no Black and White?\n\nNo, it\u2019s very much training in thinking in shades of grey.\n\n\n6. Another thing that you being busy with likely is a book that you are putting together and publishing incrementally, on Leanpub, the Leprechauns of Software Development. I\u2019ve read what is there so far and really enjoy it, but would you mind telling our listeners, our audience about that?\n\nYes, so to start with I love Leanpub, I think it\u2019s a very cool concept, it\u2019s kind of obvious in retrospect it\u2019s one of those things and until someone comes up with it, it\u2019s not so obvious. What I like about it is I\u2019m able to have my book out there, people are buying it, downloading it, giving me feedback, of which goes into giving me new directions to think about. Meanwhile I have absolutely no deadline pressure, there are 160 pages out there, I\u2019m writing more stuff but I\u2019m putting that mostly on my blog and waiting for the right time to collate and organize the material and it\u2019s really way better than the one... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9799656868,
        "format_confidence":0.9894932508
    },
    {
        "url":"https:\/\/twit.tv\/posts\/transcripts\/coding-101-62-transcript",
        "text":"Coding 101 62 (Transcript)\n\nNet casts you love from people you trust. This is Twit. Bandwidth for Coding 101 is provided by Cachefly at\n\nFather Robert Ballecer: Today on Coding 101 it\u2019s a brand new module. Folks, we\u2019re jumping into C. Welcome to a very abrupt Coding 101. I\u2019m Father Robert Ballecer, the Digital Jesuit and I\u2019m joined by the man who is the Code Warrior but as of this segment is our super special guest co-host. Mr. Lou Maresca, Lou we\u2019ve got to find a new title for you because this is going to get ridiculous.\n\nLou Maresca: Thanks for having me back though still.\n\nFr. Robert: Now we wanted to do something a little bit different because we know that a lot of our audience really enjoyed our \u201cDo we know\u201d segment so we did a module with Mark Smith from DEFCON who showed us how to make a steam punk our do we know clock. That\u2019s just C programming and I talked to Lou and he said \u201clet\u2019s do a mini module. Let\u2019s go ahead and give people the basics of C so that they could do programming for our do we know or a mega chip or any other device that may use standard C\u201d. Lou is that about right? Can we give that to the folks?\n\nLou: Yes, what we should do is start off from the very basic beginning and try to give them just the fundamentals of what C is all about. It\u2019s a very legacy, it\u2019s 1 of those things that\u2019s legendary in the programming industry and we want to make sure that they get the basics as well.\n\nFr. Robert: Right. Now before we jump into the module there are those who will look at a C module and say well why? I don\u2019t understand why, there are so many other languages that have surpassed C, that have replaced C as the standard. Why would we want to give people C? To them what would you tell?\n\nLou: You know C is alive and well in the industry. Like you saw people are using all these different Meta devices but they\u2019re still allowing C programming and the reason is it is just raw speed. Without the overhead of object oriented programming. It\u2019s just raw speed. Programmers who develop in C can develop very fast and efficient code and I think it\u2019s probably not one of those things that\u2019s going to go away any time soon. Its standards keep getting upgraded; like for instance there are the old standards but now there are some new standards that are adding what we like to call syntactic sugar in the actual logic and in the code that allows you to kind of do things today that you couldn\u2019t do in the original C code. So it\u2019s still alive and well and it\u2019s still being used by lots and lots of people in the industry.\n\nFr. Robert: Yes there was actually a host here in the studio who passed by the desk and he passed me this book and he said C is still the best language to learn, it strips away all that sugar as you said, strips away the framework and all the layers of abstraction and really gets you down to knowing your concepts which is what we\u2019re going to do. But before we do that Lou why don\u2019t we find out what\u2019s going on in the world of programming.\n\nLou: Let\u2019s do it.\n\nFr. Robert: This little story comes to us from Science Daily. Now we\u2019ve known for a while that in the world of computer science especially if you\u2019re teaching or learning it then you\u2019re going to get a lot of assignments that look at solutions. So we just want to know if you were able to solve for the problem that they gave you. Now that\u2019s relatively easy if the program works and if it gives you the right output that\u2019s great and if it takes the right input that\u2019s great. But there\u2019s an element that\u2019s been missing and that\u2019s that you can\u2019t tell from whether or not someone has the right solution if they have it. Now Lou we\u2019ve talked about this. We talked about this with Steve Gibson, we talked about this with Smitty. This idea that there are people who can put code together but then there are people who are true programmers, who can think of unique ways to reach a solution. Ways that maybe haven\u2019t been taught to them but that they conjured. That\u2019s an \u201cit\u201d factor. You look for that when working with your employees\u2019 right?\n\nLou: Definitely. We look for patterns especially in how people code for sure.\n\nFr. Robert: Well the researchers over at MIT say they may have figured out a way to automate that search for \u201cit\u201d. Now this month\u2019s ACM \u2013 that\u2019s association for computing machinery if you\u2019re a CS major you probably are a member of that organization. Their conference on human factors in computing systems \u2013 well they were able to create a computing system that they call \u201cOver Code\u201d that takes solutions presented by students and turns them into templates and then puts them side by side so you can see where they differ. Now the interesting thing about this is that they look first at variable names, then they look at the value of variable names. So even if you don\u2019t use the same variable names they\u2019ll look at the procedure that they used to compute a solution and then the look at the presence of sub function. In order to test this what they did was they got 24 expert programmers and they had them look at a few 1000 homework assignments. Beginning CS homework assignments and they identified specific patterns \u2013 people who tackled the problems in different ways. What they found was that Over Code was not only able to have all the same patterns as those 24 experts but it was able to do it in a fraction of the time. The creators of Over Code have said that the differences get even more pronounced as you get into more difficult solutions, more difficult problems and more difficult assignments. Lou let me ask you this. If you had a system at your disposal that you could run against the code base, the code portfolio of anyone who might be applying for a job at Microsoft and it could tell you if they\u2019re cookie cutter programmers or if they were programmers who showed true imagination and initiative, would that change the way that you hire?\n\nLou: You know I think it all depends on the type of question you\u2019re asking. If you\u2019re asking for a very kind of ad hoc question to somebody and they provided some kind of solution that you wanted figured out I think that would be very useful but most interviews today at least the ones I conduct are giving some common problems that people see in the industry. So they\u2019ll come in and they\u2019ll answer those questions and you can basically tell up front if they\u2019re following what the text book said or if they come up with some very unique maybe even more performance solution. So if you\u2019ve done programming for a long time you could basically tell but I think like you said if you do an ad hoc solution I think this would be really helpful to tell \u2013 did they copy this from somewhere else, did they maybe do a bunch of practicing before they came in or did they just come up with it. That would be very useful during an interview.\n\nFr. Robert: Lets jump into the module but first I want to go ahead and bring up this again. This isn\u2019t just something that Leo handed to me off the cuff. This is the book that he thinks is the C bible. He said this is his 2nd copy of Kernighan and Ritchie. Wow, I actually have this but in a different color. This is sort of the C bible. If you are interested at all in programming in C pick up a copy of this. You can still find this on Amazon and use it as your reference. Let\u2019s jump in. Lou, where do we start?\n\nLou: Well you held up what you need to start with. Dennis Ritchie and Ken Thomason came up with C during the Unix operating when they were developing that and then Brian however you pronounce his last name \u2013 Kernighan or Dennis Ritchie wrote down that prescription language and they produced that book. That\u2019s still today one of the best descriptions of the language that you can find. There\u2019s been people writing books year over year and they just never ever get even close to the description of C that they\u2019ve done in that book. So it\u2019s kind of the ultimate bible of C. Now there are some better, newer implantations of that, of versions of the book and some of them are out there. We\u2019ll put them in the show notes but there are some new standards. There\u2019s the new ISO C99 and the C11 standards that have come out in about the last 10 years for C that have improved the language quite a bit and that\u2019s not going to be in that book that you show there. So even some of the newer editions don\u2019t have that stuff in there so that would be kind of the place to start. Then the other thing that you have to do is you have to decide on what platform you\u2019re going to code on because C is universal and can be going across\u2026 you can develop something in C on Linux and then be able to compile it and use the same code on Windows but there are many different what we call compilers and we\u2019ve talked about this in the past where you have for instance Microsoft\u2019s compiler and you also have the GCC compiler or the claim compilers or the compilers that you can use on Mac or OSX or the ones at Windows and that kind of thing. So we wanted to go in really fast and talk about the version of the compiler that you can use that you can actually use across all the different platforms. It\u2019s pretty easy to set up.\n\nFr. Robert: Now let\u2019s go over some of the basics of C. Of course it\u2019s a compiled language so there is no interpretations here. It\u2019s not 1 of those hybrids, it\u2019s pure, straight, and simple compiled. You give it code and it\u2019s going to give you a binary and an executable. It\u2019s also a strong type of language. We don\u2019t have dynamic variables like we would in say C Sharp. You have to declare what type of data you\u2019re going to be putting into your variables.\n\nLou: Correct. It\u2019s basically very similar to some of these upper level or higher level languages like C Sharp and Java but again when you declare its type you know exactly what size it is going to be and how much memory it needs to take at the very raw level, at the machine code level. So for instance an integer is going to take 8 bytes and that kind of thing. So when you declare something as an integer and it com... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9878880382,
        "format_confidence":0.9731078148
    },
    {
        "url":"http:\/\/irclogs.shortcircuit.net.au\/%23circuits\/2013-03-14.log.html",
        "text":"IRC Logs for #circuits Thursday, 2013-03-14\n\n*** Osso has joined #circuits00:39\nOssogoogle reader is being shut down :(01:47\nprologicI nkow01:48\nprologicI'm outraged01:48\nOssoeverybody here too01:48\nprologicOsso, you got time to help me fix unicode support for headers?01:48\nprologicoh good!01:48\nprologicCan you have a look at the revisions related to the story?01:49\nprologicSee where I went wrong01:49\nprologicI had to backout the changes (for now)01:49\nprologicEverytime I thought I fixed it, I broke Python 3 compat01:50\nOssook checking01:51\nOssoI should use your patch as base right ?01:56\nOssoprologic: since you have been working on py3 recently02:00\nOssoI can rely on py3 on jenkins having the u\"\" literal ?02:00\nprologicwell you can try02:06\nprologichere's the thing02:07\nprologicpy33 has the u\"\" literal02:07\nprologicbut py32 doesn't02:07\nprologic(nor py31 or py30) - but we're not supporting those versions02:07\nOssois py32 tested ?02:08\nprologicunless you think we should drop it?02:09\nprologicright now we run our tests on and support02:09\nprologicpy26, py27, py32, py33 and pypy02:09\nprologicon Windows, Linux, OS X and BSD02:09\nprologicBut we only have build environments for Linux and Windows (the other two are more manual through tox)02:10\nprologickoobs has promised us a BSD jail to do testing on (still waiting on that) :)02:10\nkoobsill update my port tonight and run the unit tests for you02:10\nkoobsjail will be a little longer02:10\nkoobssmashed at the moment (as ive mentioned) :]02:11\nkoobs*updates port*02:11\nOssoI wondering about the the u() thingy02:11\nOssoit's using unicode_escape02:11\nkoobsoooo, new files02:11\nkoobsfutures and pools moved, ooo02:12\nOssoit'll be a different result from utf-802:13\nOssoso I am missing how it works02:14\nkoobsrunning build_ext02:17\nkoobserror: No such file or directory02:17\nkoobshmm02:17 test is borked02:17 test needs a test02:17\nkoobsmain.runtests isnt there02:18\nprologickoobs: what version was your port for?02:18\nkoobsim almost done on 2.1.002:18\nprologicOsso: yeah I think we should continue the style of using circuits.six02:18\nprologicbut with modifications02:18 refers to tests.main.runtests for test_suite02:18\nprologicI believe IHMO that the u() implementation is circuits.six is wrong02:18\nkoobsit doesnt exist anymore02:18\nOssoI think we should six but02:19\nprologickoobs: eh?02:19\nkoobsprologic; need to know hwo to run tests :)02:19\nOssothis 2 results are different and I clearly want the second option02:19\nprologickoobs: make clean tests02:19\nprologickoobs: py.test tests02:19\nprologickoobs: tox02:19\nkoobsprologic; contains test_suite=\"tests.main.runtests\", that module doesnt exist anymore (and i could run tests via python test in 2.0.1)02:20\nkoobstox is new since 2.0.1 ?02:20\nprologickoobs: hmmm02:20\nprologicdid we break the\nkoobsim installing tox now, lucky im the maintainer02:20\nprologickoobs: we use fox now to run tests across different envs02:20\nkoobsyah, i get it :)02:21\nprologickoobs: ok I'll fix setup.py02:21\nprologicthanks for the heads up :)02:21\nkoobsjust needs hooking up to the test cmd02:21\nprologicobviously we don't personally use it :)02:21\nprologicbut it is nice to python test02:21\nkoobswrite a test for it ;)02:21\nkoobsits very nice02:21\nprologiccould do02:21\nprologicbe a bit tricky though02:21\nkoobsand standard hook for system packages to use02:21\nkoobsgiven the variation in testing methodologies\/tools02:21\nprologicOsso: I found a better implementation (perhaps a correct one) of the u() function on stack overflow today at work02:23\nprologiclet me try to find it again02:23\nkoobsrunning tests now02:24\nprologicWhat about this?02:26\nOssofrom __future__ import unicode_literals02:27\nprologickoobs: fixed test_suite in dev02:27\nOssowe can always u\"\" then02:27\nprologicOsso: does that work in py32?02:27\nkoobsprologic; oh that was quick :)02:28\nprologicOsso: how? I just tried it on py3202:29\nprologicstill get a SyntaxError for02:29\nprologic>>> s = u\"foo\"02:29\nkoobsgood news everyone!02:29\nkoobs========================================================== 266 passed, 1 skipped in 56.61 seconds ===========================================================02:29\nkoobsbut only 266 tests *tear*02:29\nprologickoobs: good stuff :)02:29\nprologickoobs: what got skipped?02:30\nkoobsthats only on 2.702:30\nkoobsi recall we skipped one in 2.0.102:30\nprologicyeah use fox to run across all supported pythons02:30\nkoobsooo interesting02:30\nkoobsit says it passed, but.02:30\nprologicmake clean tests (is more verbose)02:30\nprologicshould give a report at the bottom of failed, skipped, etc02:30\nOssono never mind it does not work02:30\nkoobswhats make? :)02:30\nkoobsprologic; the port build_Ext's then runs tox directly from tarball extract dir02:31\nprologicyou know GNU make :)02:31\nkoobsrunning again02:31\nkoobssif use Makefiles for python modules02:31\nkoobswhat is this :]02:31\nprologicconvenience for a bund of things02:31\nprologicmake docs02:32\nprologicmake clean02:32\nprologicmake release02:32\nkoobsi thought python had cmd's for that :]02:32\nkoobsbut understood nonetheless02:32\nprologicit does02:32\nprologicbut it doesn't cover everything02:32\nkoobsi know what youre getting at :)02:32\nkoobshmm, bad file descriptors02:32\nkoobswill paste02:32\nprologicno don't02:33\nprologicif it didn't fail02:33\nprologicdon't :)02:33\nprologicwe know about them :)02:33\nkoobsmy issue is02:33\nkoobshow can there be errors02:33\nprologicwe have a story to fix those I think02:33\nkoobsand still be passing02:33\nprologicbecause the behaviour is still correct02:33\nprologicbut the cleanup is bad02:33\nprologicor something to that effect02:33\nkoobs\u00a0\u00a0File \"\/usr\/local\/lib\/python2.7\/\", line 170, in _dummy02:33\nkoobs\u00a0\u00a0\u00a0\u00a0raise error(EBADF, 'Bad file descriptor')02:33\nkoobsthat one?02:33\nprologicthe reason you're seeing the errors on stderr though02:33\nprologicis because of a new feature in 2.1.002:33\nprologicnamely default error handler02:34\nkoobsand i cant see the skipped one hm,m02:34\nprologicthat by default prints to stderr02:34\nprologicie: don't have to forget to add Debugger() to your app02:34\nprologicbut adding Debugger() or a custom error logger will override the default02:34\nprologicme here's idea :)02:34\nkoobsbe nice to know which was skipped :]02:34\nprologicpy.test -r fsxX tests02:35\nprologicpython -m tests.main02:35\nprologicthose two test runners have the reporting for skipped, failed, etc02:35\nkoobsSKIP [1] \/usr\/home\/koobs\/repos\/freebsd\/ports\/devel\/py-circuits\/work\/circuits-2.1.0\/tests\/io\/ could not import 'pyinotify'02:36\nkoobsyou shoudl write a kqueue bit for fd notifications on freebsd02:36\nkoobs supports kqueue02:37\nkoobsand windows02:37\nkoobsand FSEvents on OSX :]02:37\nprologicthere's a way to implement real-time file notifications on BSD using kqueue?02:37\nkoobshave a squiz at that pypi link02:38\nprologicwe just have to wrap watchdog?02:38\nkoobsif its good enough02:38\nkoobsif you like it02:38\nprologiccan you write it for us? :)02:38\nprologicshould be trivial02:38\nprologicjust like we wrapped pyinotify02:38\nkoobsi have never written a line of python :)02:38\nprologicdoes watchdog also support notify for linux?02:38\nkoobsi lie.02:38\nkoobsinotify on linux02:39\nprologicok I'll write it :)02:39\nkoobsfsevents\/kqueue on OSX02:39\nprologicoh sweet02:39\nkoobskqueue on *BSD02:39\nkoobsand windows02:39\nprologicok I'll investigate it02:39\nprologicand write up a new story02:39\nprologicit's better to wrap something that supports more platforms ihmo02:39\nkoobsyou may not get identical feature support, i cant tell you exactly02:39\n*** ronny has joined #circuits02:39\nkoobsbut theres plenty of tools that subscribe to file\/dir etc notifications (among other things) using kqueue02:39\nprologicyeah no that's cool02:40 just fires file system events for what it's watching02:40\nprologicif watchdog can provide the same functionality and support more platforms02:40\nprologicthat's all we need it for02:40\nkoobstheres also\nkoobswhich i think is py-kqueue on pypi02:41\nprologicahh yeah nah02:43\nprologicwe're covered with queue inc ircuits02:43\nprologicwe fully support it on OS X \/ BSD as you know02:43\nkoobskqueue for python02:43\nkoobsnot queue02:43\nprologicyeah kqueue02:43\nprologicit's built into the python std lib02:43\nkoobsif watchdog turns out to be crap, or too wide in scope02:43\nprologicpydoc select02:43\nprologicwe support Select, Poll, EPoll, KQueue pollers02:44\nkoobsnot really polling with epoll and kqueue :)02:44\nkoobstwo hard problems in computer science02:44\n*** ronny has quit IRC02:45\nprologicwell the point is we support all pollers we know about provided by select module02:46\nprologicas long as they're available on your system :)02:46\nkoobswhich is great02:46\n*** ronny has joined #circuits04:11\n*** ronny has quit IRC04:11\n*** ronny has joined #circuits04:11\n*** christopher_ has quit IRC07:35\n*** christopher_ has joined #circuits07:52\n*** christopher_ has quit IRC08:02\n*** christopher has joined #circuits08:05\n*** Osso has quit IRC09:17\n*** christopher has joined #circuits10:25\n*** ronny_ has joined #circuits11:14\n*** ronny has quit IRC11:17\nspaceoneprologic: what do you think about parsing the first http request line with regex?11:29\nspaceoneRFC2616 Section 19.3 says that a server SHOULD be tolerant when parsing it11:29\nspaceoneanother section says that the server MUST send an 505 if the request HTTP protocol version is wrong (it must be case sensitive): \"HTTP\" \"\/\" NUM \".\" NUM11:32\nronny_how about using http_parser ?11:34\nspaceonere.compile('^(?P<method>[^ \\t]+)[ \\t](?P<uri>[^ \\t]+)[ \\t]+HTTP\/[0-9]\\.[0-9]$')11:35\nspaceonere.compile('^(?P<method>[^ \\t]+)[ \\t](?P<uri>[^ \\t]+)[ \\t]+HTTP\/(?P<protocol>[0-9]\\.[0-9])$')11:36\nspaceoneronny_: hmm, only if i can see the 3p lib code11:36\nspaceonealso it is one simple regex11:36\nronny_spaceone, http-parser is open-source :P11:47\nspaceoneronny_: is it a pyhton lib ?11:47\nronny_spaceone, yes11:48\nspaceonehmm, no.. it is on pypi11:48\nspaceonehmm, it contains more than HTTP11:52\nspaceonethere are status defin... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9786269069,
        "format_confidence":0.678951323
    },
    {
        "url":"https:\/\/www.digitalrage.fm\/podcast\/36-jeff-byer-chrome-dev-tools-shopify-ccpa-and-tech-talk\/",
        "text":"36 | Jeff Byer \u2013 Chrome Dev Tools, Shopify, CCPA and Tech Talk\n\nToday Jeff Byer (@globaljeff) talks about his experiment with workspaces in Chrome Dev Tools, the Shopify universe, upcoming project launches, real-world project issues, and tech talk.\n\nShow Links\n\nChrome Dev Tools \u2013 Workspaces\n\nCCPA \u2013 https:\/\/oag.ca.gov\/privacy\/ccpa\nFact Sheet, pdf\n\n\nJeff Byer\u00a0 \u00a0 00:06\u00a0 \u00a0 Welcome to digital rage, the podcast about all things internet and the people that make it great. My name is Jeff Byer and I know last episode I have promised you guests but uh, that didn\u2019t happen. You know, people are busy, things are happening and uh, you know, just didn\u2019t get around to it. I did have an interview scheduled, we had our <inaudible> in our call \u00a0\n\nJeff Byer\u00a0 \u00a0 00:30\u00a0 \u00a0 and there was a misunderstanding. This a, I believe a the person involved was expecting some sort of a uh, you know, get to know ya. Uh, cause I sales call and I was expecting it to be an interview. So a little bit of a miscommunication. But um, anyway, I do have a ton of things to talk about. So this will be an episode full of stuff. I like to say stuff because some of it is crap set of, some of it\u2019s not. So let\u2019s get right into it. First thing is I was playing around and in Chrome and, and the dev tools and just futzing around and came across the workspaces in dev tools and thought, okay, this is interesting. Why would, why would they have a, you know, invite somebody to put their file structure inside of Chrome dev tools if it doesn\u2019t actually make any live, live edits as you go. \u00a0\n\nJeff Byer\u00a0 \u00a0 01:41\u00a0 \u00a0 Because as most developers know, whenever you get to a live website and you go into dev tools, you can modify all of the code you want and change the display of, of that website. You know, change the content, anything you want, run experiments, even if it\u2019s not your site but only you can see it. And as soon as you close dev tools or refresh the page, all those edits are gone. And it goes back to the way it is on the live server. And I do use that sometimes to one. If I\u2019m trying to get a screenshot and I want to modify things for the actual screenshot, I\u2019ll go in, remove modules, remove ads and stuff like that. But on my own sites, I use it just to play around and see, you know, if there\u2019s something visually or content wise that I want to accomplish, how fast it would be to make that adjustment and just kind of play around in dev tools before I open up, uh, you know, my editor editing environment and go ahead and go through with the change. \u00a0\n\nJeff Byer\u00a0 \u00a0 02:45\u00a0 \u00a0 It also helps me to, to create estimates on what it\u2019s going to take to accomplish certain things requested by my client. So I get into this and I\u2019ve read a couple of articles and I go through the Google tutorial on how to use Chrome tools and the whole time, uh, I\u2019m going through these tutorials and obviously it says, okay, you know, it\u2019s, you\u2019ll be able to save the changes if you use it in, in your local environment and you add your local pho folder for that environment. So I was like, okay, that\u2019s fine, but it, uh, it\u2019s very limited and uh, I didn\u2019t see any way if you are going to use a SAS or any type of, of pre compiler that it just didn\u2019t have a way to really integrate those unless you had another third party script going. So the whole time I\u2019m going through this tutorials, like who the hell would use this? \u00a0\n\nJeff Byer\u00a0 \u00a0 03:47\u00a0 \u00a0 It\u2019s just basic. If you had a basic site with no preprocessing prop, pre-processing, no minification and uh, just static something, I don\u2019t even know what, um, that\u2019s what you\u2019d use. So I went down a rabbit hole on that and found myself completely disappointed. So moving on. Um, so today I added another team member to my Shopify partner program. Uh, another developer that\u2019s going to help me launch these two big projects that we\u2019re launching this month. And so the month of November is going to be a huge month here for us. And so we needed to bring on extra help. And the way Shopify partners, the partner system on Shopify works is that if, uh, I\u2019m going to take on a new client that has an existing store, I go find the existing stores, my Shopify address, then I request access to that through my partner portal. And then the owner of that store is able to verify and then, uh, I get, I gain access through the partner portal. \u00a0\n\nJeff Byer\u00a0 \u00a0 05:05\u00a0 \u00a0 If the customer just puts in my email address and sends me access, even if it\u2019s the exact same address, it does not, uh, it\u2019s not available in my portal. So it\u2019s, uh, it\u2019s just a little quirky thing. I\u2019m sure there\u2019s plenty of reasons on the back end or privacy issues or security issues or anything like that that, that takes that into account I guess. Because you know, anybody can put in anybody\u2019s email address and then you can accept and, and, but not really have access. I don\u2019t really understand the details of why it\u2019s separate, but it\u2019s separate. So, um, with adding a new team member, um, you know, I take privacy and my accounts very, very seriously, especially when hiring outsourced developers. So I create account specific for them, you know, server accounts, user accounts, things like that. And for this one, um, the developers first request was just to send me the access and what that assumes is that he wanted to log in as me through my account and take a look at everything. \u00a0\n\nJeff Byer\u00a0 \u00a0 06:20\u00a0 \u00a0 And that doesn\u2019t really sit with me very well, uh, for plenty of different reasons. Obviously security and, and you know, my, my customers who are, the admins can see my logins. And so if something happens while I\u2019m logged in or near anything, it\u2019s just, I don\u2019t want it on me personally as, as a, you know, possible vulnerability. But the great thing is through the partner program, all of my, all of my accounts are in my partner program and when I log in, it shows that it\u2019s my company that\u2019s locked in, a representative of my company that\u2019s in there doing changes. And that\u2019s much better as far as, uh, my clients are concerned and I\u2019m concerned. So the, the great thing was it the easy way to, there\u2019s an easy way to add people. You just add staff members to your partner portal, then each of those staff members, you can define their access but then they have access to all of your partners sites. \u00a0\n\nJeff Byer\u00a0 \u00a0 07:24\u00a0 \u00a0 And this hire today was not for for one specific type but for three of them that are in my partner portal. And so it was very convenient that I just had to get his Shopify email address, the email address that he uses for Shopify and he now has access to uh, to make changes to all of my customer accounts, which is, which is very convenient for me because it\u2019s one single point of access. He can access all of them. Now if something happens or if I don\u2019t like the worker or if I\u2019m just moving on to a different developer, remove him and he wrote it, he doesn\u2019t have any access to them anymore and if he\u2019s going to ask them, access them currently he has to go through my partner portal. So a very cool thing. Uh, just thought that was really interesting cause it\u2019s, it\u2019s kind of backwards from how you access how you grant access to a customer\u2019s store rather than through a partner in, through my company. \u00a0\n\nJeff Byer\u00a0 \u00a0 08:27\u00a0 \u00a0 So that was fun. Um, another piece of information, uh, was regarding the, the California consumer consumer privacy act CC P a. So this is coming on January 1st and uh, I\u2019m making sure that all of my, all of my accounts are aware of this and so that they\u2019re prepared and know that it\u2019s coming. And so digging into the fact sheet of the CCPA, the here\u2019s the requirements. So, uh, E not a lot of my customers actually require this, but for the ones that do, I\u2019m just confirming cause I don\u2019t know the ins and outs of everything that they do. Uh, I have web stats and that\u2019s it. I don\u2019t know how the business is run specifically. I don\u2019t know revenue numbers cause they\u2019re mostly private businesses. But here are the, here\u2019s the, the requirements. So businesses are subject to the CCP. A, if one or more of the following, our true has gross Avenue S sorry, I\u2019m having trouble speaking today. \u00a0\n\nJeff Byer\u00a0 \u00a0 09:47\u00a0 \u00a0 Let\u2019s try this again. If the following are true, has gross annual revenues in excess of $25 million, um, buys, receives or sells the personal information of 50,000 or more customers, households or devices. So that one\u2019s interesting because when I first skimmed through it and I said, Oh, buy and sells information, that\u2019s, you know, none of my customers do that. But then receives and absolutely all of my customers receive information and checking databases on that information. Is, is only part of what the law entails. This is pretty much everything. So I need to verify that with all my customers because 50,000 consumers on, you know, a site that\u2019s been 10 years old, that\u2019s a pretty easy number to hit. So, uh, even for a small business, so I\u2019m confirming that, uh, all the information that they have is, is w, you know, if it\u2019s over that number or under that number, if it\u2019s over that number, then we have to start looking into being compliant. \u00a0\n\nJeff Byer\u00a0 \u00a0 11:05\u00a0 \u00a0 And the last, the last one, which I already kind of hinted to but derives 50% or more of annual revenues from selling consumers\u2019 personal information. So since I don\u2019t deal with companies that sell personal information, uh, not really concerned about that one, but receiving, so the 25 million annual revenue and receiving information of 50,000 or more consumers, those are the two key metrics that I\u2019m interested in, uh, as far as this lobbying applied. So that warning is going out to all of my customers and that, that, you know, I\u2019m pretty sure we\u2019re are either close or will qualify and we\u2019re to, uh, start planning on, uh, being compliant with that as of January one. And the one customer that I know is going to be effected by this law were already in the process of a full redesign and we started this process with them back in March. \u00a0\n\nJeff Byer\u00a0 \u00a0 12:1... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.5481590033,
        "format_confidence":0.97690624
    },
    {
        "url":"https:\/\/dev.to\/corecursive\/beautiful-and-useless-coding-with-allison-parrish",
        "text":"DEV Community\n\n\n\nBeautiful and Useless Coding with Allison Parrish\n\nCorecursive Play Button Pause Button\n\nGenerative Art involves using the tools of computation to creative ends. Adam talks to Allison Parrish about how she uses word vectors to create unique poetry. Word vectors represent a fundamentally new tool for working with text.\n\nAdam and Allison also talk about creative computer programming and building twitter bots and what makes something art.\n\n\"Computer programming is beautiful and useless. That's the reason that you should want to do it is not because it's going to get you a job, because it has a particular utility, but simply for the same reasons that you would pick up oil paints or do origami or something. It's something that has like an inherent beauty to it that is worthy of studying.\"\n\n\"For my purpose as an artist and as like someone who teaches programming to artists and designers, I want to emphasize that it's not only a vocational thing, it's not only a way for building things like to do apps for that matter. It's not only a way to, you know, write useful applications that help to organize communities or help to do scientific work and other like good applications of programming and software engineering. But there is this like very essential, very core part of computer programming that is just joyful. Um, that's about understanding your own mind in different ways and understanding the world in different lands.\"\n\nExperimental Creative Writing with the Vectorized Word\n\nEvery Icon\n\n\nAllison Parrish's Website\n\n\n\nEvery Word\n\nEpisode source\n\nForem Open with the Forem app",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.8132406473,
        "format_confidence":0.8848764896
    },
    {
        "url":"http:\/\/apmdigest.com\/hp-application-development-devops-2",
        "text":"Q&A: HP Talks About App Development and DevOps - Part 2\nJuly 30, 2015\nShare this\n\nIn Part 2 of APMdigest's exclusive interview, John Jeremiah, Technology Evangelist for HP's Software Research Group, talks about DevOps.\n\nStart with Part 1 of the interview\n\nAPM: Do you feel that developers and testers are being held more accountable for application quality today? How is their role changing?\n\nJJ: Developers and testers are taking greater and greater accountability for both speed and quality. As we've discussed, if defective code gets into a new software product or update, it becomes much more costly and time-consuming to rectify down the line. It's like one bad ingredient in a sandwich. The more ingredients are added to the sandwich, the more laborious and painful it becomes to take it apart.\n\nThe goal of having smaller, more focused releases is to improve both speed and quality. Because development is faster and releases are smaller, then it becomes easier to test and fix bugs.\n\nFaster feedback is a key to both speed and quality \u2013 if a bug is quickly found, the developer knows what to fix, as opposed to finding a bug that was created six months ago. Not only is it easier to fix, it's also possible to prevent a spiral of issues based on that one bad line of code.\n\nAPM: What about the \"Ops\" side of DevOps \u2013 how is the Ops role changing? What new demands do they face?\n\nJJ: It's not all about Dev and Test. In fact, automating the delivery \u2014 code and infrastructure \u2014 of an app change is a critical part of DevOps. The explosion of containerization and infrastructure as code is having a real impact on the definition of \"Ops\". I see their role evolving to where they provide consistent frameworks or patterns of infrastructure for DevOps teams to utilize \u2014 shifting from actually doing the provisioning, to providing \"standard\" and \"supported\" packages for Dev teams.\n\nIT Ops teams also contribute to application quality in a \"shift right\" way so to speak. There is a wealth of information in production data that can be fed back to developers, in order to help developers prioritize areas for improvement \u2013 for example, what are the most common click-through paths on a website, or where exactly in a site are end users abandoning shopping carts or transactions?\n\nAPM: How do you predict that DevOps will evolve?\n\nJJ: DevOps will become more common in many enterprises, evolving from an emerging movement. The collaborative nature and shared responsibilities of DevOps will continue to blur rigid role definitions and we will see traditional silo mentalities increasingly fade away.\n\nDevelopers are acting more like testers; IT Ops teams are feeding crucial information back to developers to assist in the development process; and developers are architecting applications \u2014 based on this feedback \u2014 to be more resource-efficient, essentially thinking and behaving like IT ops teams. Everyone is united and focused on application roll-out speed and quality, which includes functional and end-user performance quality, as well as resource-efficiency \u2014 yet another ingredient of a quality app.\n\nVisionary business leaders will take advantage of DevOps speed, and will create disruptive offerings in many industries, further accelerating the adoption of DevOps.\n\nAPM: What tools are essential to enable DevOps?\n\nJJ: To achieve velocity combined with quality, DevOps teams need automation tools that enable them to eliminate manual, error-prone tasks; and radically increase testing coverage, both earlier in the lifecycle, as well as more realistically and comprehensively, in terms of network environments and end-user devices and geographies.\n\nDevOps teams need visibility and insight into how their application is delivering value, so we're also seeing an increased need for advanced data analytics capabilities, which can identify trends within the wealth of production data being generated.\n\nAPM: Where does APM fit into DevOps?\n\nJJ: Application Performance Management (APM) is a critical success factor in DevOps. There is no point rolling out the most feature-rich application \u2014 if it performs poorly for end users, they'll just abandon it, or it's a major IT resource drain, for example.\n\nBefore DevOps, you often had situations where a poorly performing app was discovered, and developers would then promptly point their finger at IT, and vice versa. In a DevOps team, everyone owns application performance and is responsible for success. Hence, APM systems give DevOps teams a true, unbiased view of how an application is performing.\n\nToday, these systems are often combined with analytics that let DevOps teams identify the root cause of performance issues \u2014 whether code- or IT-related \u2014 in minutes, rather than days. In this way, APM helps to eliminate finger-pointing and guessing.\n\nAPM also can be used to proactively anticipate the end-user performance impact of new features and functionalities, which can help DevOps teams determine if these possible additions are worth it.\n\nAPM: With all the emphasis on testing automation lately, there is a theory that testing will go away as a discipline \u2014 \"DevOps\" will become \"NoOps.\" Do you envision this ever happening?\n\nJJ: In a word \u2013 No. It's a myth, misunderstanding and misconception that DevOps leads to reduced testing. In fact the opposite is true. A DevOps team is committed to keeping their code base ALWAYS ready for production. That means every change is tested, and defects are not tracked to be fixed later, but are fixed immediately. The team commits to keeping the build green and ready to go, ready to pass acceptance tests. The key to achieving this is the use of automation tools enabling them to provision, tweak, and de-provision testing resources, quickly and easily, so they can focus more of their time on actual testing.\n\nRead Part 3, the final installment of the interview, where John Jeremiah, Technology Evangelist for HP's Software Research Group, outlines the future of application development.\n\nShare this\n\nThe Latest\n\nMay 22, 2017\n\nAPMdigest asked experts across the industry for their opinions on the next steps for ITOA. Part 5 offers some interesting final thoughts ...\n\nMay 19, 2017\n\nAPMdigest asked experts across the industry for their opinions on the next steps for ITOA. Part 4 covers automation and the dynamic IT environment ...\n\nMay 18, 2017\n\nAPMdigest asked experts across the industry for their opinions on the next steps for ITOA. Part 3 covers monitoring and user experience ...\n\nMay 17, 2017\n\nAPMdigest asked experts across the industry for their opinions on the next steps for ITOA. Part 2 covers visibility and data ...\n\nMay 16, 2017\n\nManaging application performance today requires analytics. IT Operations Analytics (ITOA) is often used to augment or built into Application Performance Management solutions to process the massive amounts of metrics coming out of today's IT environment. But today ITOA stands at a crossroads as revolutionary technologies and capabilities are emerging to push it into new realms. So where is ITOA going next? With this question in mind, APMdigest asked experts across the industry \u2014 including analysts, consultants and vendors \u2014 for their opinions on the next steps for ITOA ...\n\nMay 15, 2017\n\nDigital transformation initiatives are more successful when they have buy-in from across the business, according to a new report titled Digital Transformation Trailblazing: A Data-Driven Approach ...\n\nMay 11, 2017\n\nThe growing market for analytics in IT is one of the more exciting areas to watch in the technology industry. Exciting because of the variety and types of vendor innovation in this area. And exciting as well because our research indicates the adoption of advanced IT analytics supports data sharing and joint decision making in a way that's catalytic for both IT and digital transformation ...\n\nMay 10, 2017\n\nColin Fletcher, Research Director at Gartner, talks about Algorithmic IT Operations (AIOps) and the challenges and recommendations for AIOps adoption ...\n\nMay 09, 2017\n\nIn APMdigest's exclusive interview, Colin Fletcher, Research Director at Gartner, talks about Algorithmic IT Operations (AIOps) and how it will impact ITOA and APM ...\n\nMay 05, 2017\n\nMicrosoft is expected to essentially wind down Windows 7 support by 2020 so inevitably Windows 10 will be on the IT task list. It would be beneficial, now, to examine some of the issues relating to migrating to Windows 10 OS and how these pain points can be alleviated and addressed. Here are 7 practices that are key to facilitating migration ...",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9758494496,
        "format_confidence":0.9762678742
    },
    {
        "url":"https:\/\/www.softwaretestpro.com\/ask-the-tester-james-bach\/",
        "text":"He\u2019s worked at some of the most innovative companies in Silicon Valley, co-authored a best-selling test textbook, keynoted at every international test conference, and, yes, has primary credit for popularizing the term \u201cexploratory testing.\u201d Did I mention he\u2019s a high school dropout who unschools his son? (For more on unschooling, see the article \u201cHow Children Learn (to Test) on page 26.) Of course, I\u2019m talking about James Bach, our invited interviewee for this, the exploratory test issue of STQA Magazine.\n\nJames Bach will now answer your questions.\n\nQuestion: I\u2019ve had a few people suggest to me that exploratory testing is just a \u2018fancy name\u2019 for ad-hoc testing. How would you suggest I respond? Peter Walen, Grand Rapids, Michigan\n\nJames: Yes, and \u201cdance\u201d is another way of saying \u201cboogie down.\u201d So what? They both refer to rhythmic movement. Yet there is a difference in connotation.\n\nMany years ago I promoted the idea of sophisticated, professional ad hoc testing. But I could not get any traction with that. Most people don\u2019t read dictionaries, I guess, and they kept thinking that \u201cad hoc\u201d meant sloppy. It doesn\u2019t mean sloppy. It means something prepared for a particular situation. I soon grew tired of fighting that battle and decided to embrace Cem Kaner\u2019s term for the same kind of testing. ET had no baggage, so it was easier to get past the silly stuff and communicate the essence of the ideas behind it.\n\nExploratory and ad hoc are two different concepts, but they both are descriptive of non-scripted testing.\n\nQuestion: In the environments I\u2019ve used it, ET has proven very useful. However, it has been suggested that large testing organizations, both large in number of testers and large in distribution of locations, are not suitable for Exploratory Testing because of the extreme difficulty in the ability of management to effectively control test efforts. Again, I am curious \u2013 how would you respond? Peter Walen, Grand Rapids, Michigan\n\nJames: I hear that a lot from people who mistake a particular form of exploratory testing for the whole enchilada. Look, exploratory testing means you think while you test, and that your thinking affects the design of your test as you go. Is there anyone who truly says that a large organization wants me to stop thinking? And if so, is it possible that whoever is saying that has already, um, stopped thinking before they started speaking? No organization that seeks to organize intellectual work can do that by telling its people to stop thinking. That\u2019s just a silly idea on its face.\n\nImagine if someone claimed that allowing people to drive their own cars was unrealistic in a large city because traffic would get all snarled up. Well, obviously we do drive our own cars, and yes, traffic CAN get all snarled up. But the alternative requires a great deal of investment to obtain a result that is a great deal less flexible.\n\nAll testing is exploratory to some degree, or else it isn\u2019t testing. The reason people get confused is that they think ET is a rejection of organization and structure, which of course it isn\u2019t. We apply whatever structures we think will help us.\n\nQuestion: Can you explain the focus and de-focus technique used in Exploratory Testing? I have read about it, but have not attended a session where it was demonstrated. Sherry Chupka, Pittsburgh, Pennsylvania\n\nJames: Focusing and defocusing is not really a technique, in itself. Focus is an attribute of techniques. Any given technique of testing focuses you in some ways and may also defocus you. To focus means to work within the constraints of a specific pattern of testing (hence, a test script is a focusing method); to defocus is to change your pattern. Focusing also means to restrict your attention to a smaller area more meticulously, while defocusing means covering something larger but with less fidelity. Generally speaking, focusing is for studying something specific (such as investigating a bug), and defocusing is for searching for something new to study (like finding a bug).\n\nAll of testing can be seen as a process of strategic and tactical focusing and defocusing.\n\nA great example of the dynamic is what we do when tuning an old fashioned analog radio. We spin the dial quickly, hearing static. Suddenly, as we speed by a radio station, we hear a moment of music. Then we slow down and go back, thus acquiring the station. Going fast was defocusing; going slow was focusing. If you tried focusing all the time as you tested, you would find few bugs. But if you never focused, you would not be able to claim that you had done a thorough job of testing, since you would not be able to relate testing to any specific model of coverage or risk.\n\nQuestion: If I may, I have a question for \u201cThe Buccaneer Scholar\u201d: What are the subjects\/thinkers outside of testing and computer science or engineering you think are crucial for developing as a software tester? Curtis Stuehrenberg, Seattle, Washington\n\nJames: I suggest that you study the social sciences. Here are some of specific social science fields that I\u2019ve been inspired and helped by. Google these:\n\n  \u2022 Grounded Theory\n  \u2022 Naturalistic Inquiry\n  \u2022 Situated Cognition\n  \u2022 Philosophy of Science\n  \u2022 General Systems Thinking\n\nAs for people, check out the\nwork of Richard Feynman, the physicist, or Julian Baggini, who has written the most readable books on general philosophy I have seen. I think the scientific and philosophical illiteracy of our field explains the strange longevity of stupid ideas that don\u2019t work, and have never worked. Our industry seems to be run by fairy tale logic.\n\nQuestion: I do a fair amount of testing in an exploratory style and I am familiar with some of the basics \u2013 but what can I do to elevate myself from getting the job done to excellence? That is to say, I am interested in ways to increase the value I provide my team and customers. David Hoppe, Dorr, Michigan\n\nJames: Here are some ideas for you:\n\n  \u2022 Can you do a stand-up test report on zero notice? What if I gave you five minutes to prepare one? Will your test report include all three levels (i.e. product status, testing activities, justification of testing activities)? Practice that.\n  \u2022 If I were to ask you what your test methodology is, could you give me a five-minute chalk talk that looked good and also was true? You should have a mental picture of testing in your mind, and some way, under pressure, to access that model. I use guideword heuristics, for instance.\n  \u2022 Learn session-based and thread-based test management.\n  \u2022 Have you studied the ET skills and tactics list that my brother and I published? Ask yourself how you stand on each item.\n\nYou should also develop a colleague network and make use of it. For that matter, I do free coaching over Skype, as does Michael Bolton and Anne-Marie Charrett, also. Come see me for a session.\n\nQuestion: How little can a user know about the application\u2019s goal, purpose, and assumed workflow in order to do effective exploratory testing? Brian J. Noggle, Springfield, Missouri\n\nJames: Effective testing requires knowledge of the product. Exploratory testing is a way of testing whereby you learn as you test. Therefore, you can do effective ET with no knowledge (since it is effective to be learning) but the quality of your testing will not be at its best until after you\u2019ve learned enough about the product to know how to observe it, control it, and recognize important problems in it.\n\nScripted testing is the same way. You can\u2019t write great test scripts unless and until you know the product. That\u2019s why most people create scripted tests by doing exploratory testing first.\n\nQuestion: What are similarities and differences between exploratory testing and something like hacking, white hat penetration testing or security testing? Would it benefit these groups to interact more and learn from each other? Daw Cannan, Raleigh, North Carolina\n\nJames: Penetration testing is inherently exploratory. Hackers are inherently exploratory. I\u2019ve dabbled in this quite a bit, myself. I would recommend that testers read 2600 magazine and books about hacking.\n\nQuestion: Can you provide one or more specific examples of what it looks like to successfully integrate exploratory testing with other test approaches on a medium to large size test project? Sean Stolberg, Seattle, Washington\n\nJames: First, it\u2019s already happening on every project that has ever been done. Exploratory testing is not some exotic weird thing. You do it whenever you investigate a bug, for instance. Geez. This is going on naturally. Have you ever been exploring a product and then decided to make an outline or test matrix to describe specific test conditions? If so, then you already know the answer to this question.\n\nMostly, when I\u2019m asked this, the questioner turns out really to have been asking a different question, so I\u2019ll answer that different question:\n\nQ: Can you provide one or more specific examples of what it looks like to do great testing in a way that doesn\u2019t worry and confuse busy-body managers who don\u2019t understand testing or trust testers and therefore wish to apply silly management theories to testing such as \u201cyou should write down each test in a procedural scripted form?\u201d\n\nA: Yes, that\u2019s why we created session-based test management. In this form of test management, unscripted testing is done in sessions. Sessions are time-boxes within which the testing occurs. Each session has a charter (a little mission) and results in a session report. We can create metrics from these which are reasonable and tend to give managers warm and fuzzy feelings inside. Managers get something to count, and testers remain relatively free to do their jobs. See more about it on my website.\n\nQuestion: In your Rapid Software Testing training class and in your book Buccaneer Scholar, you start off by saying, apparently proudly, that you\u2019re a high school drop-out. I think the readers would be interested to hear about how that has helped your professional suc... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9111586213,
        "format_confidence":0.8800519109
    },
    {
        "url":"http:\/\/www.newelectronics.co.uk\/electronics-interviews\/cyrille-comar-adacore-europe\/38796\/,",
        "text":"comment on this article\n\nCyrille Comar, AdaCore Europe\n\nCyrille Comar, AdaCore Europe\n\nCyrille Comar, co founder and managing director of AdaCore Europe, speaks to New Electronics.\n\nAdaCore evangelises an open source philosophy \u2013 how has the industry's perception of open source software changed in recent years and what benefits does it offer?\nAt AdaCore, FLOSS (Freely Licensed Open Source Software) has been our bread and butter for the last 20 years even though we don't consider ourselves as open source evangelists. Our core business is to invent, develop and maintain long-term, tools and technologies that help our customers build and validate high-assurance software and systems.\nIn doing this, we quickly realised our customers need a rich environment whenever they require a very high-level of confidence in the code they develop. We have found it more efficient business-wise, to participate to different open-source communities in order to build our commercial solution based on open-source building blocs. We also think that more industries could benefit from this type of sharing and co-operation. Many companies rely heavily on home-grown pieces of software (e.g. a real-time executive) that are very expensive to maintain and don't bring noticeable value to the final product. In order to decrease such internal costs, there are two avenues: use commercial-off-the-shelf software or share the development and maintenance of the less valuable, non-critical software in an open source community.\n\nWhat makes Adacore's software programming environments suitable for a wide range of markets?\nAda is a general programming language with capabilities similar to C++ or Java. Its main difference is that it has been designed to support long lived, very large pieces of potentially critical software. As such, it emphasises code readability, strong typing, ease of maintenance and thus is of general interest for all markets where long-term reliability is more important than time-to-market.\nAdaCore programming environments and tools are designed for the needs of such markets: Aeronautics, Space, Military, Railway, Automotive, Industrial automation, Medical devices, etc. It is also used in other areas when long-term maintenance or safety are of particular interest.\n\nYou've campaigned for better safety standards for embedded software systems across the aviation industry, where else is there a need for better safety legislation?\nWe have indeed been involved in the recent work leading to the new version of the civil avionics software standard: DO-178C. The main novelty of this standard is the capability to better take into account some of the major evolutions of Software Engineering that were not dealt with sufficiently in the previous version of the standard such as Object-Oriented Programming, Modelling or Formal Verification. Other industrial areas have different standards that are usually more recent than DO-178B and each standard has its own path to upgrade. Most probably, they will follow the DO-178C lead in due time.\nMore important than the standard itself, are the means of verifying its proper and complete usage. Some industries, such as the automotive one, still think they can enforce the use of their standard without independent assessment controlled by inter-government authorities. For sure such independent assessment has a cost that the automotive industry would like to avoid but is it a good thing for the safety of our principal mean of transportation? Imposing more stringent safety and security requirements will become a public safety issue sooner or later but isn't it also an opportunity for our own software industry to show its competitiveness in developing ultra-safe and secure software compared to emerging countries?\n\nYou've been involved with Ada for well over a decade \u2013 how has the language changed since then?\nThe Ada language is going through its 3rd major revision: Ada 95, Ada 2005 and now Ada 2012. Those evolutions have come from the feedback of the Ada community. What is particularly interesting in the evolution of the languageis that it has minimised upward compatibility issues as much as possible and thus it is relatively easy for Ada projects to adopt a newer version of the language and have access to newer paradigms without having to throw away or rewrite already written components.\n\nWhat's next? What will Ada 2012 look like and what enhancements will it bring to software developers?\nThe latest revision brings many new features that help with modern verification techniques such as programming by contract through pre and post conditions. It also generalises the notion of constrained subtypes since it is now possible for the user to specify themself the constraints in a general way.\nThe new language is now even more suitable for static analysis and formal proofs. Based on this new revision of the language, the AdaCore research lab is working on an open project whose goal is to make formal programme verification easy enough that it can help reduce the costs associated with testing (Hi-Lite project).\n\nChris Shaw\n\nComment on this article\n\nThis material is protected by MA Business copyright See Terms and Conditions. One-off usage is permitted but bulk copying is not. For multiple copies contact the sales team.\n\nWhat you think about this article:\n\nAdd your comments\n\n\nYour comments\/feedback may be edited prior to publishing. Not all entries will be published.\nPlease view our Terms and Conditions before leaving a comment.\n\nRelated Articles\n\nChange based testing\n\nA major cause of software bugs is inefficient and incomplete testing. This ...\n\nKeeping standards high\n\nEngineers \u2013 at least those who are worth their salt \u2013 ask questions. And those ...",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.913603723,
        "format_confidence":0.9801898599
    },
    {
        "url":"http:\/\/lists.w3.org\/Archives\/Public\/w3c-wai-ua\/2002AprJun\/0103.html",
        "text":"\n\nIssue 529\n\nFrom: Richard Schwerdtfeger <schwer@us.ibm.com>\nDate: Tue, 21 May 2002 15:08:37 -0500\nTo: w3c-wai-ua@w3.org\nMessage-ID: <OFD1E6182F.038C4249-ON86256BC0.00671643@raleigh.ibm.com>\nIssue 529 refers to a proposal for a revised checkpoint 6.1\/6.2 Checkoint.\n\nCheckpoint 6.2 proposes that any API can be used so long as it satisfies\nthe content required by checkpoint 6.1 and that it be documented.\n\nI would like to propose that checkpoint 6.2 be modified as follows:\n\n2.  Otherwise, where W3C normative bindings do not exist that the W3C DOM\nAPI must be used and that the User Agent's document binding for it must be\npublicly documented so that it may be used by an assistive technology.\n\nMy reasons for making this more stringent are as follows:\n\n- At IBM we consider the Web as a platform. It has its own accessibility\ninfrastructure and sets of guidelines and is a framework on which to\ndeliver applications\/content independent of the target device. In the\nprocess of making Web accessible, the WAI working groups, and in particular\nthe PF group, evaluate and produce accessibility guidelines to make the Web\nand all content delivered on it accessible. To ensure access to the Web we\ndefine changes to programmatic W3C API that must be platform neutral. The\nDOM is the W3C API that is being targeted to provide access to structured\ncontent. Without having control over its specification we could not have\naddressed the recent \"events\" API changes that we made to the DOM for the\npurposes of accessibility.\n\n- Recent accessiblity information targeted through XML, SVG, XFORMS, etc.\naccessibility done by the PF group can be addressed with the DOM WG to\nensure that it can be accessed by an AT through a designated W3C API. The\nWAI has the ability to control the specification of the DOM API because it\nis a W3C specification. The W3C does not have the ability to control the\ninformation provided by a proprietary API.\n\n- Implementing the DOM does not preclude additional Accessibility API (such\nas MSAA) from being used but it does guarantee the core API set needed by\nthe WAI working groups be provided for.\n\n- The W3C is one of the few places where all the platform vendors (Sun,\nIBM, Microsoft, etc.) get together and agree on standards. A standard API\nagreed on by the W3C then comes more easiliy adopted and supported in the\n\n- Having a standard W3C accessibility API for Web content makes it easier\nto develop cross-platform AT solutions even in the absence of normative\n\n- Having a standard W3C accessibility API for Web content makes it easier\nto develop multi-user agent, single-platform AT solutions (Adobe, IE,\nNetscape, Opera, Amaya on Windows) even in the absence of normative\n\n- We have plenty of implentation experience with using a platform-specific\nDOM binding in IE with IBM's Home Page Reader and Freedom Scientific's\n\nI understand fully that in the case of C++ we are at the mercy of the\noperating system implementation. By itself, C++ does not address\ncross-process marshalling. It does not address in-process multi-threaded\naccess. This is why platform bindings need different C++ bindings to\nsupport these features such as through the use of COM, XPCOM, or Corba.\nThis does not preclude a platform documented C++ binding for the DOM. With\na standard W3C API (that starts with the core DOM) we can build upon it to\nrequire DOM events notification, the CSS DOM API, a DOM Views API, etc. in\nthe future.\n\nMany of the people on the call only have had to deal with Windows. At IBM\nwe have to deal with Solaris, Linux, Windows, AIX, the Web, Java, etc.\nHaving a single W3C DOM API that, in the case of C++, may require different\nplatform documented bindings is essential.\n\n\n Rich Schwerdtfeger\n Senior Technical Staff Member\n IBM Accessibility Center\n Research Division\n EMail\/web: schwer@us.ibm.com\n\n \"Two roads diverged in a wood, and I -\n I took the one less traveled by, and that has made all the difference.\",\nReceived on Tuesday, 21 May 2002 16:08:42 UTC\n\nThis archive was generated by hypermail 2.3.1 : Wednesday, 7 January 2015 14:49:31 UTC",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9577701688,
        "format_confidence":0.6094864607
    },
    {
        "url":"http:\/\/bitmason.blogspot.com\/2016_09_01_archive.html",
        "text":"Tuesday, September 06, 2016\n\nRed Hat's Jen Krieger on DevOps feedback loops\n\nThe focus on metrics and feedback loops in software development tends to be around technical measurements like uptime. Or, if we're being really clever, the business outcomes associated with those numbers. In this episode, Red Hat Chief Agile Architect Jen Krieger argues that the human side of feedback loops may be the most important.\n\nShow notes:\n\nMP3 audio (14:55)\nOGG audio (14:55)\n\n\nGordon: \u00a0Hi, everyone. I'm here with Jen Krieger, the Chief Agile Architect with Red Hat again. In the last podcast, we were talking about distributed teams and in general how to make distributed teams work effectively and presumably leading to get better outcomes in terms of delivering software, because that's the name of the game after all.\nI'd like to follow up with Jen with something else that we were discussing came out of the Agile 2016 Conference. That was the idea of feedback loops. I've been talking alone with my colleague William Henry at some events about this idea of metrics. How do you know if your DevOps is working?\nThe focus tends to be measuring the throughput per second or the latency or the number of successful deploys or the down time. Those are important as are the business outcomes that stem from them.\nFor all the talk we have about the culture in DevOps though, we don't talk an awful lot about how you monitor how the team is doing, both from a day to day basis and in a longer term. What are some of your thoughts on that, Jen?\nJen: \u00a0Yes, it is absolutely true that when we think about the words DevOps, and we think about feedback loops, we immediately go to the space in which we're thinking about some sort of computer somewhere giving us some sort of data that will help us make a decision about some sort of thing that we're stumbling over, but we rarely, if ever, think about the fact that every single thing that we do with other people is a feedback loop, having visual confirmation of your idea.\nSay you're in a meeting and you're sitting with a bunch of people. You are saying, \"What do you think about my idea?\" Somebody sitting across the table leans in and maybe crosses their arms and raises an eyebrow.\nYou might take this as a visual form of feedback and not really know what to do with it. There are all these other methods of feedback that we're getting on a daily basis that we have zero idea of what to do with.\nI recently wrote an article for opensource.com where I was talking about feedback loops. I was talking about the idea that for everybody in the DevOps space \u2011\u2011 and this might be a very salacious thing \u2011\u2011 you just need to stop thinking about the feedback loops that you're worried about at work.\nAll the computer feedback loops, and all those things that come out of a machine, you just need to stop worrying about them right now. You need to start focusing on the human feedback that you're getting and the human feedback loops that you're trying to cultivate, because, I guarantee you, those feedback loops are about 90 times harder to resolve than whatever's going on with your computers.\nIf your production server is down, don't ignore that. I assure you, you can resolve [laughs] that problem.\nIf you are having a fight or a negative interaction with somebody that you're working with, I guarantee you that, if you cannot figure out how to deal with that feedback loop, you will continue to not be able to deal with that feedback loop, and it probably will also impact other relationships that you have around that particular interaction.\nOne of the things that I've been kind of thinking about lately that might help folks processing feedback is to also recognize that we live in a digital age where there is so much feedback coming in that sometimes it's hard to identify what feedback we actually need to listen to.\nWe were talking last podcast about distributed teams, and the nature of IRC, and the amount of information that comes in across just one form of communication. If you are a person who likes to participate in open source communities, email will be a problem. Trying to keep up with email from a popular open source project like kubernetes is impossible.\nIt's just impossible. You can't do it. So at some point you have to start identifying what feedback is the right feedback to focus on. That is going to also be critical to your addressing the entire loop.\nGordon: \u00a0What you say there in terms of having all these numbers, metrics, what have you, the fact is it's true from a technical standpoint, too. I was at the DevOpsDays and I think it was London earlier in the year, and we were talking about metrics. We were having an OpenSpace about metrics. What do you really want to measure?\nOne of the participants, he made the comment that his company, they used to get this monthly report that tracked, I think it was 2,000 something metrics associated with their IT systems, and nobody ever looked at this thing.\nEven if we're just talking about the technical aspect of things, much less the entire system, it includes people. It's really important to think about what matters. Maybe you won't log all that other stuff, and use Splunk, and maybe do predictive analytics, but it's not something you should be focusing your attention on, on a day\u2011to\u2011day basis.\nJen: \u00a0The win here is not the number of data points that you're logging. The win is, \"If you're logging the data, is your company actually using the data that you are logging?\" Because, frankly, logging data costs you money, and it costs you a lot of money. It's not for free. If you are using Splunk, you pay them based on the amount of data that you're storing.\nYeah, [laughs] just to produce a report that no one is doing anything with is not the ideal feedback loop.\nGordon: \u00a0It may have been the same event. Somebody from \u2011\u2011 I think it was Google \u2011\u2011 made the comment that data you don't use has a negative ROI.\nJen: \u00a0A couple years ago we were talking about that. It was something like black data or dark data, or something like that. [laughs] It was ridiculous buzzword. In any case, the most critical part about feedback loops is not just receiving the data, it's understanding what to do with it, which is what I was alluding to before.\nYou can be getting a bunch of information coming in, so data from somewhere is the first step. You're actually getting some sort of feedback from something, whether it be human or machine. Identifying which data you want to respond to is important. Figuring out what it means when something happens is important.\nThen, somehow, emotionally tying value, and that's an individual thing. The example I used in my blog article was the fact that my husband...he's a video gamer, and he plays a game called \"Dark Souls.\" He continues to run around in this game and die over and over and over again.\nWe were having a simple conversation in which he told me that he had a lot of souls, which is the monetary system in the game. I said to him, \"If you die, you're going to lose all your money.\"\nHe's like, \"Oh, no, no, no. That won't happen.\" I was like, \"Sure,\" but I knew for sure that he was going to die again. Sure enough, a day later I heard him cursing in the basement about this. I said, \"Sure enough.\" He died and he lost all his souls.\nI looked at that situation and I thought, \"Well, gosh. I'm thinking about feedback loops, and I should assign that whole thought process that I've been having on feedback loops and figure out how I might have compelled him to change his behavior in order to not be in the situation right now where he's angry in the basement.\"\nI thought, \"Well, instead of me just saying, 'you're gonna die,' I could have said something to the tune of, 'You've got a lot of money, and there's all these things that you can buy. You could upgrade your character. You could do all these things with it. What are the things that you want, that you haven't done yet? You don't have to spend it all.'\"\nI know he's frugal, which is probably why he's not spending it. I thought, \"You can spend half of it, and just go ahead. It's OK.\" He could have actually spent half of it and gotten a brand new sword or something and been happier for it, but he didn't.\nIt's the lack of assigning some sort of emotional value to it, or emotional feeling to it that prevents people from understanding feedback, or really participating in a loop.\nIf you go all the way back to that early conversation about stand up and where it becomes just a status meeting for everybody, it's because no one is assigning, or at least buying into, the idea that it\u2019s actually going to improve the situation for the team. No one really understands how it does that. No one really sees the value of participating in the activity, and, therefore, they don't.\nThat is the fundamental part in the feedback loop when things kind of go south. You can monitor the heck out of your computer systems, but, if you fundamentally don't care whether they're up or down, it doesn't matter. It just doesn't. That's when you can...sure, monitoring what data you should be doing on your systems, throughput, all that good stuff is really critical.\n\"What is the most valuable data you should be capturing?\" means absolutely nothing if you don't care about the data. Caring about the data is a human quality. You can't make somebody care about it, they just have to want to.\nGordon: \u00a0Talking about teams and for the help of teams, what are some of the things as Agile Architect, Agile Coach that you really think about, that you really keep your eye on?\nJen: \u00a0The engagement of teams. I tend to pop in and out of a lot of team meetings. I keep my eye on a lot of different people. We, as an industry, have an engagement problem. We have a bunch of people who are incredibly highly paid thought workers who are probably taken advantage of a little bit.\nI don't mean that in a way that we should have higher egos, but we are also \u2011\u2011 especially in the United States \u2011\u2011 in a situ... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9575228691,
        "format_confidence":0.959002614
    },
    {
        "url":"https:\/\/www.infoq.com\/presentations\/netflix-edge-scalability-patterns\/?utm_source=presentations&utm_medium=sf&utm_campaign=qcon",
        "text":"InfoQ Homepage Presentations Scaling Patterns for Netflix's Edge\n\nScaling Patterns for Netflix's Edge\n\n\n\nJustin Ryan talks about Netflix\u2019 scalability issues and some of the ways they addressed it. He shares successes they\u2019ve had from unintuitively partitioning computation into multiple services to get better runtime characteristics. He introduces us to useful probabilistic data structures, innovative bi-directional data passing, open-source projects available from Netflix that make this all possible.\n\n\nJustin Ryan is Playback Edge Engineering at Netflix. He works on some of the most critical services at Netflix, specifically focusing on user and device authentication. Years of building developer tools has also given him a healthy set of opinions on developer productivity.\n\nAbout the conference\n\nSoftware is changing the world. QCon empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams.\n\n\nRyan: My talks can be a little less dense on the slides here. We're going to bring it down a little. I guess we're going to talk about laundry. Everyone knows how to do laundry, we all know how it works. We get a big pile of clothes, are all over the place, and you have to start folding them and you have different clothes. I got a family of four, so I have my wife's clothes, her pants. I got kids' socks all over the place and I need a way of folding it. I have a certain way of folding it, and then my spouse has a certain way of folding it. The way she goes about it is she folds, which is great - it's great that she's folding laundry, and I don't have to do it - but what she's going to do is stack it all up in a big pile. Every time you get a piece of laundry and you're folding it, she just puts it into a pile, and then on goes the next one, and you end up with a pile of some kid pants and my shirt, and to put it all over the house, you have to walk over the house. Admittedly, I do appreciate the exercise, can't argue with that, but parents, we're busy - we don't have time for that. I have a slightly different way of doing it. I take over the whole living room - I'm talking the couch, the coffee table, the ottoman. I make separate piles for each kind of clothing - kid's pants, kid number one pants, kid number two pants, something like that. I take over the whole living room, then I can go around the house putting my pants away, my shirts away, my kids' pants away, kids' shirts away, and it goes really fast.\n\nWhat we're doing here is making a space-time compromise. It sounds like software, but it's not. It's t-shirts and pants and stuff like that. I was able to take the same amount of time folding. It didn't take me crazy long to fold the clothes. I just took them in more space, and then when I was able to put them away, I was able to save time. That's this classic trade-off that we as architects have to think about. What happens when you have 158 million t-shirts?\n\nLast reported by Netflix, we have 158 million memberships with the service. When I started eight years ago, we had less than a million, so at least you can get a sense of this growth and what happens over time and how things aren't the same as they used to be. It is not one monolith server we run in one little data center. It is the full-blown microservice architecture with tons of features, with services calling services, almost like to say ad infinitum because it does feel like that sometimes. That's a lot more complex than a few servers. There's a lot of servers, and there's problems that come up when you have that many servers and that many systems running altogether, as you can imagine.\n\nYou might hear me say edge. Hopefully, you read the title of the talk - edge is in there. That means the edge of this graph. For most people, the far left here where it says edge. I'm talking about these four nodes over here, the edge when the data comes into the service. There's a lot of other services going on here. I'm just not generally talking about that. I'm talking about edge. It's where I work, and it's where we saw the most scale.\n\nScaling Problems\n\nWhen we talk about that scale, there's problems. That's what I was alluding to, and I also want to get to a little bit. Like, debuggability. When you have 1,000 servers, you have 1,000 logs that go through, and there's no way you're going to go to every single machine and look at the logs anymore. That's just out, that's not an option anymore. Debugging these problems becomes harder because a lot of problems can lurk in 1,000 servers.\n\nInfrastructure - when you can keep some piece of infrastructure in one box and tracks everything, that's great, but as these number of servers is growing, you deal with 10,000 instances, 20,000 instances, it gets more difficult. Things start to fall apart. Then the third point there is managing. We subscribe to DevOps. I'm the developer, I'm operating, and I'm deploying it. I now have to deploy 1,000 instances. Now, there's automation around it, but things just take longer now, and I have to monitor more servers. Things just get harder.\n\nOn the topic of scalability, it's the ability to take more traffic without noticeable degradation. I should be able to take more traffic and not have the whole system collapse. It means that I have problems that don't exist on a smaller system, but it also means that I want to give myself some headroom. I don't need to design for 10X scale, I just want 2X. I want to be able to failover to a region. I want some new feature to ship and not have the system fall over.\n\nThat's where I want to talk about scalability and where I'm coming to here. I'm trying to give myself a little leeway here as my system grows because, as you saw, the system is going to grow. That's a given. The way we do that, we make some trade-offs. We're architects here, this is what we do. We look for a degree of freedom that we can move around in to make the solution work without either costing lots of money or lots of servers, or something like that. A classic trade-off here would be the CAP theorem. In Netflix, we have a service discovery service called Eureka. It's open-source, feel free to check it out. It leans heavily on the A and the P, available and partition tolerant. Then when it does that, it takes all the instances it's keeping track of, and it distributes it out over to all the other instances, meaning if you get a partition, it still knows about these instances in general. That's a trade-off. It also means that when you have 10,000 instances, and you add one more, you have to go tell the other 10,000 about that one new one, and that gets into the infrastructure side of things.\n\nI'm going to focus on the architectural things that we can do as architects and not on the policy decisions. If you have a policy to keep data for six months and you turn it to three months, that's less data, that will cost you less money, go for it, save us money. That's not what I'm talking about. That's a turning the dial kind of thing, like just turning the dial down. Not super helpful. Not what I'm talking about, I should say. Likewise, if you run less servers, there'll be less problems. Don't run less servers just to run less servers. Don't run them hot. We've learned that this has caused additional latency, and that's not great either. I'm talking about being smart about this complexity as we add it.\n\nThe things that I'm going to show, I have some case stories I'm going to go through. Each of them was implemented with one or two engineers over the course of a few weeks, maybe a month. The number of person-months here is pretty low. I would say the total amount of work is not a lot. It's something that I think you guys could walk away with a little bit. I would say that all the technologies we used was generally off-the-shelf. This is not some academic paper you have to implement. You should feel perfectly capable of implementing the things that I'm going to show here.\n\n\nI'm going to go through five use cases today. I'm going to focus a little more at the beginning, and as I go on, they're going to get a little shorter and faster just so I can get through enough material. First one, how do you tell someone is logged in? How do you tell if they're a member? That's pretty important. You're going to show a very different website if someone is logged in or not or if their membership is active or not. And you have to tell this on every request, this is a stateless system. They're going to come with a request, and another request. You have to know if they're a member or not. You could say, \"Every request comes in the system, we just go to the database. Look it up. No big deal.\" It does become a big deal when you're doing 2 million requests per second. That is the scale that I'm talking about here. There are databases that scale like that. I would like to avoid it. My scalability message is that I want to give myself a lot more leeway. If I don't have to put pressure on the database, let's not.\n\nLet's go back to what we have. We have cookies, that's how most of our devices talk to Netflix or service, that gives each the cookies. These are long-lived cookies. They sit on your box for a year, nine months, but in there is a little expiration date that we put in, and this expiration date so tells us how long we can trust the cookie. If it comes into our system and we crack it open, and we look, we can look for its expiration. That's the dot in the middle that you see up here. If it has not expired, we're just going to trust it. This fits our business use case. At Netflix, if you're familiar with it, you're going to load it up, and maybe the last time you used the service was last night, so it's been eight hours. We're going to need to go make sure you're still a member. After that, w... (truncated)",
        "topic_id":2,
        "format_id":21,
        "topic_confidence":0.9672333598,
        "format_confidence":0.8921129704
    }
]