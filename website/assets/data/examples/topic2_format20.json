[
    {
        "url":"http:\/\/browserchooser2.codeplex.com\/wikipage?title=Release%20Path",
        "text":"\nThis page will serve as a roadmap to release of Browser Chooser 2\n\nAlpha 1\n\n  \u2022 Update Names and Contributors to differentiate between original and BC2\n  \u2022 Convert 'On on goto' to correct Try Catch blocks (I know they are slower but more robust).\n  \u2022 Fix file names and class name to proper names and casing (on going)\n  \u2022 Add flexible first screen\n\nAlpha 2\n\n  \u2022 Enable default browser settings\n  \u2022 Enable short url lookup\n  \u2022 Enable auto-update - latest version is populated in the source files, publicly available\n  \u2022 Enable \/ detect \/ migrate \/ default to portable mode\n  \u2022 Update links to BC2\n\nAlpha 3\n\n  \u2022 Enable dynamic protocol and filetype (Per conversation in 930\n  \u2022 Complete windows 8+ support\n  \u2022 Extract icons from EXEs\n  \u2022 UI cleanup\n\nBeta 1\n\nSee Beta 1 To-Do List for what is in Beta 1\n\nPost Release\n\n  \u2022 Add option to download known browsers instead of limited built-in list\n\nThis will be expanded as the source code is explored and refactored.\n\nLast edited May 6, 2015 at 2:51 PM by gmyx, version 6",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9658550024,
        "format_confidence":0.9522938728
    },
    {
        "url":"http:\/\/www.lispworks.com\/documentation\/lw51\/CLHS\/Issues\/iss069_w.htm",
        "text":"\n\n\nForum:         Cleanup\n\n\nReferences: CLtL p.208, 212\n\nRelated issues: IEEE-ATAN-BRANCH-CUT\n\nCategory: CHANGE\n\nEdit history: Version 1, 13-Dec-88, Steele\n\nProblem description:\n\nThe formula that defines ATAN results in a branch cut that is at\n\nvariance with the recommendations of Prof. W. Kahan and with the\n\nimplementations of that function in many computing systems and\n\n\n\nReplace the formula\n\narctan z = - i log ((1+iz) sqrt (1\/(1+z^2)))\n\nwith the formula\n\narctan z = (log (1+iz) - log (1-iz)) \/ (2i)\n\nThis leaves the branch cuts pretty much in place; the only change is\n\nthat the upper branch cut (on the positive imaginary axis above i)\n\nis continuous with quadrant I, where the old formula has it continuous\n\nwith quadrant II.\n\n\n(atan #c(0 2)) => #c(-1.57... 0.549...) ;Current\n\n(atan #c(0 2)) => #c(1.57... -0.549...) ;Proposed\n\nNote: 1.57... = pi\/2, and 0.549... = (log 3)\/2.\n\n\nCompatibility with what seems to be becoming standard practice.\n\nCurrent practice:\n\n(atan #c(0 2)) => #c(-1.57... 0.549...) ;Symbolics CL\n\n(atan #c(0 2)) => #c(-1.57... 0.549...) ;Allegro CL 1.1 (Macintosh)\n\n(atan #c(0 2)) => #c(-1.57... 0.549...) ;Sun-4 CL 2.1.3 of 10-Nov-88\n\n(atan #c(0 2)) => #c(1.57... -0.549...) ;Sun CL 2.0.3 of 30-Jun-87\n\n(atan #c(0 2)) => #c(1.57... 0.549...) ;KCL of 3-Jun-87\n\nNote that in KCL the upper branch cut is thus continuous with\n\nquadrant I, but its lower branch cut is continuous with quadrant III!\n\nCost to Implementors:\n\nATAN must be rewritten. It is not a very difficult fix.\n\nCost to Users:\n\nIt is barely conceivable that some user code could depend on this.\n\nNote that the proposed change invalidates the identities\n\narctan i z = i arctanh z\n\nand arctanh i z = i arctan z\n\non the upper branch cut.\n\nThe compatibility note on p. 210 of CLtL gave users fair warning that\n\na change of this kind might be adopted.\n\nCost of non-adoption:\n\nIncompatibility with HP calculators.\n\n\nNumerical analystsmay find the new definition easier to use.\n\n\nA toss-up, except to those who care.\n\n\nSteele has sent a letter to W. Kahan at Berkeley to get any last\n\ncomments he may have on the matter.\n\nPaul Penfield of MIT, after whose article the Common Lisp branch\n\ncuts were originally patterned, endorses this change.\n\n[Starting Points][Contents][Index][Symbols][Glossary][Issues]\nCopyright 1996-2005, LispWorks Ltd. All rights reserved.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9535427094,
        "format_confidence":0.9497449398
    },
    {
        "url":"https:\/\/sigtar.com\/tag\/h264\/",
        "text":"HEVC media optimization\n\nDo you have a large collection of video media files not using HEVC (H265) yet? There is a massive amount of disk space coming your way if you flick over to the new video codec format.\n\nHEVC definitely lives up to its name, for most media you can expect a 70% or more disk savings from transcoding from an old codec. There are some catches though\u2026 If you want your TV to play it direct (i.e. straight off the file) the codec will need to be supported by it. You can of course get around this by using a media server such as Plex or Emby which will transcode from HEVC back to a compatible format.\n\nWhy would you transcode to HEVC? \u2013 again, disk space. HEVC as stated above can reduced you Media footprint significantly. You could boost your quality and save your disk space at the same time by recording at a higher resolution then applying the HEVC codec.\n\nI created a powershell script to transcode my media to HEVC using my AMD graphics card. The advantage of doing this is that transcoding completed by my GPU is significantly faster than my CPU. I do not have the graphics card in my media server, so instead connect via SMB and let my gaming machine run the transcoding from remote\u2026\n\nThe powershell script uses ffmpeg to ;\n\n  \u2022 transcodes video stream to hevc using AMD h\/w encoder\n  \u2022 copys all existing audio and subtitles (i.e. no conversion)\n  \u2022 works in batches (to prevent constant scanning of files) \u2013 able to set max batch size and processing time before re-scanning disk\n  \u2022 overwrites source with new HEVC transcode if\u00a0move_file = 1\u00a0(WARNING this is default!)\n  \u2022 checks to see if video codec is already HEVC (if so, skips)\n  \u2022 writes\u00a0transcode.log\u00a0for successful transcode (duration and space savings)\n  \u2022 writes\u00a0skip.log\u00a0for already hevc and failed transcodes (used to skip in next loop, errors in transcode.log)\n\nCheck here for updates and script \u2013 https:\/\/github.com\/dwtaylornz\/hevctranscode",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.7694448829,
        "format_confidence":0.5453820229
    },
    {
        "url":"https:\/\/git.netfilter.org\/libnetfilter_conntrack\/plain\/README?h=libnetfilter_conntrack-1.0.3&id=08f5d2318442a6a3bc19d732e4743e6685673077",
        "text":"libnetfilter_conntrack - userspace library for the connection tracking system (C) 2005-2011 Pablo Neira Ayuso ============================================================================= = Connection Tracking System = The connection tracking system is a in-kernel subsystem that stores information about the state of a connection in a memory structure that contains the source and destination IP addresses, port number pairs, protocol types, state, and timeout. With this extra information, we can define more intelligent filtering policies. Moreover, there are some application protocols, such as FTP, TFTP, IRC, PPTP that have aspects that are hard to track for a firewall that follows the traditional static filtering approach. The connection tracking system defines a mechanism to track such aspects. The connection tracking system does not alter the packets themselves; the default behavior always lets the packets continue their travel through the network stack, although there are a couple of very specific exceptions where packets can be dropped (e.g., under memory exhaustion). So keep in mind that the connection tracking system just tracks packets; it does not filter. For further information on the connection tracking system, please see the reference section at the bottom of this document. = What is libnetfilter_conntrack? = libnetfilter_conntrack is an userspace library that provides an interface to the in-kernel connection tracking system. = License = libnetfilter_conntrack is released under GPLv2 or any later at your option. = Prerequirements for libnetfilter_conntrack = Linux kernel version >= 2.6.18 ( and enable support for: * connection tracking system (quite obvious ;) * nfnetlink * ctnetlink (ip_conntrack_netlink) * connection tracking event notification API = Documentation = You can generate the doxygen-based documentation by invoking: $ doxygen doxygen.cfg = Examples = You can find a set of handy examples on the use of libnetfilter_conntrack under the directory utils\/ distributed with this library. You can compile them by invoking: $ make check = Heads Up = libnetfilter_conntrack used to provided two different APIs: The old one had several limitations, for that reason, it was deprecated time ago. The existing library only provides the new API that solves former deficiencies. Thus, make sure you use recent versions of libnetfilter_conntrack and, in case that you are using the old API, consider porting your application to the new one. Since libnetfilter_conntrack >= 0.9.1, you can use the same handler obtained via nfct_open() to register conntrack and expectation callbacks (before this version, this was not possible). = References = [1] Pablo Neira Ayuso. Netfilter's Connection Tracking System:",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9802905321,
        "format_confidence":0.9707668424
    },
    {
        "url":"https:\/\/maidsage.com\/doku.php?id=architecture&bootswatch-theme=cerulean",
        "text":"Approved 2019\/07\/16 23:37 by drirmbda (version: 3)\n\n<= Back to FRONT\n\nA good architecture of SAFE Network is incredibly important for long-term success. This section will be an attempt to give an accurate overview.\n\nLayers 0-4 will provide the basis for communication, without modification. Internet\/Routing (Layer 3) will be based on IPv4 and\/or IPv6. Host-to-host Transport (Layer 4) will provide QUIC transport, which is based on UDP (a connection-less service). SAFE Network will not modify anything below and including Layer 3. SAFE Network will add to Layer 4, and functions in Layers 5, 6, and 7:\n\nLAYER 7 - Application: Vaults, SAFE Browser, Authenticator, SDNS, (not sure about SFTP, SNTP, SIMAP) (tentatively placed here until verified)\n\nLAYER 6 - Presentation: \u201cSelf-Encryption\u201d (tentatively placed here until verified)\n\nLAYER 5 - Session: Session ID and network entry point selection, Self-Authentication (tentative)\n\nLAYER 4 - Routing: SAFE Network PARSEC Consensus, and Kademlia Distributed Hash Table-based Routing, using QUIC\/UDP\n\nNote on CRUST\nNote on CRUST\n\nCRUST will be replaced by QUIC (originally: Quick UDP Internet Connections), which relies on UDP\/IP.\n\nThe SAFE Network is a virtual overlay network on the Internet, consisting of SAFE Network Nodes, which correspond to an instance of SAFE Network \u201cVault\u201d software in the real world. Nodes are dynamically organized by the network as Vaults come, gain trust over time, and leave the network.\n\nSAFE Network Address Space\n\nSAFE Network Nodes exist in the virtual overlay network in a multi-dimensional space called the XOR Space. There is no correlation between geographical distance (or IP address distance) between physical nodes in the real world, and distance between the same nodes in XOR Space. This is the result of how Node IDs are allocated by the SAFE Network. People running a Vault have no control over this process, which is an important security feature.\n\nIn XOR space, the distance between Nodes is the XOR of the IDs of the two Nodes, which is read as an unsigned integer. Distance is used for routing in this space and for organizing Nodes into groups, as well as for allocating responsibilities over other entities in SAFE Network XOR space such as \u201cClients\u201d and units of data (\u201cChunks\u201d), which also have an ID using which distances to Nodes can be calculated in exactly the same way.\n\nClients need to connect to Nodes to access the SAFE Network and the appropriate node is decided based on distance. Chunks are managed by groups of Nodes, which are also determined by distance. Chunks of data derive their ID from a hash of the contents of the chunk.\n\n  \u2022 Entities in XOR Space: Vault Nodes, Clients, Chunks (other?)\n  \u2022 XOR Space addresses length: 256 bits\n\nOrganization of Objects and Entities\n\nThe XOR space is dynamically subdivided by a sectioning algorithm into a growing number of dis-joint XOR sub-spaces as the number of Nodes grows. The sub-spaces are called \u201cSections\u201d. Nearby Sections of Nodes watch over each other, and within each Section a hierarchy of Nodes exists based on Age, which determines the level of trust, responsibility of a Node, and the power it has over other Nodes.\n\nSections run PARSEC consensus algorithms to make decisions, and sign decisions using a group signature so that neighboring Sections can verify them. Decisions (and events) are stored in Data Chains.\n\n  \u2022 Algorithms: Sectioning, \u2026 (other TBD)\n  \u2022 Section group size and split threshold: 8 (typical number which may be changed in the future)\n  \u2022 Consensus threshold: 5 out of 8 (62.5% (of \u201cquorum size\u201d?))\n  \u2022 Information held by Sections: Data Chains (of a Section, or a neighboring Section), Routing Table\n\nObject and Entity Types\n\n(open for review and contribution; may be incomplete or outdated, as of 2015)\n\n  \u2022 Users accessing the network (clients)\n    \u2022 Passports (ID linked to multiple sub-IDs)\n      \u2022 Anonymous ID (MA-ID) - for anonymous network access\n      \u2022 Public IDs (MP-ID) - for public network access\n      \u2022 Share IDs (MS-ID) - can be associated with multiple Passports - for access to private shares\n      \u2022 Proxy IDs (PM-ID) - for providing own resources\n  \u2022 People providing resources to the network: vaults (farmers)\n    \u2022 Vaults (instances of a vault node)\n      \u2022 ClientManager vault persona - part of a group but independently relaying traffic between client and the correct DataManagers\n      \u2022 DataManager vault persona - part of a group but independently forwarding requests to the correct DataHolders\n      \u2022 DataHolderManager vault persona - part of a group but independently selecting and observing DataHolders\n      \u2022 DataHolder vault persona - part of a group but independently judging validity of requests\n      \u2022 (what is the 5th persona, or is the 5th not a persona but the client?)\n\n(work in progress)\n\n  \u2022 Storage of ALL information is in a network wide DHT\n  \u2022 ALL nodes are connected to at least four neighbors\n  \u2022 ALL inter-node communication is authenticated and encrypted (PKI)\n\n3.4.1 IETF QUIC Introduction\n\nQUIC (originally: Quick UDP Internet Connections), is a new Internet transport protocol that intends to replace TCP and TLS. It relies on UDP datagrams over IP. This is a good introduction. Standard IETF QUIC started as Google's QUIC or qQUIC but has significantly diverged and improved.\n\nMultiplexing of multiple streams reduces the number of QUIC connections and handshakes and makes streams less sensitive to congestion and faster. Authentication and encryption are not delegated to a higher layer protocol like TLS resulting in higher speed and better security. A typical handshake takes only one round trip. However, UDP protocols are susceptible to reflection attacks and to avoid this the server can cryptographically confirm if requestor and destination are the same, at the expense of doubling the handshake duration.\n\nThere are some early adopter issues such as worse handling of NAT by the typical router designed with only TCP in mind. Second, connections are uniquely identified and tracked by connection ID because ports and even IP address of endpoints may change, which is a feature of QUIC. However, conventional routers may not be able to deliver the UDP packets correctly to the right end-point after such QUIC endpoint change. Finally, performance may suffer compared to TCP due to a general lack of off-loading capabilities of processing to OS or network interface hardware.\n\n3.4.2 MDSF Lib quic-p2p\n\nMaidSafe quic-p2p crate (0.2.0 docs,to do) is a library implementing QUIC using TLS 1.3.\n\nWatch the master branch.\n\n\nThe library provides 2 connection types in P2P mode: Bi-directional and uni-directional.\n\nUni-directional type is used to force to prove a node's ability to accept incoming connections and create outgoing connections, which is a necessary requirement to be accepted as a network peer worker node.\n\nThe library provides 3 connection types of different identity certification levels:\n\n  \u2022 Agreed CA certificates required\n  \u2022 Private CA certificates allowed\n  \u2022 No Identity certification required\n\nThe most recent 200 endpoints that are not behind NAT, and their certificates are cached and used for re-joining of nodes.\n\n3.5.1 Kademlia Routing Introduction\n\nKademlia routing (it's just a name) was first proposed in a paper of 2002. A Kademlia route is a shortest sequence of nodes to query to locate a node with a given address, in a network of nodes with distributed routing tables. For a network with n node addresses the number of nodes to query is Order log(n).\n\nNodes build their individual routing tables from queries they receive during the routing process. Routing tables contain IP addresses and Port numbers for a limited number of certain other nodes. Communication between nodes during and after this process continues to be physically routed using conventional routing.\n\nNodes know exponentially less about nodes that are farther away, and are experts about nodes in their closest vicinity. Kademlia routes converge to an increasingly common route for each given destination address, as that destination is approached. This exponentially homing-in behavior from originating node towards given destination node is thanks to a special kind of mathematical distance that is used to decide which node to query next.\n\nKademlia's creators not only proposed an algorithm but also a \u201cnovel\u201d distance metric, the XOR distance. It satisfies all requirements of a proper metric:\n\n  1. Zero distance between two nodes always means that those nodes must be one and the same.\n  2. Distances between two nodes are the same no matter which is the origin, and are always positive.\n  3. The triangle inequality applies; for any three nodes there always is a distance between one pair that is equal or less than the sum of the distances between the two other possible pairs of nodes.\n\nHowever, XOR distance is non-Euclidean, i.e. Euclid\u2019s 5th \u201cparallel\u201d postulate does not hold. This results in some very interesting and hard to grasp properties:\n\n  \u2022 Nodes observed from any particular node, all have a unique distance to that observing node.\n  \u2022 Closeness (e.g. k-nearest nodes) is uni-directional or asymmetric, which means that a node n1 in the set of k-nearest nodes to node n0, does not need to be in the set of k-nearest nodes to n1.\n\nThis includes an answer explaining most of the above by @fraser in other better words. This is a long but useful write-up by @dirvine to help understand things better intuitively and pointing out many peculiarities of XOR distance. Finally there is this Medium post by MaidSafe on this topic.\n\nArticle of 2015 on Kademlia applications.\n\n3.5.2 XOR Distance\n\nThe following table shows XOR distances in a 3-bit space, or 0..7, to help you grasp XOR distances better. Horizontally the table shows source node number, and the vertical axis is decimal increment to obtain the destination node number modulo 8. For example, distance from node 2 (on top), to node 4 (on skip 1 row), is 6.\n\nThe ... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.8259106278,
        "format_confidence":0.902289331
    },
    {
        "url":"https:\/\/app.cnvrg.io\/docs\/core_concepts\/apps.html",
        "text":"# Apps\n\ncnvrg is a full-stack platform, designed to simplify every aspect of your workflow.\n\nOne important part of any data scientist\u2019s work is presenting their work to others. That project you\u2019ve been working on for weeks won\u2019t go on much longer if you aren\u2019t able to effectively communicate what you have achieved. cnvrg can make that task easier than ever.\n\ncnvrg works easily and seamlessly with three major data communication platforms so you can use whatever is most convenient and efficient for your and your team\u2019s needs.\n\nThe topics in this page:\n\n# R Shiny\n\nShiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.\n\nLearn how to use R Shiny.\n\n# Use R Shiny\n\nTo set up an R Shiny dashboard:\n\n  1. Navigate to Apps in your project.\n  2. Click R Shiny\n  3. For File, enter the full path of the R Shiny app.R file.\n  4. In Advanced Settings:\n    1. For Image, select the default cnvrg_r image.\n    2. For Compute, click the compute you would like to use.\n  5. Click Publish. cnvrg will spin up the compute and start running the R Shiny server.\n\n\nThe R Shiny file must be called app.R.\n\nIf no path is given, app.R is assumed to be in the project's parent directory. If you enter the full path (in the project) for File, for example, my_folder\/app.R, then that file will be loaded from the correct subdirectory.\n\n# R Shiny logs\n\nThe logs for the R Shiny server can be found within the compute at \/var\/log\/shiny-server\n\n# Customize the R Shiny configuration with a custom image\n\nYou can modify the R Shiny configuration to better suit your own requirements. By default, cnvrg starts the R Shiny server using the command exec shiny-server 2>&1 and runs using the following configuration:\n\n# Instruct Shiny Server to run applications as the user \"shiny\"\nrun_as shiny;\n# Define a server that listens on port 3838\nserver {\n  listen 3838;\n  # Define a location at the base URL\n  location \/ {\n    # Host the directory of Shiny Apps stored in this directory\n    site_dir \/cnvrg; #or a path of the app.R file\n    # Log all Shiny output to files in this directory\n    log_dir \/var\/log\/shiny-server;\n    # When a user visits the base URL rather than a particular application,\n    # an index of the applications available in this directory will be shown.\n    directory_index on;\n\nIf you would like to start the server in your own customized way, create a custom image and save your custom configuration file as \/init. If \/init exists, cnvrg will use it start the shiny server otherwise it will run with the default server startup:\n\n# Make sure the directory for individual app logs exists\nmkdir -p \/var\/log\/shiny-server\nchown shiny.shiny \/var\/log\/shiny-server\nif [ \"$APPLICATION_LOGS_TO_STDOUT\" != \"false\" ];\n    # push the \"real\" application logs to stdout with xtail in detached mode\n    exec xtail \/var\/log\/shiny-server\/ &\n# start shiny server\nexec shiny-server 2>&1\n\n# Dash\n\nDash is a productive Python framework for building web applications.\n\nWritten on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python.\n\nThrough a few simple patterns, Dash abstracts away all of the technologies and protocols that are required to build an interactive web-based application. Dash is simple enough that you can bind a user interface around your Python code in an afternoon.\n\nDash apps are rendered in the web browser. You can deploy your apps to servers and then share them through URLs. Since Dash apps are viewed in the web browser, Dash is inherently cross-platform and mobile ready.\n\nLearn how to use Dash.\n\n# Voil\u00e0\n\nVoil\u00e0 turns Jupyter notebooks into standalone web applications.\n\nNot only does it create visual graphs to make reporting easier, it also democratizes Jupyter notebooks for non-technical users. With Voil\u00e0, every notebook can be turned into a Voil\u00e0 app. Better yet, you can create a customized dashboard with results of your machine learning project. Voil\u00e0 supports Jupyter interactive widgets, including roundtrips to the kernel. It improves the conciseness by not permitting arbitrary code execution by consumers of dashboards.\n\nOther benefits of Voil\u00e0 are that it is built upon Jupyter standard protocols and file formats, and works with any Jupyter kernel (C++, Python, Julia). This makes Voil\u00e0 a language-agnostic dashboarding system. On top of everything, Voil\u00e0 is extensible and includes a flexible template system to produce rich application layouts. That means you can easily customize your dashboard based on the Jupyter notebook being deployed.\n\n# Deploy an App\n\nTo deploy an app:\n\n  1. Enter the project you want to publish your app from.\n  2. Select Apps on the sidebar.\n  3. Choose the app type, enter the file that will be used for the app and allocate a compute.\n  4. If necessary, choose a dataset and fill in the other options.\n  5. Click Publish.\n\ncnvrg will do the rest for you!\n\n# Update a Deployed App\n\nYou can update a deployed app without redeploying from scratch. To update an app:\n\n  1. Go to the app you wish to update.\n  2. Click Versions on the top menu bar.\n  3. Select a new cnvrg.io Commit to update the service to.\n  4. Click Update. cnvrg will update the app to use the new chosen commit.\n\nWhen connected to git, clicking Update will also pull the latest git commit for your branch (even if you do not choose a new cnvrg Commit).\n\nLast Updated: 8\/25\/2020, 8:06:12 AM",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9403790236,
        "format_confidence":0.9500359297
    },
    {
        "url":"https:\/\/thunder.github.io\/thunder-documentation\/feature-notes",
        "text":"Skip to main content\n\nRelease 3.2\n\nAutosave Form\n\nThe \u201cAutosave Form\u201d module provides an autosave feature for all forms. The autosave submits will be triggered every 60 seconds and store the changes from the currently logged-in user in the database.\n\nIn Thunder, it is active for node articles and basic pages and all media types.\n\nFeature walkthrough\n\nFor demonstration purpose, it\u2019s recommended to decrease the interval to trigger the autosave on admin\/config\/content\/autosave_form.\n\nAutosaves only works on existing entities, for now, so navigate to one. For example, you can use Node 6 from the Thunder Demo module.\n\n  \u2022 Wait for the first \u201cSaving draft\u2026\u201d without doing any changes\n  \u2022 Reload the page and you will notice no restore messages.\n  \u2022 Now change the title to \u201cMy new title.\u201d\n  \u2022 Reload the page again and you will get a restoring message.\n  \u2022 Resume editing\n  \u2022 Add a new tag \u201cCMS\u201d\n  \u2022 Add a new image paragraph between some existing paragraphs.\n  \u2022 Wait for the first \u201cSaving draft\u2026\u201d and reload the page again\n  \u2022 Resume editing and verify that your changes are still there\n  \u2022 Reload again and discard the changes\n  \u2022 Your initial article is back",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9341325164,
        "format_confidence":0.9661146998
    },
    {
        "url":"https:\/\/docs.lansa.com\/14\/en\/lansa087\/content\/lansa\/wamengb2_2220.htm",
        "text":"The field name to be used to post to the WAM the value that is specified in the reentryvalue property. The field name should be in single quotes.\n\nDefault value\n\n\nValid values\n\nAny repository- or WAM-defined field name. A list of known field names is available by clicking the corresponding dropdown button in the property sheet.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.5176909566,
        "format_confidence":0.9701958299
    },
    {
        "url":"https:\/\/developer.apple.com\/design\/human-interface-guidelines\/macos\/menus\/menu-anatomy\/",
        "text":"A menu presents a list of items\u2014commands, attributes, or states\u2014from which a user can choose. An item within a menu is known as a menu item, and may be configured to initiate an action, toggle a state on or off, or display a submenu of additional menu items when selected or in response to an associated keyboard shortcut. Menus can also include separators, and menu items can contain icons and symbols, like checkmarks. By default, all menus adopt translucency.\n\nScreenshot of the Safari File menu, with the Share submenu opened and the AirDrop item chosen.\n\nWhen a menu is displayed onscreen, it remains open until the user chooses a menu item, navigates to another menu, clicks outside of the menu, switches to another app, or quits the app; or until the system displays an alert.\n\nTypes of Menus\n\nThere are three types of menu in macOS, each of which has a specific use case, as noted in the following table:\n\nMenu Type Description\nMenu bar menu Exposes app-specific menu items when chosen from the menu bar at the top of the screen. An app typically displays several menus in the menu bar. See Menu Bar Menus.\nContextual menu Exposes menu items related to the user\u2019s current context. A contextual menu (or shortcut menu) is displayed by Control-clicking a view or selected element in an app. See Contextual Menus.\nDock menu Exposes system-defined menu items (like Show in Finder) and app-specific menu items (like Compose New Message) when Control-clicking an app\u2019s Dock icon. See Dock Menus.\n\nTIP A pop-up button, often referred to as a pop-up menu, is a type of button that displays a menu of choices when clicked. See Pop-Up Buttons.\n\nA title describes a menu or menu item. People navigate menus and choose menu items based on their titles, so it\u2019s important for the titles to be accurate and informative.\n\nUse title-style capitalization. Title-style capitalization is used consistently for menu and menu item titles throughout the system. For more information, see Apple Style Guide.\n\nTIP Contextual and Dock menus don\u2019t need titles because they\u2019re opened after focusing on a selection, view, or Dock icon. See Contextual Menus and Dock Menus.\n\nA menu title describes the contents of the menu.\n\nProvide intuitive menu titles. A menu title should help people anticipate the types of items the menu contains. For example, you would expect a menu titled Font to include options for adjusting text attributes, not performing editing activities like copying and pasting.\n\nKeep menus enabled even when menu items are unavailable. It\u2019s important for people to be able to browse the contents of all menus to learn where commands reside, even when those commands aren\u2019t available.\n\nMake menu titles as short as possible without sacrificing clarity. One-word menu titles are best because they take up very little space in the menu bar and are easy to scan. If you must use more than one word in a menu title, use title-style capitalization.\n\nUse text, not icons, for menu titles. Only menu bar extras use icons to represent menus. See Menu Bar Extras. It\u2019s also not acceptable to use a mixture of text and icons in menu titles.\n\nA menu item title describes an action or attribute.\n\nUse verbs and verb phrases for menu items that initiate actions. Describe the action that occurs when the menu item is chosen, such as Print or Copy.\n\nUse adjectives or adjective phrases for menu items that toggle attribute states. Describe the attribute the menu item affects. Adjectives appearing in menu item titles imply an action and can often fit into the sentence \u201cChange the selected object to\u2026\u201d\u2014for example, Bold or Italic.\n\nRefrain from using articles in menu item titles. For example, use Add Account instead of Add an Account, or Hide Toolbar instead of Hide the Toolbar. Articles rarely add value because the user has already made a selection or entered a specific context. Use this style consistently in all menu item titles.\n\nUse an ellipsis whenever choosing a menu item requires additional input from the user. The ellipsis character (\u2026) means a dialog or separate window will open and prompt the user for additional information or to make a choice.\n\nDisable unavailable menu items. A disabled menu item\u2014which appears gray and doesn\u2019t highlight when the pointer moves over it\u2014helps people understand that an item is unavailable.\n\nConsider assigning keyboard shortcuts to frequently used menu items in the menu bar. A keyboard shortcut, like Command-C for Copy, lets people quickly invoke the menu item anytime using a simple keystroke. Keyboard shortcuts aren\u2019t used in contextual menus or Dock menus.\n\nA submenu is a menu item that operates as a menu, displaying a set of nested items when selected. Submenus let you construct hierarchical menus that group related commands together to keep menus organized and intuitive. For example, the Edit menu often includes Find, Spelling and Grammar, Substitutions, Transformations, and Speech submenus, each of which contains menu items that are helpful when editing text. Menu items that have a submenu include a triangle to differentiate them from menu items that don't have submenus. When the user highlights (or uses the keyboard to select) a menu item with a triangle, the submenu appears alongside its parent menu.\n\nLimit the use of submenus. Every submenu adds a layer of complexity and hides menu items from the user. Reserve submenus for when you have groups of closely related commands that can be intuitively grouped under a single parent menu item, or when you need to reduce the length of your menus.\n\nLimit the depth and length of submenus. If you must include submenus, restrict them to a single level. If a submenu contains more than five items, consider giving it its own menu.\n\nMake sure the menu items within a submenu are logically related. In general, submenus work best for menu items that toggle attributes on and off rather than initiate actions. For example, the Format menu in Pages has a Font submenu that includes menu items for enabling and disabling text attributes, such as Bold, Italic, and Underline.\n\nProvide an intuitive submenu title. Provide a succinct, descriptive title that hints at the menu items the submenu contains. For guidance, see Menu and Menu Item Titles.\n\nKeep submenus enabled even when their nested menu items are unavailable. It\u2019s important for people to be able to browse menus and submenus to learn where commands reside, even when those commands aren\u2019t available.\n\nUse a submenu instead of indenting menu items. Indentation results in an inconsistent interface and doesn\u2019t express relationships between menu items. If you need to indent, then a submenu is a better choice.\n\nOrganizing Menu Items\n\nOrganize menu items to help people locate commands.\n\nUse separator lines to create visually distinct groups of related menu items. The number of groups to provide is partly an aesthetic decision and partly a usability decision.\n\nPlace the most frequently used items at the top of a menu. When users click to open a menu, their focus is on the top area of that menu. Placing the most used items at the top of the menu ensures that people will always find the item they\u2019re looking for. At the same time, avoid arranging an entire menu based on frequency of use. It\u2019s better to create groups of related items and place the more frequently used groups above the less frequently used groups. For example, the Find Next command typically appears beneath the Find command.\n\nCreate separate groups for menu items that initiate actions and menu items that set attributes. Menu items become unpredictable when the behavior of grouped items varies.\n\nGroup interdependent menu items that set attributes. People expect to find related attribute menu items together. For example, when setting font attributes, it makes sense to see the menu items for applying bold, italic, and underline grouped together. Within a group, attribute menu items can be mutually exclusive (the user can select only one item, like an alignment) or independent (the user can select multiple items, like bold and italic).\n\nMake it easy for people to locate menu items by grouping similar actions together. For example, the Arrange menu in Numbers includes groups for aligning and distributing objects, as well as items for Group and Ungroup.\n\nConsolidate related menu items. If a term is used more than twice within a group of menu items, consider dedicating a separate menu or submenu to the term. For example, instead of offering separate menu items for Sort by Date, Sort by Subject, and Sort by Unread, the View menu in Mail includes a Sort By submenu that contains items like Date, Subject, and Unread.\n\nBe mindful of menu length. In general, long menus are difficult to scan and can be overwhelming. If a menu becomes too long, try redistributing its items. See if some items fit naturally in other menus or if it makes sense to create a new menu. Groups of related items sometimes call for the use of a submenu. See Submenus.\n\nDon\u2019t intentionally design a scrolling menu. Scrolling menus have items that extend beyond the top or bottom edge of the screen. Scrolling menus are acceptable when a menu contains user-defined or dynamically generated content. For example, the History and Bookmarks menus in Safari may have scrolling submenus. The History menu includes submenus for websites visited on specific days; the length of the Bookmarks submenus depends on how the user has organized their favorite websites. A scrolling menu displays a downward or upward triangle at its bottom or top to hint at scrollable offscreen content.\n\nVariable Menu Items\n\nSometimes it makes sense for a menu to display slightly different items based on a user action. macOS supports two ways to do this: dynamic menu items and toggled menu items.\n\nDynamic Menu Items\n\nA menu item is dynamic when its behavior changes with the addition of a modifier key (Control, Option, Shift, or Command). For example, the Minimize item in the Window menu changes to Min... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.729806602,
        "format_confidence":0.9452585578
    },
    {
        "url":"http:\/\/ltt-www.lcs.mit.edu\/Public\/scanrec\/scanrec.cstr.1.1.1.html",
        "text":"Scanned document record, version CSTR 1.1\nOctober 30, 1994\nBy Jerry Saltzer\n\nThe objective of this document is to specify both the form and content of\nthe information that must be captured when a document is scanned, as an\non-line record that becomes a component of the scanned form of the\n\nThe objective of the scanning record is to capture information that is not\nexplicit in the scanned image, yet is needed:\n\n1.  to view, display, or print the image properly.\n\n2.  to understand how to interpret the image.\n\n3.  to meet contractual or legal requirements.\n\nThe form of a scanning record is a series of named fields, one per line.\nEach field line begins with the field name followed by a colon, then the\nfield value. Any line of the scanning record may have a comment at the end,\npreceded by a semicolon.  Any program reading the file will ignore all\ncomments.  A field line may be of any length, but it may NOT include typed\ncarriage returns.  This form is intended to be easily created by a human\noperator using a word processor, but it is to be used by computer programs\nthat browse, display, or print documents, so the contents of some of the\nfields must conform to specific standards.\n\nThe content of a scanning record is most easily explained by first\nexhibiting a complete example, and then describing the requirements on\nfield contents:\n\nScanning record version: CSTR 1.1\nPublishing department: M. I. T. Lab for Computer Science\nTechnical report label:  MIT-LCS-TM-13\nDocument series: TM\nPage count:  24\nImage count:  31\nScanning agent:  M. I. T. Document Services\nOriginal form: single-sided\nOriginal size:  8.5 x 11\nIntended print form: double-sided\nIntended print size:  6 x 9\nScanner used: Fujitsu 3096g ADF\nSoftware used:  Optix version 1.01\nOperator:  Michael Cook\nDate Scanned: 9\/28\/1994\nResolution(dpi): 400\nGreyscale depth(bits): 8\nScanner settings: default\nFile name generator:  LCS-TM-13-\nScanning record file name:  LCS-TM-13-data.txt\nMap:  LCS-TM-13-image-01=cover\nMap:  LCS-TM-13-image-02=blank\nMap:  LCS-TM-13-image-03=unnumbered          ;  title page\nMap:  LCS-TM-13-image-04=blank\nMap:  LCS-TM-13-image-05=unnumbered          ;  acknowledgement\nMap:  LCS-TM-13-image-06=blank\nMap:  LCS-TM-13-image-07=page 1\nMap:  ...\nMap:  LCS-TM-13-image-24=page 18\nMap:  LCS-TM-13-other-01=spine\nMap:  LCS-TM-13-other-02=supporting          ; instructions to printer\nMap:  LCS-TM-13-other-03=doccontrol\nMap:  LCS-TM-13-other-04=calibration IEEE-167a-1987\nMap:  LCS-TM-13-other-05=calibration AIIM-#2\nMap:  LCS-TM-13-other-06=agent\nMap:  LCS-TM-13-other-07=scancontrol\n\nHere are the requirements on the field contents that come about in order\nfor a computer program to be able to unambiguously interpret the scanning\n\nScanning record version:  must contain exactly the string shown in the example.\n\nPublishing department, Technical report label, Document series, Scanner\nused, Software used, and Operator:  each can contain any string of\n\nPage count and Image count:  must contain integers.  (The page count is the\nnumber of different original sides that need to be scanned.  The image\ncount is the number of image files created for this document, including\ncalibration, identification, and blank targets, etc.)\n\nOriginal form and Intended print form:  Must contain either \"single-sided\"\nor \"double-sided\".\n\nOriginal size and Intended print size:  Must contain two decimal numbers\nseparated by the letter \"x\".  The integers are assumed to be measurements\nin inches.\n\nDate scanned:  Month\/Day\/Year, each component being an integer and the year\nbeing four digits.\n\nResolution and Greyscale depth:  must be integers\n\nScanner settings:  At this time the only allowed value for this field is\n\nFile name generator:  A string that is used to prefix the names of all\nimage and data files produced in the course of scanning this document.\nNote that upper- and lower-case letters in the file names appear in this\nfield exactly as they appear in the actual file name, because on some\ncomputer systems upper and lower-case letters are distinct.\n\nScanning record file name:  The exact file name of the scanning record itself.\n\nNote:  an optional field containing any desired string.  If this field is\nnot empty, a properly constructed browser or print program will display its\ncontents to the reader.\n\nMap:  Every scanned image is represented by a Map: field.  This is the only\nfield that may appear more than once in the scanning record.  All Map\nfields must be together at the end of the scanning record.  With one\nexception, every Map field consists of an image file name followed by an\nequal sign followed by an image content identifier.  The content identifier\nmust be one of the following:\n\n     scancontrol              Scanning project control form\n     control                  Government control form\n     calibration              Test target\n     agent                    Logo of the scanning agent\n     blank                    Blank page inserted to maintain duplex\nprinting order\n     spine                    Image intended for the book spine\n     cover                    Image intended for the book cover\n     unnumbered               Publisher did not intend a page number\n     page                     Publisher's intended page number, even if omitted\n     supporting               Material not intended for display or printing\n\nThe content identifier \"calibration\" is followed by a string that\nidentifies a calibration test target.  The content identifier \"page\" is\nfollowed by a string that represents a publisher's originally assigned page\nnumber.  A Map field may contain just the string \"...\", which means that\nthe preceding and following Map fields are the beginning and the end of a\nconsecutive series of numbered pages.  The order of the Map fields is taken\nto be the order in which the images they describe are intended to be\nprinted or displayed.\n\n\nOpen questions:\n\n1.  The \"Document series\" field is probably not needed, because the\ninformation is contained in the document label field.\n\n2.  The \"File name generator\" field may not be needed, because all file\nnames are explicitly listsed in the map.\n\n3.  Original and printed sheet sizes are not recorded; they probably should\nbe, because some older TR's are printed in a 6 x 9-inch format from 8.5 x\n11-inch originals, and other TR's may be printed from pages produced on\ncut-sheet printers.\n\n4.  The original document date isn't captured.  Should it be?\n\n5.  Need systematic way to insert scanning agent logo on cover page.\n\n6.  The content identifiers \"scancontrol\" and \"spine\" could be handled as\n\"supporting\", with their intended use appearing as comments.\n\n7.  The conventional file names suggested here are a little different from\nthose currently in use, but they are completely arbitrary.  The form of the\nmap defined here eliminates any need for a program to gather information\nfrom a file name except when using consecutive image numbers to deduce\nconsecutive page numbers.\n\n8.  There is no support for bringing the reader's attention to the\ncopyright notice.  One suspects that some scheme will be needed for a\nbrowser to locate that notice and display it.  (Since TR's and TM's\ngenerally don't have copyright notices, this issue isn't particularly\n\n\nAcknowledgement:  This note is expanded from a set of ideas originally\ndeveloped at a Library 2000 group meeting on March 17, 1994.  Discussants:\nJack Eisan, Mitchell Charity, Ali Alavi, Sally Richter, Mary Anne Ladd,\nJeremy Hylton, Geoff Seyon, Eytan Adar, Greg Anderson, Jerry Saltzer.\nSince that time additional suggestions and ideas have come from Michael\nCook, Gillian Elcock, Yoav Yerushalmi, and Andrew Kass.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.8238672614,
        "format_confidence":0.9535301328
    },
    {
        "url":"https:\/\/github.com\/GroestlCoin\/sgminer",
        "text":"Groestlcoin GPU miner\nClone or download\nPull request Compare This branch is 8 commits ahead of sgminer-dev:master.\nFetching latest commit\u2026\nCannot retrieve the latest commit at this time.\nFailed to load latest commit information.\n\n\n\nThis is a multi-threaded multi-pool GPU miner with ATI GPU monitoring, (over)clocking and fanspeed support for scrypt-based cryptocurrency. It is based on cgminer by Con Kolivas (ckolivas), which is in turn based on cpuminer by Jeff Garzik (jgarzik).\n\n\ngit tree:\n\n\nirc: #sgminer and #sgminer-dev on freenode\n\nmailing lists:\n\nLicense: GPLv3. See COPYING for details.\n\n\nDocumentation is available in directory doc. It is organised by topics:\n\n  \u2022 API for the RPC API specification;\n  \u2022 for (largely incomplete) detailed information on all configuration options;\n  \u2022 for frequently asked questions;\n  \u2022 GPU for semi-obsolete information on GPU configuration options and mining SHA256d-based coins;\n  \u2022 for OpenCL kernel-related information, including development procedure;\n  \u2022 for how to find the right balance in GPU configuration to mine Scrypt-based coins efficiently;\n  \u2022 windows-build.txt for information on how to build on Windows.\n\nNote that most of the documentation is outdated or incomplete. If you want to contribute, fork this repository, update as needed, and submit a pull request.\n\n\n\n\n\n  \u2022 curses dev library - libncurses5-dev on Debian or libpdcurses on WIN32, for text user interface\n  \u2022 AMD ADL SDK - required for ATI GPU monitoring & clocking\n\nIf building from git:\n\n  \u2022 autoconf\n  \u2022 automake\n\nsgminer-specific configuration options:\n\n--disable-adl           Override detection and disable building with adl\n--without-curses        Do not compile support for curses TUI\n\nDebian Example\n\napt-get install libcurl4-openssl-dev pkg-config libtool libncurses5-dev\n\nAMD APP SDK and AMD ADL SDK must be downloaded from the amd websites.\n\n*nix build instructions\n\nIf needed, place include headers (*.h files) from ADL_SDK_*<VERSION>*.zip in sgminer\/ADL_SDK.\n\n\ngit submodule init\ngit submodule update\nautoreconf -i\nCFLAGS=\"-O2 -Wall -march=native -std=gnu99\" .\/configure <options>\n\nTo compile a version that can be used accross machines, remove -march=native.\n\nTo compile a debug version, replace -O2 with -ggdb.\n\nDepending on your environment, replace -std=gnu99 with -std=c99.\n\nSystemwide installation is optional. You may run sgminer from the build directory directly, or make install if you wish to install sgminer to a system location or a location you specified with --prefix.\n\nWindows build instructions\n\nSee doc\/windows-build.txt for MinGW compilation and cross-compiation, doc\/cygwin-build.txt for building using Cygwin, or use the provided winbuild Microsoft Visual Studio project (tested on MSVS2010), with instructions in winbuild\/README.txt.\n\nBasic Usage\n\nWARNING: documentation below this point has not been updated since the fork.\n\nAfter saving configuration from the menu, you do not need to give sgminer any arguments and it will load your configuration.\n\nAny configuration file may also contain a single\n\n\"include\" : \"filename\"\n\nto recursively include another configuration file.\n\nWriting the configuration will save all settings from all files in the output.\n\nSingle pool:\n\nsgminer -o http:\/\/pool:port -u username -p password\n\nMultiple pools:\n\nsgminer -o http:\/\/pool1:port -u pool1username -p pool1password -o http:\/\/pool2:port -u pool2usernmae -p pool2password\n\nSingle pool with a standard http proxy, regular desktop:\n\nsgminer -o \"http:proxy:port|http:\/\/pool:port\" -u username -p password\n\nSingle pool with a socks5 proxy, regular desktop:\n\nsgminer -o \"socks5:proxy:port|http:\/\/pool:port\" -u username -p password\n\nSingle pool with stratum protocol support:\n\nsgminer -o stratum+tcp:\/\/pool:port -u username -p password\n\nThe list of proxy types are: http: standard http 1.1 proxy http0: http 1.0 proxy socks4: socks4 proxy socks5: socks5 proxy socks4a: socks4a proxy socks5h: socks5 proxy using a hostname\n\nIf you compile sgminer with a version of CURL before 7.19.4 then some of the above will not be available. All are available since CURL version 7.19.4.\n\nIf you specify the --socks-proxy option to sgminer, it will only be applied to all pools that don't specify their own proxy setting like above.\n\nFor more advanced usage , run sgminer --help.\n\nSee doc\/GPU for more information regarding GPU mining and doc\/SCRYPT for more information regarding Scrypt mining.\n\nRuntime usage\n\nThe following options are available while running with a single keypress:\n\n[P]ool management [G]PU management [S]ettings [D]isplay options [Q]uit\n\nP gives you:\n\nCurrent pool management strategy: Failover [F]ailover only disabled [A]dd pool [R]emove pool [D]isable pool [E]nable pool [C]hange management strategy [S]witch pool [I]nformation\n\nS gives you:\n\n[Q]ueue: 1 [S]cantime: 60 [E]xpiry: 120 [W]rite config file [C]gminer restart\n\nD gives you:\n\n[N]ormal [C]lear [S]ilent mode (disable all output) [D]ebug:off [P]er-device:off [Q]uiet:off [V]erbose:off [R]PC debug:off [W]orkTime details:off co[M]pact: off [L]og interval:5\n\nQ quits the application.\n\nG gives you something like:\n\nGPU 0: [124.2 \/ 191.3 Mh\/s] [A:77 R:33 HW:0 U:1.73\/m WU 1.73\/m] Temp: 67.0 C Fan Speed: 35% (2500 RPM) Engine Clock: 960 MHz Memory Clock: 480 Mhz Vddc: 1.200 V Activity: 93% Powertune: 0% Last initialised: [2011-09-06 12:03:56] Thread 0: 62.4 Mh\/s Enabled ALIVE Thread 1: 60.2 Mh\/s Enabled ALIVE\n\n[E]nable [D]isable [R]estart GPU [C]hange settings Or press any other key to continue\n\nThe running log shows output like this:\n\n[2012-10-12 18:02:20] Accepted f0c05469 Diff 1\/1 GPU 0 pool 1 [2012-10-12 18:02:22] Accepted 218ac982 Diff 7\/1 GPU 1 pool 1 [2012-10-12 18:02:23] Accepted d8300795 Diff 1\/1 GPU 3 pool 1 [2012-10-12 18:02:24] Accepted 122c1ff1 Diff 14\/1 GPU 1 pool 1\n\nThe 8 byte hex value are the 2nd 8 bytes of the share being submitted to the pool. The 2 diff values are the actual difficulty target that share reached followed by the difficulty target the pool is currently asking for.\n\nThe output line shows the following: (5s):1713.6 (avg):1707.8 Mh\/s | A:729 R:8 HW:0 WU:22.53\/m\n\nEach column is as follows: 5s: A 5 second exponentially decaying average hash rate avg: An all time average hash rate A: The total difficulty of Accepted shares R: The total difficulty of Rejected shares HW: The number of HardWare errors WU: The Work Utility defined as the number of diff1 shares work \/ minute (accepted or rejected).\n\nGPU 1: 73.5C 2551RPM | 427.3\/443.0Mh\/s | A:8 R:0 HW:0 WU:4.39\/m\n\nEach column is as follows: Temperature (if supported) Fanspeed (if supported) A 5 second exponentially decaying average hash rate An all time average hash rate The total difficulty of accepted shares The total difficulty of rejected shares The number of hardware erorrs The work utility defined as the number of diff1 shares work \/ minute\n\nThe sgminer status line shows: ST: 1 SS: 0 NB: 1 LW: 8 GF: 1 RF: 1\n\nST is STaged work items (ready to use). SS is Stale Shares discarded (detected and not submitted so don't count as rejects) NB is New Blocks detected on the network LW is Locally generated Work items GF is Getwork Fail Occasions (server slow to provide work) RF is Remote Fail occasions (server slow to accept work)\n\nThe block display shows: Block: 0074c5e482e34a506d2a051a... Started: [17:17:22] Best share: 2.71K\n\nThis shows a short stretch of the current block, when the new block started, and the all time best difficulty share you've found since starting sgminer this time.\n\n\nFailover strategies\n\nA number of different strategies for dealing with multipool setups are available. Each has their advantages and disadvantages so multiple strategies are available by user choice, as per the following list:\n\n\nThe default strategy is failover. This means that if you input a number of pools, it will try to use them as a priority list, moving away from the 1st to the 2nd, 2nd to 3rd and so on. If any of the earlier pools recover, it will move back to the higher priority ones.\n\nRound robin\n\nThis strategy only moves from one pool to the next when the current one falls idle and makes no attempt to move otherwise.\n\n\nThis strategy moves at user-defined intervals from one active pool to the next, skipping pools that are idle.\n\nLoad balance\n\nThis strategy sends work to all the pools on a quota basis. By default, all pools are allocated equal quotas unless specified with --quota. This apportioning of work is based on work handed out, not shares returned so is independent of difficulty targets or rejected shares. While a pool is disabled or dead, its quota is dropped until it is re-enabled. Quotas are forward looking, so if the quota is changed on the fly, it only affects future work. If all pools are set to zero quota or all pools with quota are dead, it will fall back to a failover mode. See quota below for more information.\n\nThe failover-only flag has special meaning in combination with load-balance mode and it will distribute quota back to priority pool 0 from any pools that are unable to provide work for any reason so as to maintain quota ratios between the rest of the pools.\n\n\nThis strategy monitors the amount of difficulty 1 shares solved for each pool and uses it to try to end up doing the same amount of work for all pools.\n\n\nThe load-balance multipool strategy works off a quota based scheduler. The quotas handed out by default are equal, but the user is allowed to specify any arbitrary ratio of quotas. For example, if all the quota values add up to 100, each quota value will be a percentage, but if 2 pools are specified and pool0 is given a quota of 1 and pool1 is given a quota of 9, pool0 will get 10% of the work and pool1 will get 90%. Quotas can be changed on the fly by the API, and do not act retrospectively. Setting a quota to zero will effectively disable that pool unless all other pools are disabled or dead. In that scenario, load-balance falls back to regular failover priority-based strategy. While a pool is dead, it loses it... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.8997871876,
        "format_confidence":0.9674459696
    },
    {
        "url":"https:\/\/www.sqlite.org\/c3ref\/backup.html",
        "text":"Small. Fast. Reliable.\nChoose any three.\n\nSQLite C Interface\n\nOnline Backup Object\n\ntypedef struct sqlite3_backup sqlite3_backup;\n\nThe sqlite3_backup object records state information about an ongoing online backup operation. The sqlite3_backup object is created by a call to sqlite3_backup_init() and is destroyed by a call to sqlite3_backup_finish().\n\nSee Also: Using the SQLite Online Backup API\n\nSee also lists of Objects, Constants, and Functions.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9821352959,
        "format_confidence":0.9778758287
    },
    {
        "url":"https:\/\/docs.rstudio.com\/shinyapps.io\/applications.html",
        "text":"Chapter 3 Applications hosts each app on its own virtualized server, called an instance. Each instance runs an identical copy of the code, packages, and data that you deployed; collectively, this is called an image. Uploaded bundles are limited to a maximum of 6000 files as well as a maximum size of 1 GB for the Free and Starter plans, and up to 5 GB for the Basic, Standard and Professional plans. For the latter plans, note that the rsconnect package has a default bundle size limit of 3 GB, independent of your plan limit. If you plan to deploy application bundles larger than 3 GB in size, you can override the rsconnect default by setting this option first:\n\noptions(rsconnect.max.bundle.size=...) where the value is in bytes. Do not set the maximum size larger than the limit for your plan, or application deployments will fail.\n\nWhen you deploy an app, creates a new image with the updated code and packages, and starts one or more instances with the new image. If the app was previously deployed, shuts down and destroys the old instances. This design comes with some important considerations:\n\n  1. Data written by an application to the local filesystem of an instance will be lost when you re-deploy the application. Additionally, the distributed nature of the platform means that instances may be shut down and re-created at any time for maintenance, or to recover from server failures.\n\n  2. It is possible to have more than one instance of an application. This means that multiple instances of an application do not share a local filesystem. A file written to one instance will not be available to any other instance. limits the availability of system resources that an instance can consume by its type. The table below outlines the various instance types and how much memory is allowed for each. By default, deploys all applications on \u2018large\u2019 instances, which are allowed to use 1024 MB of memory.\n\nInstance Memory\nsmall 256 MB\nmedium 512 MB\nlarge (default) 1024 MB\nxlarge 2048 MB\nxxlarge 4096 MB\nxxxlarge 8192 MB\n\nNote: Instance types and limits may change in the future. does not make any guarantees regarding the number of CPU cores, or the speed of the CPUs that are allocated to the deployed applications. If your applications are particularly computationally intensive, please contact us at to determine whether Shiny Server or Shiny Server Pro installed on your own infrastructure would better suit your needs.\n\n3.1 Configuring your application\n\nThere are two ways that you can configure your application settings. The easiest and most comprehensive is to log into the dashboard and select the application you wish to configure in the Application View. From there, click the Settings menu bar choice to access the various options for application configuration.\n\nThe second method uses the rsconnect::configureApp() call to modify a subset of settings. For example, this is how you can change the instance size used by an application:\n\nrsconnect::configureApp(\"APPNAME\", size=\"small\")\n\n3.1.1 Custom Domains\n\nApplications deployed on are accessible by loading a URL of the form: https:\/\/<account-name><application-name>\/. Organizations that would like to have greater control over the URLs that their applications are served on can subscribe to the Professional plan and host the application on domains that belong to them.\n\nTo enable this feature, which is only available with the Professional plan, you will need to follow these steps:\n\n  \u2022 Decide on domain(s) or subdomain(s) that you would want to host your applications on. (Example: or\n  \u2022 Ask your IT administrator to setup a CNAME from that domain\/subdomain to your account domain. For example, if your account name is acme and your domain you would like to setup is, then you must create a CNAME from to Steps to accomplish this can vary depending on domain registrar or DNS provider, so we recommend that you consult your provider\u2019s documentation for exact instructions on completing this step.\n  \u2022 Once the domain record has been created, log into the dashboard and navigate to Account -> Domains. From here, you can add the domain or subdomain from above and then click Add Domain.\n  \u2022 Now you are ready to add custom URLs to any of your existing applications. Within the dashboard, find your application in the Applications tab and click on the URLs menu bar choice. You will notice that there is already a single URL, which is the one that is created by default, and cannot be removed.\n  \u2022 Click on Add URL. You can now select from the list of domains you have entered, and specify the path to the application. The URL field below will show you what the final URL would look like.\n\nNote that a single application can be hosted on multiple domains and using different paths; all paths are case-sensitive. does not currently support secure URLs for custom domains. The URLs you define in the application settings are accessible only via the http protocol, not the https protocol.\n\n3.1.2 Embedding applications\n\nEvery application you deploy will have a unique URL, served over a secure socket (SSL) connection and accessible from a web browser in the format:\n\n\nYou can embed your application within other pages by using an iframe. Here is an example of an iframe for a fictitious application. Note that you may want to size the frame differently based on your application's display requirements.\n\n<iframe id=\"example1\" src=\"https:\/\/<accountname><applicationame>\"\nstyle=\"border: non; width: 100%; height: 500px\"\n\nBe aware that since a shiny application is always served over SSL, your application cannot load HTML UI elements that are served over plain http; this is a security feature of all browsers.\n\n3.1.3 Application Instances\n\nAn application instance is a virtualized server that serves an application. To serve your application, will start at least one application instance. You can start or stop your instance by visiting the Application page within the dashboard.\n\nWith the Basic plan and above, you can tune your application to use multiple application instances, and control when additional instances are added to meet your performance requirements. We describe how to tune your application in the Application Performance Tuning section below.\n\nThe maximum number of instances you can add is governed by your subscription plan:\n\nPlan Instances\nFree 1\nStarter 1\nBasic 3\nStandard 5\nProfessional 10\n\n3.1.4 Workers\n\nA worker is a special type of R process that an Application Instance runs to service requests to an application. Each Application Instance can run multiple workers, and each worker process is capable of servicing multiple end users, depending on the configuration and performance requirements of the application. If there are no processes available to handle a new request, the Application Instance will start a new worker process. With the Basic plan and above, you can configure how many workers you would like to use and when they should be added.\n\n3.2 Application performance tuning\n\nWith the Basic plan and above, you can configure your application to fine-tune its performance. You can tune the number of workers that run in each Application Instance, the number of Application Instances that you wish to run, how many Instances should start by default, how long to wait before your applications are considered idle and can be shut down, and many other parameters.\n\nTo learn more about instances, workers, and other details of the architecture, please read Scaling and Performance Tuning.\n\n3.3 Application life cycle\n\nApplications start out in a running state when they are first deployed. An application will remain in that state as long as it is handling requests from end users. Once all end user sessions disconnect (or the connections time out), an application will be put to sleep. A sleeping application will return to a running state when it is requested again.\n\nState Description\nSleeping Currently not receiving any traffic, but can be brought online to service new requests\nRunning Currently running with at least one instance\nArchived Cannot run unless it is unarchived\n\n3.3.1 Application restarts\n\nYou can force a restart of your application by clicking on the Restart button in the Application view. Restarting an application in this manner tells to force a redeployment of the application that was last uploaded to the service. This can be helpful in circumstances where you want to force a rebuild of the packages again.\n\nAlternately, you can force this restart programmatically by making the following call:\n\nrsconnect::deployApp(upload = FALSE)\n\n3.3.2 Active hours\n\nActive hours are defined as the number of hours that any Application Instance was up and able to serve your application. Application Instances will be put into a sleep state when they have been idle for the Instance Idle Timeout value (default is 15 minutes). Idle time counts toward your account's active hours, but sleep time does not.\n\nFor example, an application that is accessed for 15 minutes by a single user, will use 30 minutes or 1\/2 an active hour.\n\n15 minutes of usage + 15 minutes for the idle timeout = 30 minutes\n\nIf the application was configured to use two application instances and is used by multiple users for the same 15 minutes, the active hours will be 60 minutes, or 1 active hour:\n\n(15 minutes of usage x 2 Application instances) + (15 minute idle timeout x 2 Application Instances) = 30 + 30 = 60 minutes\n\n3.4 Archiving your application\n\nYou can remove an app on from the web with the terminateApp() command. To use it, run:\n\n\nAlternately, you can click on the Archive button in the Application view.\n\nArchived applications will not be served on the web. If you restart the application, or redeploy it, it will be moved out of the Archived state and will be available. Note that if you are subscribed to the Free plan, you have a limit to the number of applications that can be active at once. Archiving unused applications will allow you to deploy new applications.\n\n3.5 Downloa... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.8819244504,
        "format_confidence":0.9803569913
    },
    {
        "url":"http:\/\/www.lispworks.com\/documentation\/lw50\/CAPUG-U\/html\/capiuser-u-54.htm",
        "text":"5.7.2 Selections\n\nAll choices have a selection. This is a state representing the items currently selected. The selection is represented as a vector of offsets into the list of the choice's items, unless it is a single-selection choice, in which case it is just represented as an offset.\n\nThe initial selection is controlled with the initarg :selection . The accessor choice-selection is provided.\n\nGenerally, it is easier to refer to the selection in terms of the items selected, rather than by offsets, so the CAPI provides the notion of a selected item and the selected items . The first of these is the selected item in a single-selection choice. The second is a list of the selected items in any choice.\n\nThe accessors choice-selected-item and choice-selected-items and the initargs :selected-item and :selected-items provide access to these conceptual slots.\n\nLispWorks CAPI User Guide (Unix version) - 14 Jun 2006",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9555914402,
        "format_confidence":0.9868260026
    },
    {
        "url":"http:\/\/www.cpan.org\/authors\/id\/K\/KJ\/KJETILK\/RDF-Generator-Void-0.04.readme",
        "text":"NAME RDF::Generator::Void - Generate VoID descriptions based on data in an RDF model VERSION Version 0.04 Note that this is a beta release. It has the core functionality in place to create a basic VoID description and what's there should be working well. Nevertheless significant changes in this module may be coming up really soon. SYNOPSIS use RDF::Generator::Void; use RDF::Trine::Model; my $mymodel = RDF::Trine::Model->temporary_model; [add some data to $mymodel here] my $generator = RDF::Generator::Void->new(inmodel => $mymodel); $generator->urispace(''); $generator->add_endpoints(''); my $voidmodel = $generator->generate; DESCRIPTION This module takes a RDF::Trine::Model object as input to the constructor, and based on the data in that model as well as data supplied by the user, it creates a new model with a VoID description of the data in the model. For a description of VoID, see . METHODS new(inmodel => $mymodel, dataset_uri => URI->new($dataset_uri)); The constructor. It can be called with two parameters, namely, \"inmodel\" which is a model we want to describe and \"dataset_uri\", which is the URI we want to use for the description. Users should make sure it is possible to get this with HTTP. If this is not possible, you may leave this field empty so that a simple URN can be created for you as a default. \"inmodel\" Read-only accessor for the model used in description creation. \"dataset_uri\" Read-only accessor for the URI to the dataset. Property Attributes The below attributes concern some essential properties in the VoID vocabulary. They are mostly arrays, and can be manipulated using array methods. Methods starting with \"all_\" will return an array of unique values. Methods starting with \"add_\" takes a list of values to add, and those starting with \"has_no_\" return a boolean value, false if the array is empty. \"vocabulary\", \"all_vocabularies\", \"add_vocabularies\", \"has_no_vocabularies\" Methods to manipulate a list of vocabularies used in the dataset. The values should be a string that represents the URI of a vocabulary. \"endpoint\", \"all_endpoints\", \"add_endpoints\", \"has_no_endpoints\" Methods to manipulate a list of SPARQL endpoints that can be used to query the dataset. The values should be a string that represents the URI of a SPARQL endpoint. \"title\", \"all_titles\", \"add_titles\", \"has_no_titles\" Methods to manipulate the titles of the datasets. The values should be RDF::Trine::Node::Literal objects, and should be set with language. Typically, you would have a value per language. \"license\", \"all_licenses\", \"add_licenses\", \"has_no_licenses\" Methods to manipulate a list of licenses that regulates the use of the dataset. The values should be a string that represents the URI of a license. \"urispace\", \"has_urispace\" This method is used to set the URI prefix string that will match the entities in your dataset. The computation of the number of entities depends on this being set. \"has_urispace\" can be used to check if it is set. Running this stuff \"stats\", \"clear_stats\", \"has_stats\" Method to compute a statistical summary for the data in the dataset, such as the number of entities, predicates, etc. \"clear_stats\" will clear the statistics and \"has_stats\" will return true if exists. generate( [ $model ] ) Returns the VoID as an RDF::Trine::Model. You may pass a model with statements as argument to this method. This model may then contain arbitrary RDF that will be added to the RDF model. If you do not send a model, one will be created for you. AUTHORS Kjetil Kjernsmo \"\" Toby Inkster \"\" Tope Omitola, \"\" TODO * Allow arbitrary RDF to be added to the VoID. * Larger test dataset for more extensive tests. * URI regexps support. * Partitioning based on properties and classes. * Technical features (esp. serializations). * Example resources and root resources. * Data dumps. * Subject classification. * Method to disable heuristics. * More heuristics. * Linkset descriptions. * Set URI space on partitions. * Conditional updates based on model ETags. * Save the description to files? BUGS Please report any bugs you find to SUPPORT You can find documentation for this module with the perldoc command. perldoc RDF::Generator::Void The Perl and RDF community website is at where you can also find a mailing list to direct questions to. You can also look for information at: * AnnoCPAN: Annotated CPAN documentation * CPAN Ratings * MetaCPAN ACKNOWLEDGEMENTS LICENSE AND COPYRIGHT Copyright 2012 Tope Omitola, Kjetil Kjernsmo, Toby Inkster. This program is free software; you can redistribute it and\/or modify it under the terms of either: the GNU General Public License as published by the Free Software Foundation; or the Artistic License. See for more information.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9882752895,
        "format_confidence":0.9538083673
    },
    {
        "url":"https:\/\/www.fz-juelich.de\/ias\/jsc\/EN\/Expertise\/Support\/Software\/Nassi\/_node.html",
        "text":"Navigation and service\n\n\nThe nassi program developed at the J\u00fclich Supercomputing Centre (JSC) at the Forschungszentrum J\u00fclich is a tool to generate Nassi-Shneiderman diagrams under Unix\/X11. nassi is designed primarily for the creation of documentation, but may as well be used for source code analysis\/browsing.\n\nC and PASCAL programs and pseudo code that follows the conventions of either of these languages can be transformed into a graphical representation of the program flow. For representation and postprocessing nassi provides a convenient interface with which single diagrams can be selected, drawn on the screen, exported in several output formats and printed.\n\nA graphics editor allows layout changes of the whole diagram or of single statements or control structures. Statements and whole structures can be hidden or shifted as a block to a separate diagram. Such changes to the layout and structure of diagrams can be inserted into the source code via special comments and are then available for further treatment of the source code.\n\nFor output purposes, nassi also provides the option of generating source data of the Tgif and Xfig graphics editors in addition to screen output, Encapsulated PostScript graphics and printable PostScript files. This makes it possible to also change and extend diagrams far beyond the functionality of the built-in graphics editor. For the rapid generation of diagrams nassi provides a batch option which generates diagrams in the desired output format.\n\nFor details see the documentation or take a look at some screen shots in the tutorial.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.8772737384,
        "format_confidence":0.7546297908
    },
    {
        "url":"http:\/\/www.technology-architect.com\/2012\/08\/scenarios-of-ms-dynamics-crm-2011.html",
        "text":"Friday, August 3, 2012\n\nScenarios of MS Dynamics CRM 2011 Online usage in Windows Azure hosted applications and Silverlight applications with cross domain access\n\nToday we will briefly go through the usage of Microsoft Dynamics CRM 2011 Online data in external applications. The most interesting business case there is the usage of Windows Azure cloud hosting for our application. Windows Azure has some differences comparing to common dedicated server environment. That is why it requires another approaches to applications development and integration.\n\nSo, lets say that we have a web site which hosts fast and beautiful Silverlight application. And we would like to show some data within the application. Lets also say that we are a small company who would like to use all benefits of Windows Azure hosting environment.\n\nScenario: direct access from Silverlight to Dynamics CRM 2011 Online\n\nThe standard approach to our case is to host an application as a Windows Azure web site. The web site will host a Silverlight application. The Silverlight application will access a MS CRM 2011 Online and will grab a data. There is the only one weakness in this plan: CRM Online didn\u2019t publish cross domain policy files like crossdomain.xml and clientaccesspolicy.xml. There is no tools to somehow manage this or upload the files as resources. This does mean that you are not able to connect to CRM Online using Silverlight instead you host it within the CRM. But such scenario requires that all your visitors were registered as a CRM users what is not possible for internet-facing application. Lets work around this problem.\n\nScenario for Direct Silverlight to Dynamics CRM 2011 Online access\n\nScenario: access to CRM 2011 Online Organisation service from a web server\n\nThe approach with accessing CRM from a web server component requires some extra work. First, we need provide a WCF RIA service for the Silverlight application. This service will wrap a call of CRM Organisation service. Additionally it could be used to increase the security of the application and restrict an API access.\n\nAccess to Dynamics CRM 2011 Online from a web application\n\nFor that scenario it is required to have a Windows Identity Foundation installed on a server. As expected, there is no WIF installed in a cloud. So you need to add a reference on Microsoft.IdentityModel.dll (C:\\Program Files\\Reference Assemblies\\Microsoft\\Windows Identity Foundation\\v3.5) in a project with parameter CopyLocal = true. The code for interaction with CRM uses proxy classes from SDK and entities classes generated by CrmSvcUtil.exe. Class DeviceIdManager also available in SDK samples (sdk\\samplecode\\cs\\helpercode):\n\nstring userName = \"<windows live>\";\nstring password = \"<live password>\";\n\nClientCredentials credentials = new ClientCredentials();\ncredentials.UserName.UserName = userName;\ncredentials.UserName.Password = password;\n\nUri organizationUri = new Uri(@\"\");\nUri homeRealmUri = null;\nUri issuerUri = new Uri(@\"\");\n\nstring deviceName, devicePassword;\n\nDeviceIdManager.PersistToFile = true;\nClientCredentials cred = DeviceIdManager.LoadDeviceCredentials(issuerUri);\ndeviceName = cred.UserName.UserName;\ndevicePassword = cred.UserName.Password;\n\n\nDeviceIdManager.PersistToFile = false;\ndeviceName = \"cvrmd6i7y6fozei5renofkmt\";\ndevicePassword = \"-r~-~pe`3ecWZ+ExW3Kb%F#Z\";\n\n\nClientCredentials deviceCredentials = DeviceIdManager.LoadOrRegisterDevice(issuerUri, deviceName, devicePassword);\nOrganizationServiceProxy proxy = new OrganizationServiceProxy(organizationUri, homeRealmUri, credentials, deviceCredentials);\n\nXrm.XrmServiceContext context = new Xrm.XrmServiceContext(proxy);\ntechart_growerapplication gapp = new techart_growerapplication();\ngapp.techart_firstname = app.FirstName;\ngapp.techart_FamilyName = app.LastName;\ngapp.techart_SecondName = app.SecondName;\ngapp.EmailAddress = app.Email;\n\n\n\nThe important notes for this code:\n\n  1. The exact URI for issuer in your case can be found in WSDL for Organisation service under\n\n    or you can use WsdlTokenManger class demonstrated in SDK (sdk\\samplecode\\cs\\wsdlbasedproxies\\online).\n\n  2. The call of EnableProxyTypes is mandatory. You will receive an exception without it:\n    The formatter threw an exception while trying to deserialize the message: There was an error while trying to deserialize parameter The InnerException message was 'Error in line 1 position 8997. Element '' contains data from a type that maps to the name 'Xrm:techart_application'. The deserializer has no knowledge of any type that maps to this name. Consider changing the implementation of the ResolveName method on your DataContractResolver to return a non-null value for name 'techart_application' and namespace 'Xrm'.'.  Please see InnerException for more details.\n\n  3. Set the DeviceIdManager.PersistToFile = false and device name and password is mandatory in order to make it working on Windows Azure. Device ID will be registered in Windows Live. Windows Azure do not support storing the user or machine level files, that is why we should restrict the storing of the Device ID. As you can see, it is only required for release environment on Windows Azure.\n\nSo, this scenario will allows you to implement the required behaviour.\n\nScenario: using Windows Azure Service Bus and ACS to interact with Dynamics CRM 2011 Online\n\nThis scenario allows you to use all benefits of the Microsoft cloud platform. Dynamics CRM 2011 Online has an internal support for integration using Azure Service Bus. Commonly you can download a certificate from CRM and use it to maintain a trusted relationships with another application through Service Bus. Details regarding the configuration you can find in training materials by Microsoft.\u00a0\u00a0\u00a0\n\nWindows Azure Service Bus trusted relationships\n\nSo, this is in general it. The main issues will rise as always during the implementation of the solutions. But currently Azure provides spectacular tools which allows you to deliver a solution as quick as possible and do not worry about the hosting environment maintenance and support. Azure Service Bus could be expensive for small company, but it is a good tool for middle size organisations. I must admit that the current implementation of the Service Bus is far from enterprise level product and you should consider other available products such as MS BizTalk On-premises, Oracle Service Bus or Tibco EAI. But I expect that in two years it will become a real pearl for integration projects.\n\nNo comments:\n\nPost a Comment",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9825823307,
        "format_confidence":0.8015985489
    },
    {
        "url":"http:\/\/cloudsecurity.codeplex.com\/wikipage?title=Tasks",
        "text":"Task Lists\n\n- J.D. Meier, Prashant Bansode, Paul Enfield.\n\nTask lists are a compilation of expected activities of customers with this technology. We attempt to determine the areas that will likely need the most guidance and prioritize them here.\n\nLast edited Aug 18, 2009 at 6:36 PM by paulenfield, version 3\n\n\nNo comments yet.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9252284765,
        "format_confidence":0.6555643678
    },
    {
        "url":"https:\/\/www.adobe.com\/devnet\/svg1\/quickstart-1\/articles\/creating_non-rectangular_windows.html",
        "text":"by Joe Ward\n\nJoe Ward\n\n\n10 June 2010\n\nRectangular windows are fine, and appropriate for most applications. However, just because they are the easiest to draw doesn't mean that all your windows have to be curve impoverished. The !Square sample application creates a window based on the ellipse, rather than the rectangle. The window uses vector graphics for most of its chrome, so there are no issues with bitmap scaling to limit the window size or aspect ratio.\n\nThe !Square sample application, shown in Figure 1, illustrates how to extend the NativeWindow class to create windows with alternative visuals and behavior using the AIR APIs and the rich Flash graphics capabilities.\n\nNote: This is a sample application provided, as is, for instructional purposes.\n\nThis sample application includes the following files and classes:\n\n  \u2022 NotSquare-app.xml: The AIR application descriptor file\n  \u2022 A stub class that creates the main application window and closes\n  \u2022 !Square.fla: Flash document for use with Adobe Flash CS3 Professional.\n  \u2022 Extends the NativeWindow class\n  \u2022 Extends the Sprite class to create a content container that clips anything outside the window client area\n  \u2022 Implements the gripper controls for resizing the window\n  \u2022 Implements a button for minimizing or restoring the window\n  \u2022 Implements a button for maximizing the window\n  \u2022 Implements a button for closing the window\n  \u2022 Loads the images used for the dock and system tray icons\n  \u2022 A class providing some sample content for the window\n  \u2022 A visual object\n  \u2022 Simulates spring forces\n  \u2022 SourceViewer.js: Implements a source code browser using an HTML window\n  \u2022 Sample AIR icon files and bitmap graphics used by the window chrome\n\nTesting the application\n\nLaunch the !Square application (!Square.air). Resize the window using any of the eight grippers along the window drag bar. Move the window using the drag bar. Drag the white disk to the edge of the window to observe that any part of the disk that is outside the window client area is properly clipped.\n\nUnderstanding the code\n\nThe !Square application uses several graphics functions not specific to AIR. For more information about these functions, see the ActionScript 3.0 Language Reference.\n\nExtending the NativeWindow class\n\nThe RoundWindow class extends the AIR NativeWindow class to specialize the constructor and to define the methods and properties to draw its window chrome. Because you cannot use a custom class for the initial application window, the !Square example uses the initial window primarily to launch an instance of the RoundWindow class to serve as the main application window. The initial window created by AIR is never made visible and is closed once initialization is complete.\n\nThe constructor of the RoundWindow class creates its own NativeWindowInitOptions object and uses it to create the underlying native window. The constructor then creates the window chrome elements and activates the window to make it visible and active.\n\npublic function RoundWindow(title:String=\"\"){ var initOptions:NativeWindowInitOptions = new NativeWindowInitOptions(); initOptions.systemChrome = NativeWindowSystemChrome.NONE; initOptions.transparent = true; super(initOptions); this.minSize = new Point(350,350); bounds = new Rectangle(0,0,viewWidth,viewHeight); this.title = title; stage.align = StageAlign.TOP_LEFT; stage.scaleMode = StageScaleMode.NO_SCALE; stage.addChild(clippedStage); \/\/Put buttons on conventional side of window if(isWindows()){ closeButton = new CloseButton(287); minRestoreButton = new MinRestoreButton(283.5); maxButton = new MaxButton(280); } else { closeButton = new CloseButton(252); minRestoreButton = new MinRestoreButton(255.5); maxButton = new MaxButton(259); } addWindowDressing(); addEventListener(NativeWindowBoundsEvent.RESIZE,onBoundsChange); draw(viewWidth,viewHeight); activate(); }\n\nDrawing elliptical borders\n\nThe border of the window is drawn using standard Flash drawing functions provided by the Graphics class, which can be accessed through the graphics property of any display object. The border is composed of three rings. When you draw overlapping figures between the calls to beginFill() and endFill(), only the difference between the figures, in this case ellipses, will be filled. The following function draws the three rings by drawing two ellipses for each ring:\n\nwith({ clear(); beginFill(bevelColor,1); drawEllipse(0,0,viewWidth,viewHeight); drawEllipse(4,4,viewWidth-8,viewHeight-8); endFill(); beginFill(borderColor,1); drawEllipse(4,4,viewWidth-8,viewHeight-8); drawEllipse(16,16,viewWidth-32,viewHeight-32); endFill(); beginFill(bevelColor,1); drawEllipse(16,16,viewWidth-32,viewHeight-32); drawEllipse(20,20,viewWidth-40,viewHeight-40); endFill(); }\n\nDrawing a section of an ellipse\n\nBecause the Graphics class does not have a function for drawing just a piece of an elliptical arc, drawing the resizing gripper onto the border is actually more challenging than drawing the border itself. You could use the curveTo() method, but calculating the proper control point locations to match the elliptical border has its own mathematical challenges. For this task, !Square uses a simpler polyline technique based on the parametric equation of the ellipse. A point on the border is calculated based on the height and width and angle from the midpoint of the ellipse. Then the next point is calculated by increasing the angle a small amount and a line is drawn between them. This process is repeated until the desired arc is drawn.\n\nFigure 2 shows the formula that can be used to calculate a point along the ellipse, given the angle, and the width and the height of the ellipse.\n\nThe grippers are arranged around the border at preset angles, so the angle is known. The width and height of the ellipse is the width and height of the window for the outer edge of the window border (viewWidth and viewHeight), and the width and height minus the thickness of the border for the inner edge of the window border.\n\nThe drawing routine uses the following formula to calculate the x and y coordinates of the four corners of the gripper, points A, B, C, and D:\n\nvar A:Point = new Point(); A.x = Math.cos(startAngle) * viewWidth\/2; A.y = Math.sin(startAngle) * viewHeight\/2; var B:Point = new Point(); B.x = Math.cos(startAngle) * (viewWidth-40)\/2; B.y = Math.sin(startAngle) * (viewHeight-40)\/2; var C:Point = new Point(); C.x = Math.cos(stopAngle) * (viewWidth-40)\/2; C.y = Math.sin(stopAngle) * (viewHeight-40)\/2; var D:Point = new Point(); D.x = Math.cos(stopAngle) * viewWidth\/2; D.y = Math.sin(stopAngle) * viewHeight\/2;;\n\nThe start and stop angles are calculated by adding and subtracting half the angular width of the gripper from the preset gripper angle. Defining the length with angles rather than a distance makes the math a bit easier and also provides a more pleasing effect when the window is resized since the gripper length stays proportional to the border diameter.\n\nvar startAngle:Number = (angleRadians - spreadRadians); var stopAngle:Number = (angleRadians + spreadRadians);\n\nThe routine next draws the shape of the gripper between the four points (see Figure 3).\n\nFirst, a straight line is drawn from A to B:\n\nmoveTo(A.x,A.y); lineTo(B.x,B.y);\n\nNext, a series of line segments is drawn between B and C. If enough segments are used, then the visual effect is indistinguishable from an actual curve. !Square uses ten segments, which seems sufficient.\n\nfor(var i:int = 1; i < 10; i++){ lineTo(Math.cos(startAngle + i * incAngle) * (viewWidth-40)\/2, Math.sin(startAngle + i * incAngle) * (viewHeight-40)\/2); }\n\nAnother straight line is drawn from C to D and the shape is closed by drawing a polyline segment from D back to A. The shape is started with the beginBitmapFill() method so when endFill() is called the area defined by the drawing commands is filled with a bitmap texture.\n\nClipping the client area\n\nThe RoundWindow class uses a sprite with a clipping mask applied as the container for its contents. Any content objects that are drawn outside the border of the window are clipped. Window chrome elements are added directly to the stage so that they are not clipped.\n\nClipping can be implemented in Flash by setting the mask property of a Sprite object with another Sprite object. The masking sprite is not drawn, but only the parts in the first sprite that fall under the shape defined by the mask's graphics commands are visible.\n\nIn !Square, the clipped sprite is defined by the ClippedStage class. This class creates a clipping mask using the familiar commands for drawing an ellipse based on the width and height of the window. Any part of an object added as a child of the ClippedStage object that falls outside the ellipse are clipped.\n\nprivate function setClipMask(bounds:Rectangle):void{;,1);,0,bounds.width,bounds.height);; }\n\nThe class also listens for resize events from the parent window and responds by redrawing the clipping mask based on the new window dimensions.\n\nprivate function onResize(event:NativeWindowBoundsEvent):void{ setClipMask(event.afterBounds); }\n\nThis type of clipping is not limited to simple shapes, so the technique can be used for any window that has areas which should be masked.\n\nResizing an elliptical window\n\nTo resize the window, the border and grippers must be redrawn based on the new width and height of the window. The resize event object includes an afterBounds property that reports the new dimensions. The width and height from this property are passed to the window draw() method.\n\nprivate function onBoundsChange(boundsEvent:NativeWindowBoundsEvent):void{ draw(boundsEvent.afterBounds.width, boundsEvent.afterBounds.height); }\n\nHandling content in an elliptical window\n\nBecause the content in the !Square window is aware of its container (the springs are attached to the border), it must react to changes in window size and shape. It does this by listening for the window resize event and recalculating the dependent variables.\n\nprivate... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9879456758,
        "format_confidence":0.7974740267
    },
    {
        "url":"https:\/\/www.kernel.org\/doc\/html\/v4.20\/admin-guide\/mm\/userfaultfd.html",
        "text":"\n\nFor example userfaults allows a proper and more optimal implementation of the PROT_NONE+SIGSEGV trick.\n\n\nUserfaults are delivered and resolved through the userfaultfd syscall.\n\nThe userfaultfd (aside from registering and unregistering virtual memory ranges) provides two primary functionalities:\n\n  1. read\/POLLIN protocol to notify a userland thread of the faults happening\n  2. various UFFDIO_* ioctls that can manage the virtual memory regions registered in the userfaultfd that allows userland to efficiently resolve the userfaults it receives via 1) or to manage the virtual memory in the background\n\nThe real advantage of userfaults if compared to regular virtual memory management of mremap\/mprotect is that the userfaults in all their operations never involve heavyweight structures like vmas (in fact the userfaultfd runtime load never takes the mmap_sem for writing).\n\nVmas are not suitable for page- (or hugepage) granular fault tracking when dealing with virtual address spaces that could span Terabytes. Too many vmas would be needed for that.\n\nThe userfaultfd once opened by invoking the syscall, can also be passed using unix domain sockets to a manager process, so the same manager process could handle the userfaults of a multitude of different processes without them being aware about what is going on (well of course unless they later try to use the userfaultfd themselves on the same region the manager is already tracking, which is a corner case that would currently return -EBUSY).\n\n\nWhen first opened the userfaultfd must be enabled invoking the UFFDIO_API ioctl specifying a uffdio_api.api value set to UFFD_API (or a later API version) which will specify the read\/POLLIN protocol userland intends to speak on the UFFD and the uffdio_api.features userland requires. The UFFDIO_API ioctl if successful (i.e. if the requested uffdio_api.api is spoken also by the running kernel and the requested features are going to be enabled) will return into uffdio_api.features and uffdio_api.ioctls two 64bit bitmasks of respectively all the available features of the read(2) protocol and the generic ioctl available.\n\nThe uffdio_api.features bitmask returned by the UFFDIO_API ioctl defines what memory types are supported by the userfaultfd and what events, except page fault notifications, may be generated.\n\nIf the kernel supports registering userfaultfd ranges on hugetlbfs virtual memory areas, UFFD_FEATURE_MISSING_HUGETLBFS will be set in uffdio_api.features. Similarly, UFFD_FEATURE_MISSING_SHMEM will be set if the kernel supports registering userfaultfd ranges on shared memory (covering all shmem APIs, i.e. tmpfs, IPCSHM, \/dev\/zero MAP_SHARED, memfd_create, etc).\n\nThe userland application that wants to use userfaultfd with hugetlbfs or shared memory need to set the corresponding flag in uffdio_api.features to enable those features.\n\nIf the userland desires to receive notifications for events other than page faults, it has to verify that uffdio_api.features has appropriate UFFD_FEATURE_EVENT_* bits set. These events are described in more detail below in \u201cNon-cooperative userfaultfd\u201d section.\n\nOnce the userfaultfd has been enabled the UFFDIO_REGISTER ioctl should be invoked (if present in the returned uffdio_api.ioctls bitmask) to register a memory range in the userfaultfd by setting the uffdio_register structure accordingly. The uffdio_register.mode bitmask will specify to the kernel which kind of faults to track for the range (UFFDIO_REGISTER_MODE_MISSING would track missing pages). The UFFDIO_REGISTER ioctl will return the uffdio_register.ioctls bitmask of ioctls that are suitable to resolve userfaults on the range registered. Not all ioctls will necessarily be supported for all memory types depending on the underlying virtual memory backend (anonymous memory vs tmpfs vs real filebacked mappings).\n\nUserland can use the uffdio_register.ioctls to manage the virtual address space in the background (to add or potentially also remove memory from the userfaultfd registered range). This means a userfault could be triggering just before userland maps in the background the user-faulted page.\n\nThe primary ioctl to resolve userfaults is UFFDIO_COPY. That atomically copies a page into the userfault registered range and wakes up the blocked userfaults (unless uffdio_copy.mode & UFFDIO_COPY_MODE_DONTWAKE is set). Other ioctl works similarly to UFFDIO_COPY. They\u2019re atomic as in guaranteeing that nothing can see an half copied page since it\u2019ll keep userfaulting until the copy has finished.\n\n\nQEMU\/KVM is using the userfaultfd syscall to implement postcopy live migration. Postcopy live migration is one form of memory externalization consisting of a virtual machine running with part or all of its memory residing on a different node in the cloud. The userfaultfd abstraction is generic enough that not a single line of KVM kernel code had to be modified in order to add postcopy live migration to QEMU.\n\nGuest async page faults, FOLL_NOWAIT and all other GUP features work just fine in combination with userfaults. Userfaults trigger async page faults in the guest scheduler so those guest processes that aren\u2019t waiting for userfaults (i.e. network bound) can keep running in the guest vcpus.\n\nIt is generally beneficial to run one pass of precopy live migration just before starting postcopy live migration, in order to avoid generating userfaults for readonly guest regions.\n\nThe implementation of postcopy live migration currently uses one single bidirectional socket but in the future two different sockets will be used (to reduce the latency of the userfaults to the minimum possible without having to decrease \/proc\/sys\/net\/ipv4\/tcp_wmem).\n\nThe QEMU in the source node writes all pages that it knows are missing in the destination node, into the socket, and the migration thread of the QEMU running in the destination node runs UFFDIO_COPY|ZEROPAGE ioctls on the userfaultfd in order to map the received pages into the guest (UFFDIO_ZEROCOPY is used if the source page was a zero page).\n\nA different postcopy thread in the destination node listens with poll() to the userfaultfd in parallel. When a POLLIN event is generated after a userfault triggers, the postcopy thread read() from the userfaultfd and receives the fault address (or -EAGAIN in case the userfault was already resolved and waken by a UFFDIO_COPY|ZEROPAGE run by the parallel QEMU migration thread).\n\nAfter the QEMU postcopy thread (running in the destination node) gets the userfault address it writes the information about the missing page into the socket. The QEMU source node receives the information and roughly \u201cseeks\u201d to that page address and continues sending all remaining missing pages from that new page offset. Soon after that (just the time to flush the tcp_wmem queue through the network) the migration thread in the QEMU running in the destination node will receive the page that triggered the userfault and it\u2019ll map it as usual with the UFFDIO_COPY|ZEROPAGE (without actually knowing if it was spontaneously sent by the source or if it was an urgent page requested through a userfault).\n\nBy the time the userfaults start, the QEMU in the destination node doesn\u2019t need to keep any per-page state bitmap relative to the live migration around and a single per-page bitmap has to be maintained in the QEMU running in the source node to know which pages are still missing in the destination node. The bitmap in the source node is checked to find which missing pages to send in round robin and we seek over it when receiving incoming userfaults. After sending each page of course the bitmap is updated accordingly. It\u2019s also useful to avoid sending the same page twice (in case the userfault is read by the postcopy thread just before UFFDIO_COPY|ZEROPAGE runs in the migration thread).\n\nNon-cooperative userfaultfd\n\nWhen the userfaultfd is monitored by an external manager, the manager must be able to track changes in the process virtual memory layout. Userfaultfd can notify the manager about such changes using the same read(2) protocol as for the page fault notifications. The manager has to explicitly enable these events by setting appropriate bits in uffdio_api.features passed to UFFDIO_API ioctl:\n\nenable userfaultfd hooks for fork(). When this feature is enabled, the userfaultfd context of the parent process is duplicated into the newly created process. The manager receives UFFD_EVENT_FORK with file descriptor of the new userfaultfd context in the uffd_msg.fork.\nenable notifications about mremap() calls. When the non-cooperative process moves a virtual memory area to a different location, the manager will receive UFFD_EVENT_REMAP. The uffd_msg.remap will contain the old and new addresses of the area and its original length.\nenable notifications about madvise(MADV_REMOVE) and madvise(MADV_DONTNEED) calls. The event UFFD_EVENT_REMOVE will be generated upon these calls to madvise. The uffd_msg.remove will contain start and end addresses of the removed area.\nenable notifications about memory unmapping. The manager will get UFFD_EVENT_UNMAP with uffd_msg.remove containing start and end addresses of the unmapped area.\n\nAlthough the UFFD_FEATURE_EVENT_REMOVE and UFFD_FEATURE_EVENT_UNMAP are pretty similar, they quite differ in the action expected from the userfaultfd manager. In the former case, the virtual memory is removed, but the area is not, the area remains monitored by the userfaultfd, and if a page fault occurs in that area it will be delivered to the manager. The proper resolution for such page fault is to zeromap the faulting address. However, in the latter case, when an area is unmapped, either explicitly (with munmap() system call), or implicitly (e.g. during mremap()), the area is removed and in turn the userfaultfd context for such area disappears too and the manager will not get further userland page faults from the removed area. Still, the notification is required in order to prevent manager from using UFFDIO_COP... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9611905813,
        "format_confidence":0.9870917797
    }
]