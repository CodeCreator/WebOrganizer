[
    {
        "url":"http:\/\/www.lispworks.com\/documentation\/lw51\/CLHS\/Issues\/iss069_w.htm",
        "text":"\n\n\nForum:         Cleanup\n\n\nReferences: CLtL p.208, 212\n\nRelated issues: IEEE-ATAN-BRANCH-CUT\n\nCategory: CHANGE\n\nEdit history: Version 1, 13-Dec-88, Steele\n\nProblem description:\n\nThe formula that defines ATAN results in a branch cut that is at\n\nvariance with the recommendations of Prof. W. Kahan and with the\n\nimplementations of that function in many computing systems and\n\n\n\nReplace the formula\n\narctan z = - i log ((1+iz) sqrt (1\/(1+z^2)))\n\nwith the formula\n\narctan z = (log (1+iz) - log (1-iz)) \/ (2i)\n\nThis leaves the branch cuts pretty much in place; the only change is\n\nthat the upper branch cut (on the positive imaginary axis above i)\n\nis continuous with quadrant I, where the old formula has it continuous\n\nwith quadrant II.\n\n\n(atan #c(0 2)) => #c(-1.57... 0.549...) ;Current\n\n(atan #c(0 2)) => #c(1.57... -0.549...) ;Proposed\n\nNote: 1.57... = pi\/2, and 0.549... = (log 3)\/2.\n\n\nCompatibility with what seems to be becoming standard practice.\n\nCurrent practice:\n\n(atan #c(0 2)) => #c(-1.57... 0.549...) ;Symbolics CL\n\n(atan #c(0 2)) => #c(-1.57... 0.549...) ;Allegro CL 1.1 (Macintosh)\n\n(atan #c(0 2)) => #c(-1.57... 0.549...) ;Sun-4 CL 2.1.3 of 10-Nov-88\n\n(atan #c(0 2)) => #c(1.57... -0.549...) ;Sun CL 2.0.3 of 30-Jun-87\n\n(atan #c(0 2)) => #c(1.57... 0.549...) ;KCL of 3-Jun-87\n\nNote that in KCL the upper branch cut is thus continuous with\n\nquadrant I, but its lower branch cut is continuous with quadrant III!\n\nCost to Implementors:\n\nATAN must be rewritten. It is not a very difficult fix.\n\nCost to Users:\n\nIt is barely conceivable that some user code could depend on this.\n\nNote that the proposed change invalidates the identities\n\narctan i z = i arctanh z\n\nand arctanh i z = i arctan z\n\non the upper branch cut.\n\nThe compatibility note on p. 210 of CLtL gave users fair warning that\n\na change of this kind might be adopted.\n\nCost of non-adoption:\n\nIncompatibility with HP calculators.\n\n\nNumerical analystsmay find the new definition easier to use.\n\n\nA toss-up, except to those who care.\n\n\nSteele has sent a letter to W. Kahan at Berkeley to get any last\n\ncomments he may have on the matter.\n\nPaul Penfield of MIT, after whose article the Common Lisp branch\n\ncuts were originally patterned, endorses this change.\n\n[Starting Points][Contents][Index][Symbols][Glossary][Issues]\nCopyright 1996-2005, LispWorks Ltd. All rights reserved.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9535427094,
        "format_confidence":0.9497449398,
        "weight":0.0315897601
    },
    {
        "url":"https:\/\/sigtar.com\/tag\/h264\/",
        "text":"HEVC media optimization\n\nDo you have a large collection of video media files not using HEVC (H265) yet? There is a massive amount of disk space coming your way if you flick over to the new video codec format.\n\nHEVC definitely lives up to its name, for most media you can expect a 70% or more disk savings from transcoding from an old codec. There are some catches though\u2026 If you want your TV to play it direct (i.e. straight off the file) the codec will need to be supported by it. You can of course get around this by using a media server such as Plex or Emby which will transcode from HEVC back to a compatible format.\n\nWhy would you transcode to HEVC? \u2013 again, disk space. HEVC as stated above can reduced you Media footprint significantly. You could boost your quality and save your disk space at the same time by recording at a higher resolution then applying the HEVC codec.\n\nI created a powershell script to transcode my media to HEVC using my AMD graphics card. The advantage of doing this is that transcoding completed by my GPU is significantly faster than my CPU. I do not have the graphics card in my media server, so instead connect via SMB and let my gaming machine run the transcoding from remote\u2026\n\nThe powershell script uses ffmpeg to ;\n\n  \u2022 transcodes video stream to hevc using AMD h\/w encoder\n  \u2022 copys all existing audio and subtitles (i.e. no conversion)\n  \u2022 works in batches (to prevent constant scanning of files) \u2013 able to set max batch size and processing time before re-scanning disk\n  \u2022 overwrites source with new HEVC transcode if\u00a0move_file = 1\u00a0(WARNING this is default!)\n  \u2022 checks to see if video codec is already HEVC (if so, skips)\n  \u2022 writes\u00a0transcode.log\u00a0for successful transcode (duration and space savings)\n  \u2022 writes\u00a0skip.log\u00a0for already hevc and failed transcodes (used to skip in next loop, errors in transcode.log)\n\nCheck here for updates and script \u2013 https:\/\/github.com\/dwtaylornz\/hevctranscode",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.7694448829,
        "format_confidence":0.5453820229,
        "weight":0.0181401201
    },
    {
        "url":"http:\/\/www.nitag-resource.org\/fr\/mediatheque\/documents\/3465-prevention-of-conflicts-of-interest-in-nitags",
        "text":"Prevention of Conflicts of Interest in NITAGs\n\nDirectives (Guidelines)\nR\u00e9gions multiples\nTitre original\nPrevention of Conflicts of Interest in NITAGs\nAMP - Agence de M\u00e9decine Pr\u00e9ventive\nAnn\u00e9e de publication\n\n\nThe objectives of these guidelines are:\n\n  \u2022 To explain what Conflict of Interest is and how it can arise at the NITAG level\n  \u2022 To explain the importance of preventing and managing Conflict of Interests at the NITAG level and how this can be achieved\n  \u2022 To provide guidance to NITAGs on the development and implementation of their own Conflict of Interest prevention policy, taking into account the country\u2019s specific circumstances\n  \u2022 To provide a generic and ready-to-use Conflict of Interest management policy for NITAGs (to be used as provided or after adjustment to better capture the country\u2019s specific situation).\n\nActualit\u00e9s et \u00e9v\u00e8nements en relation",
        "topic_id":12,
        "format_id":20,
        "topic_confidence":0.7771626115,
        "format_confidence":0.576451838,
        "weight":0.0012682291
    },
    {
        "url":"https:\/\/thunder.github.io\/thunder-documentation\/feature-notes",
        "text":"Skip to main content\n\nRelease 3.2\n\nAutosave Form\n\nThe \u201cAutosave Form\u201d module provides an autosave feature for all forms. The autosave submits will be triggered every 60 seconds and store the changes from the currently logged-in user in the database.\n\nIn Thunder, it is active for node articles and basic pages and all media types.\n\nFeature walkthrough\n\nFor demonstration purpose, it\u2019s recommended to decrease the interval to trigger the autosave on admin\/config\/content\/autosave_form.\n\nAutosaves only works on existing entities, for now, so navigate to one. For example, you can use Node 6 from the Thunder Demo module.\n\n  \u2022 Wait for the first \u201cSaving draft\u2026\u201d without doing any changes\n  \u2022 Reload the page and you will notice no restore messages.\n  \u2022 Now change the title to \u201cMy new title.\u201d\n  \u2022 Reload the page again and you will get a restoring message.\n  \u2022 Resume editing\n  \u2022 Add a new tag \u201cCMS\u201d\n  \u2022 Add a new image paragraph between some existing paragraphs.\n  \u2022 Wait for the first \u201cSaving draft\u2026\u201d and reload the page again\n  \u2022 Resume editing and verify that your changes are still there\n  \u2022 Reload again and discard the changes\n  \u2022 Your initial article is back",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9341325164,
        "format_confidence":0.9661146998,
        "weight":0.0321342398
    },
    {
        "url":"https:\/\/blogs.arubanetworks.com\/solutions\/securing-the-distributed-enterprise\/",
        "text":"Securing the Distributed Enterprise\u00a0\n\nShare Post\n\nSecurity is a critical component of an SD-Branch solution for both the WAN and the LAN. Customers using SD-WAN and SD-Branch (a superset of SD-WAN that includes the branch LAN) need to know that their network and services are well protected.\n\nThe goal of zero trust, according to NIST, is to provide \u201cnetwork security paradigms that narrow defenses from wide network perimeters to individuals or small groups of resources.\u201d This is in response to the ever-heightening emphasis on cloud-based assets and remote users, as opposed to enterprise-controlled network boundaries.\n\nOne of the key tenets of zero trust assumes that any entity on the internal network may be untrusted or at worst malicious. For instance, SD-Branch looks at communications between internal devices and users and Internet resources. Within the branch, intrusion detection and protection functionality (IDS\/IPS) inspects inter- and intra-branch traffic for attacks on the network, preventing malware (and other compromised traffic) from communicating with a home based of attack such as a command-and-control server.\n\nSecurity Layers\nThe security of the Aruba SD-Branch solution is built in layers, from the hardening of the operating system to the integration with best-of-breed security partners (see\u00a0Figure 1 below).\n\nSecurity Layers in the Aruba SD-Branch Solution\n\nFigure 1. Security Layers in the Aruba SD-Branch Solution\n\nAruba\u2019s SD-Branch portfolio provides a comprehensive solution across all aspects of WAN and LAN performance and branch security. With Aruba SD-Branch, built-in integrated security provides stateful firewall, intrusion detection and prevention, deep packet inspection (DPI), web content filtering, and other policy-based security, privacy, and compliance controls, all managed from Aruba Central.\n\nFurthermore, the solution implements and mandates very robust hardening policies, which is critical since branches are directly exposed to the Internet. Finally, Aruba-based branch networks benefit from \u201cbest-of-breed\u201d security.\n\nAruba branch gateways include rich firewall functionality that can be hardened to further ensure a secure branch.\n\nSolution Components\nZero-trust security in Aruba branch and headend gateways begins with ArubaOS: a tightly hardened operating system. The functionality of ArubaOS includes:\n\n  \u2022 Secure boot: Heavily restricting communications until the gateway has received its configuration from Aruba Central. Only trusted systems can boot the device.\n  \u2022 IPsec VPN: Aruba branch gateways and headend gateways support high-performance IPsec VPN for secure overlay networking across the Internet or other untrusted networks. Aruba uses AES-256 encryption (using trusted key exchanges) for all branch-to-hub tunnels. Notably, Aruba branch gateways and headend gateways support VPN termination from client endpoints directly. In a branch, this enables employees or contractors to access internal systems, such as security cameras or Internet of Things (IoT) sensors, based on their allowed role.\n  \u2022 Role-based Stateful firewall: The Aruba Policy Enforcement Firewall (PEF) is a full, stateful firewall able to tightly control what users and devices are permitted to do, enabling application-layer security and providing separation between user roles. Roles are trusted or not based on their profiles. This gives network administrators insight into the applications running on the network and who is using them.\n  \u2022 Deep Packet Inspection (DPI) module: Includes the capacity to identify close to 3,200 known and\/or trusted applications via application fingerprinting.\n  \u2022 Intrusion detection and prevention (IDS\/IPS): This is part of an advanced threat defense, which will be detailed more in an upcoming blog. This is preventing communications from untrusted systems.\n  \u2022 Web content and reputation filtering: The Aruba branch gateway uses Webroot cloud-based machine learning classification technology. Websites are classified for content-based filtering. The reputation of all public IP address space is monitored to detect and block threats such as spam, exploits, botnets, phishing, proxies, and mobile threats. Geolocation information allows you to block IP ranges based on country. Only safe or trusted domains and locations are permitted.\n  \u2022 Cloud security integration: This allows organizations that use cloud security services from third parties to have the same policy applied to user groups in the branch or at headquarters. Aruba delegates control to our third-party partners for trust authorization.\n  \u2022 Dynamic segmentation: Users can tunnel connections on Aruba wired switch ports to the branch gateway and apply consistent policy to the user or device the same way you apply policy to wireless users. Trust is based upon user and device roles.\n\nThe Aruba SD-Branch solution integrates with Aruba ClearPass (or other AAA servers) to form a zero-trust, policy-driven branch. This model dynamically assigns policies based on users and devices, as opposed to the traditional way of assigning these policies manually based on ports, VLANs, and IP addresses.\n\nFinally, the Aruba SD-Branch solution can integrate with best-of-breed third-party security infrastructure partners in the Aruba 360 Security Exchange Program. With these integrations, the Aruba SD-Branch architecture seeks to offer enterprise-grade advanced threat protection in a scalable manner.\n\nFor more information on Aruba\u2019s zero-trust security approach for SD-WAN and SD-Branch, follow the links in the article and check with you Aruba representative (account manager to systems engineer).",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.7086096406,
        "format_confidence":0.5932508111,
        "weight":0.0031090653
    },
    {
        "url":"https:\/\/developer.apple.com\/design\/human-interface-guidelines\/macos\/menus\/menu-anatomy\/",
        "text":"A menu presents a list of items\u2014commands, attributes, or states\u2014from which a user can choose. An item within a menu is known as a menu item, and may be configured to initiate an action, toggle a state on or off, or display a submenu of additional menu items when selected or in response to an associated keyboard shortcut. Menus can also include separators, and menu items can contain icons and symbols, like checkmarks. By default, all menus adopt translucency.\n\nScreenshot of the Safari File menu, with the Share submenu opened and the AirDrop item chosen.\n\nWhen a menu is displayed onscreen, it remains open until the user chooses a menu item, navigates to another menu, clicks outside of the menu, switches to another app, or quits the app; or until the system displays an alert.\n\nTypes of Menus\n\nThere are three types of menu in macOS, each of which has a specific use case, as noted in the following table:\n\nMenu Type Description\nMenu bar menu Exposes app-specific menu items when chosen from the menu bar at the top of the screen. An app typically displays several menus in the menu bar. See Menu Bar Menus.\nContextual menu Exposes menu items related to the user\u2019s current context. A contextual menu (or shortcut menu) is displayed by Control-clicking a view or selected element in an app. See Contextual Menus.\nDock menu Exposes system-defined menu items (like Show in Finder) and app-specific menu items (like Compose New Message) when Control-clicking an app\u2019s Dock icon. See Dock Menus.\n\nTIP A pop-up button, often referred to as a pop-up menu, is a type of button that displays a menu of choices when clicked. See Pop-Up Buttons.\n\nA title describes a menu or menu item. People navigate menus and choose menu items based on their titles, so it\u2019s important for the titles to be accurate and informative.\n\nUse title-style capitalization. Title-style capitalization is used consistently for menu and menu item titles throughout the system. For more information, see Apple Style Guide.\n\nTIP Contextual and Dock menus don\u2019t need titles because they\u2019re opened after focusing on a selection, view, or Dock icon. See Contextual Menus and Dock Menus.\n\nA menu title describes the contents of the menu.\n\nProvide intuitive menu titles. A menu title should help people anticipate the types of items the menu contains. For example, you would expect a menu titled Font to include options for adjusting text attributes, not performing editing activities like copying and pasting.\n\nKeep menus enabled even when menu items are unavailable. It\u2019s important for people to be able to browse the contents of all menus to learn where commands reside, even when those commands aren\u2019t available.\n\nMake menu titles as short as possible without sacrificing clarity. One-word menu titles are best because they take up very little space in the menu bar and are easy to scan. If you must use more than one word in a menu title, use title-style capitalization.\n\nUse text, not icons, for menu titles. Only menu bar extras use icons to represent menus. See Menu Bar Extras. It\u2019s also not acceptable to use a mixture of text and icons in menu titles.\n\nA menu item title describes an action or attribute.\n\nUse verbs and verb phrases for menu items that initiate actions. Describe the action that occurs when the menu item is chosen, such as Print or Copy.\n\nUse adjectives or adjective phrases for menu items that toggle attribute states. Describe the attribute the menu item affects. Adjectives appearing in menu item titles imply an action and can often fit into the sentence \u201cChange the selected object to\u2026\u201d\u2014for example, Bold or Italic.\n\nRefrain from using articles in menu item titles. For example, use Add Account instead of Add an Account, or Hide Toolbar instead of Hide the Toolbar. Articles rarely add value because the user has already made a selection or entered a specific context. Use this style consistently in all menu item titles.\n\nUse an ellipsis whenever choosing a menu item requires additional input from the user. The ellipsis character (\u2026) means a dialog or separate window will open and prompt the user for additional information or to make a choice.\n\nDisable unavailable menu items. A disabled menu item\u2014which appears gray and doesn\u2019t highlight when the pointer moves over it\u2014helps people understand that an item is unavailable.\n\nConsider assigning keyboard shortcuts to frequently used menu items in the menu bar. A keyboard shortcut, like Command-C for Copy, lets people quickly invoke the menu item anytime using a simple keystroke. Keyboard shortcuts aren\u2019t used in contextual menus or Dock menus.\n\nA submenu is a menu item that operates as a menu, displaying a set of nested items when selected. Submenus let you construct hierarchical menus that group related commands together to keep menus organized and intuitive. For example, the Edit menu often includes Find, Spelling and Grammar, Substitutions, Transformations, and Speech submenus, each of which contains menu items that are helpful when editing text. Menu items that have a submenu include a triangle to differentiate them from menu items that don't have submenus. When the user highlights (or uses the keyboard to select) a menu item with a triangle, the submenu appears alongside its parent menu.\n\nLimit the use of submenus. Every submenu adds a layer of complexity and hides menu items from the user. Reserve submenus for when you have groups of closely related commands that can be intuitively grouped under a single parent menu item, or when you need to reduce the length of your menus.\n\nLimit the depth and length of submenus. If you must include submenus, restrict them to a single level. If a submenu contains more than five items, consider giving it its own menu.\n\nMake sure the menu items within a submenu are logically related. In general, submenus work best for menu items that toggle attributes on and off rather than initiate actions. For example, the Format menu in Pages has a Font submenu that includes menu items for enabling and disabling text attributes, such as Bold, Italic, and Underline.\n\nProvide an intuitive submenu title. Provide a succinct, descriptive title that hints at the menu items the submenu contains. For guidance, see Menu and Menu Item Titles.\n\nKeep submenus enabled even when their nested menu items are unavailable. It\u2019s important for people to be able to browse menus and submenus to learn where commands reside, even when those commands aren\u2019t available.\n\nUse a submenu instead of indenting menu items. Indentation results in an inconsistent interface and doesn\u2019t express relationships between menu items. If you need to indent, then a submenu is a better choice.\n\nOrganizing Menu Items\n\nOrganize menu items to help people locate commands.\n\nUse separator lines to create visually distinct groups of related menu items. The number of groups to provide is partly an aesthetic decision and partly a usability decision.\n\nPlace the most frequently used items at the top of a menu. When users click to open a menu, their focus is on the top area of that menu. Placing the most used items at the top of the menu ensures that people will always find the item they\u2019re looking for. At the same time, avoid arranging an entire menu based on frequency of use. It\u2019s better to create groups of related items and place the more frequently used groups above the less frequently used groups. For example, the Find Next command typically appears beneath the Find command.\n\nCreate separate groups for menu items that initiate actions and menu items that set attributes. Menu items become unpredictable when the behavior of grouped items varies.\n\nGroup interdependent menu items that set attributes. People expect to find related attribute menu items together. For example, when setting font attributes, it makes sense to see the menu items for applying bold, italic, and underline grouped together. Within a group, attribute menu items can be mutually exclusive (the user can select only one item, like an alignment) or independent (the user can select multiple items, like bold and italic).\n\nMake it easy for people to locate menu items by grouping similar actions together. For example, the Arrange menu in Numbers includes groups for aligning and distributing objects, as well as items for Group and Ungroup.\n\nConsolidate related menu items. If a term is used more than twice within a group of menu items, consider dedicating a separate menu or submenu to the term. For example, instead of offering separate menu items for Sort by Date, Sort by Subject, and Sort by Unread, the View menu in Mail includes a Sort By submenu that contains items like Date, Subject, and Unread.\n\nBe mindful of menu length. In general, long menus are difficult to scan and can be overwhelming. If a menu becomes too long, try redistributing its items. See if some items fit naturally in other menus or if it makes sense to create a new menu. Groups of related items sometimes call for the use of a submenu. See Submenus.\n\nDon\u2019t intentionally design a scrolling menu. Scrolling menus have items that extend beyond the top or bottom edge of the screen. Scrolling menus are acceptable when a menu contains user-defined or dynamically generated content. For example, the History and Bookmarks menus in Safari may have scrolling submenus. The History menu includes submenus for websites visited on specific days; the length of the Bookmarks submenus depends on how the user has organized their favorite websites. A scrolling menu displays a downward or upward triangle at its bottom or top to hint at scrollable offscreen content.\n\nVariable Menu Items\n\nSometimes it makes sense for a menu to display slightly different items based on a user action. macOS supports two ways to do this: dynamic menu items and toggled menu items.\n\nDynamic Menu Items\n\nA menu item is dynamic when its behavior changes with the addition of a modifier key (Control, Option, Shift, or Command). For example, the Minimize item in the Window menu changes to Min... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.729806602,
        "format_confidence":0.9452585578,
        "weight":0.0314405372
    },
    {
        "url":"https:\/\/www.sqlite.org\/c3ref\/backup.html",
        "text":"Small. Fast. Reliable.\nChoose any three.\n\nSQLite C Interface\n\nOnline Backup Object\n\ntypedef struct sqlite3_backup sqlite3_backup;\n\nThe sqlite3_backup object records state information about an ongoing online backup operation. The sqlite3_backup object is created by a call to sqlite3_backup_init() and is destroyed by a call to sqlite3_backup_finish().\n\nSee Also: Using the SQLite Online Backup API\n\nSee also lists of Objects, Constants, and Functions.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9821352959,
        "format_confidence":0.9778758287,
        "weight":0.0325254303
    },
    {
        "url":"http:\/\/www.skf.com\/sg\/products\/bearings-units-housings\/ball-bearings\/principles\/application-of-bearings\/radial-location-of-bearings\/dimensional-form-and-running-accuracy-of-bearing-seats-and-abutments\/index.html",
        "text":"Dimensional, form and running accuracy of bearing seats and abutments\n\nThe accuracy of cylindrical bearing seats on shafts and in housing bores, of seats for thrust bearing washers and of the support surfaces (abutments for bearings provided by shaft and housing shoulders etc.) should correspond to the accuracy of the bearings used. In the following, guideline values for the dimensional, form and running accuracy are provided. These should be followed when machining the seats and abutments.\n\nDimensional tolerances\n\nFor bearings made to Normal tolerances, the dimensional accuracy of cylindrical seats on the shaft should be at least to grade 6 and in the housing at least to grade 7. Where adapter or withdrawal sleeves are used, wider diameter tolerances (grades 9 or 10) can be permitted than for bearing seats, see table. The numerical values of standard tolerance grades IT to ISO 286-1:1988 can be found in table. For bearings with higher accuracy, correspondingly better grades should be used.\n\nTolerances for cylindrical form\n\nThe cylindricity tolerances as defined in ISO\u00a01101:2004 should be 1 to 2 IT grades better than the prescribed dimensional tolerance, depending on requirements. For example, if a bearing shaft seat has been machined to tolerance m6, then the accuracy of form should be to IT5 or IT4. The tolerance value t1 for cylindricity is obtained for an assumed shaft diameter of 150\u00a0mm from t1\u00a0=\u00a0IT5\/2\u00a0=\u00a018\/2\u00a0=\u00a09\u00a0\u00b5m. However, the tolerance t1 is for a radius, hence 2t1 applies for the shaft diameter. table provides guideline values for the cylindrical form tolerance and the total runout tolerance for the different bearing tolerance classes.\nWhen bearings are to be mounted on adapter or withdrawal sleeves, the cylindricity of the sleeve seat should be IT5\/2 (for h9) or IT7\/2 (for h10), see table.\n\nTolerances for perpendicularity\n\nAbutments for bearing rings should have a rectangularity tolerance as defined in ISO\u00a01101:2004, which is better by at least one IT grade than the diameter tolerance of the associated cylindrical seat. For thrust bearing washer seats, the tolerance for perpendicularity should not exceed the values of IT5. Guideline values for the tolerance for rectangularity and for the total axial runout can be found in table.\n\nTolerances for tapered journal seats\n\nWhen a bearing is mounted directly onto a tapered shaft seat, the seat diameter tolerance can be wider than in the case of cylindrical seats. Fig 1 shows a grade 9 diameter tolerance, while the form tolerance stipulations are the same as for a cylindrical shaft seat. SKF recommendations for tapered shaft seats for rolling bearings are as follows.\nThe permissible deviation of the taper incline is a \u00b1 tolerance in accordance with IT7\/2 based on the bearing width B (fig 2). The value can be determined by \u0394k = IT7\/2B\n\nThe permissible range of dispersion (variation of the taper incline) thus becomes\n\nVk = 1\/k \u00b1 IT7\/2B\n\n\nVk=the permissible range of dispersion of the taper incline\n\u0394k=the permissible deviation of the taper incline\nk=factor for the taper\n12 for taper 1:12\n30 for taper 1:30\nB=bearing width, mm\nIT7=the value of the tolerance grade, based on the bearing width, mm\n  \u2022 The straightness tolerance is IT5\/2, based on the diameter d and is defined as: \"In each axial plane through the tapered surface of the shaft, the tolerance zone is limited by two parallel lines a distance \"t\" apart.\"\n  \u2022 The radial deviation from circularity is IT5\/2, based on the diameter d and is defined as: \"In each radial plane along the tapered surface of the shaft, the tolerance zone is limited by two concentric circles a distance \"t\" apart.\" When particularly stringent running accuracy requirements are stipulated, IT4\/2 is to apply instead.\nThe best way to check that the taper is within the recommended tolerances is to measure with special tapered gauges, based on two saddles. More practical methods, but less accurate, are to use ring gauges, tapered gauges or sine bars.\nSKF logo",
        "topic_id":15,
        "format_id":20,
        "topic_confidence":0.8098343015,
        "format_confidence":0.953389585,
        "weight":0.0010946428
    },
    {
        "url":"http:\/\/www.lispworks.com\/documentation\/lw50\/CAPUG-U\/html\/capiuser-u-54.htm",
        "text":"5.7.2 Selections\n\nAll choices have a selection. This is a state representing the items currently selected. The selection is represented as a vector of offsets into the list of the choice's items, unless it is a single-selection choice, in which case it is just represented as an offset.\n\nThe initial selection is controlled with the initarg :selection . The accessor choice-selection is provided.\n\nGenerally, it is easier to refer to the selection in terms of the items selected, rather than by offsets, so the CAPI provides the notion of a selected item and the selected items . The first of these is the selected item in a single-selection choice. The second is a list of the selected items in any choice.\n\nThe accessors choice-selected-item and choice-selected-items and the initargs :selected-item and :selected-items provide access to these conceptual slots.\n\nLispWorks CAPI User Guide (Unix version) - 14 Jun 2006",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9555914402,
        "format_confidence":0.9868260026,
        "weight":0.0328231248
    },
    {
        "url":"http:\/\/www.epanorama.net\/zen_schematics\/Circuits\/Testgear\/emfprobe2.html",
        "text":"Electromagnetic Field Probe Version 2\n\n\nAn electromagnetic field probe designed to detect changing electric and magnetic fields. The probe has switchable gain, a frequency response up to 400kHz and independent audio and meter monitoring.\n\nCircuit Notes:\n\nThis EMF probe uses an inductor to locate stray electromagnetic (EM) fields. It will respond to both changing magnetic and electric fields as each will induce a voltage in the inductor. The circuit is built around a quad low noise FET input op-amp, type TL084.\n\nPower supply is a single 9 Volt battery, the supply being divided by R5 and R6. C1 and C2 help smooth variations in battery voltage, S1 is the on off switch. The input stage U1, is direct coupled to the probe, a radial wound 1mH inductor, type Toko 8RB as shown in the probe construction. This part appears only available from Jabdog Electronics in the UK, part number 187LY-102J. If not available then the 1.2, 1.5 or 1.8mH inductor will work equally well. The reactance of the inductor changes with input frequency and stage gain is very high. As there is no offset null control in the TL084 then the output is capacitively coupled via C3 to the next Tl084 amplifier U2. This stage has switchable gain of approximately 1.5x and 4.7x controlled by S2.\n\nThe output gain of both U1 and U2 stages ( with switch S2 open ) is about 70dB at 1KHz. Gain is still about 30dB at 400KHz, although the signal meter will not be too accurate at such high frequency. The bode plot simulated in LTspice is shown below:\n\nThe output from U2 is split by C4 and C5 and drives an independent headphone amplifier built around U4. VR1 acts as a volume control the output being either a mono or stereo miniature jack plug as shown. The output stage of the TL084 is sufficiently low to drive 32 ohm headphones like Sennheiser or Ipod Shuffle, etc. U3 is the meter amplifier. All EMF fields are amplified across the load resistor R8. D1 now acts as a half wave rectifier and creates sufficient DC voltage to drive a small signal meter, shown below.\n\nThis signal meter is available from Maplin Electronics part number LB80B and has a FSD of 250uA and an internal resistance of 675 ohms. However any meter will work having a similar sensitivity. Meters of 100 or 50uA FSD can also be used providing a suitable series resistor is used. Because the circuit is responding to RF frequencies up to several hundred kHz a smoothing capacitor across the meter should not be used as this would appear as an effective short circuit reducing the average current through the meter to zero.\n\nProbe Construction:\n\nThe probe is made from an old pen tube, the end cap being removed. A 50cm length of audio screened cable is threaded through the pen tube and soldered to the radial inductor. The capacitance of 50cm audio cable is about 2pF, longer cable should not be used as high frequency performance will deteriorate.\n\nThe cable may be used with a 3.5mm mono plug and socket if desired. My completed probe is shown below. The diameter of the inductor fitted neatly against the body of the pen tube. A layer of insulating tape or glue may be used to secure the pen body to the inductor.\n\nSimulation Model:\n\nTo model this circuit in LTspice or any other simulator you have to take into account the input capacitance of the probe cable, and the impedance of the inductor itself. The cable capacitance was measured by a capacitance meter and came out at 1.9pF, so 2pF was added in parallel with L2 which is the probe inductor. The simulation schematic is shown below:\n\nThe Toko 8RB inductor has a series resistance of 7 ohms, at 100kHz the impedance is 628.3 ohms. The series resistance of L2 needs to be included, in LTspice the inductor L2 can be right clicked and a value for series resistance entered, or as shown above can be entered in the value of Rs. A transient response at 10kHz is shown below:\n\nThe simulation model has 3 nodes labeled Vgain, Vheadphone and Vmeter for clarity. These waveforms are shown above. The input has been simulated by a signal generator feeding another coil. The coupling coefficient of 0.9 is used and input voltage of 10mV pk-pk used.\n\nDownload Simulation Circuit:\n\nThe simulation circuit for LTspice can be downloaded here. Please note that you will also have to download the model for the potentiometer and TL072 op amp from the LTspice yahoo group. The TL072 simulation model is the same as the TL084 model.\n\n\nIf you have access to an audio or RF signal generator you can apply an input signal to the windings of a small transformer or another inductor. This will set up an electromagnetic field which will be easily detected by the probe. Without a signal generator, just place the probe near a power supply, mains wiring or other electrical device. There will be a deflection on the meter and sound in the headphones if the frequency is below 15KHz.\n\nParts List:\n\nIC1 TL084\nD1 1N4148\nL1 1mH radial inductor part 187LY-102J\nR1 470k\nR2,R5,R6,R7,R9 10k\nR3 22k\nR4 47k\nR8 4k7\nVR1 10k log\nC1,C6,C8 220u\nC2,C4,C5 10u\nC3 1u\nC7 2.2u\nSignal meter 250uA FSD, Maplin LB80B or similar\n\nIn Use\n\nSwitch on, set VR1 to minimum and plug in headphones (optional). The circuit can be built on veroboard and is designed to be portable. Try moving the probe near a light switch or electric socket and a loud hum will be heard in the headphones and meter will deflect.\n\nReturn to Testgear Circuits\n\nContent sourced from Zen Schematics\n\nCircuit: Andy Collinson",
        "topic_id":5,
        "format_id":20,
        "topic_confidence":0.7128244042,
        "format_confidence":0.8182865977,
        "weight":0.0015353873
    },
    {
        "url":"http:\/\/www.cpan.org\/authors\/id\/K\/KJ\/KJETILK\/RDF-Generator-Void-0.04.readme",
        "text":"NAME RDF::Generator::Void - Generate VoID descriptions based on data in an RDF model VERSION Version 0.04 Note that this is a beta release. It has the core functionality in place to create a basic VoID description and what's there should be working well. Nevertheless significant changes in this module may be coming up really soon. SYNOPSIS use RDF::Generator::Void; use RDF::Trine::Model; my $mymodel = RDF::Trine::Model->temporary_model; [add some data to $mymodel here] my $generator = RDF::Generator::Void->new(inmodel => $mymodel); $generator->urispace(''); $generator->add_endpoints(''); my $voidmodel = $generator->generate; DESCRIPTION This module takes a RDF::Trine::Model object as input to the constructor, and based on the data in that model as well as data supplied by the user, it creates a new model with a VoID description of the data in the model. For a description of VoID, see . METHODS new(inmodel => $mymodel, dataset_uri => URI->new($dataset_uri)); The constructor. It can be called with two parameters, namely, \"inmodel\" which is a model we want to describe and \"dataset_uri\", which is the URI we want to use for the description. Users should make sure it is possible to get this with HTTP. If this is not possible, you may leave this field empty so that a simple URN can be created for you as a default. \"inmodel\" Read-only accessor for the model used in description creation. \"dataset_uri\" Read-only accessor for the URI to the dataset. Property Attributes The below attributes concern some essential properties in the VoID vocabulary. They are mostly arrays, and can be manipulated using array methods. Methods starting with \"all_\" will return an array of unique values. Methods starting with \"add_\" takes a list of values to add, and those starting with \"has_no_\" return a boolean value, false if the array is empty. \"vocabulary\", \"all_vocabularies\", \"add_vocabularies\", \"has_no_vocabularies\" Methods to manipulate a list of vocabularies used in the dataset. The values should be a string that represents the URI of a vocabulary. \"endpoint\", \"all_endpoints\", \"add_endpoints\", \"has_no_endpoints\" Methods to manipulate a list of SPARQL endpoints that can be used to query the dataset. The values should be a string that represents the URI of a SPARQL endpoint. \"title\", \"all_titles\", \"add_titles\", \"has_no_titles\" Methods to manipulate the titles of the datasets. The values should be RDF::Trine::Node::Literal objects, and should be set with language. Typically, you would have a value per language. \"license\", \"all_licenses\", \"add_licenses\", \"has_no_licenses\" Methods to manipulate a list of licenses that regulates the use of the dataset. The values should be a string that represents the URI of a license. \"urispace\", \"has_urispace\" This method is used to set the URI prefix string that will match the entities in your dataset. The computation of the number of entities depends on this being set. \"has_urispace\" can be used to check if it is set. Running this stuff \"stats\", \"clear_stats\", \"has_stats\" Method to compute a statistical summary for the data in the dataset, such as the number of entities, predicates, etc. \"clear_stats\" will clear the statistics and \"has_stats\" will return true if exists. generate( [ $model ] ) Returns the VoID as an RDF::Trine::Model. You may pass a model with statements as argument to this method. This model may then contain arbitrary RDF that will be added to the RDF model. If you do not send a model, one will be created for you. AUTHORS Kjetil Kjernsmo \"\" Toby Inkster \"\" Tope Omitola, \"\" TODO * Allow arbitrary RDF to be added to the VoID. * Larger test dataset for more extensive tests. * URI regexps support. * Partitioning based on properties and classes. * Technical features (esp. serializations). * Example resources and root resources. * Data dumps. * Subject classification. * Method to disable heuristics. * More heuristics. * Linkset descriptions. * Set URI space on partitions. * Conditional updates based on model ETags. * Save the description to files? BUGS Please report any bugs you find to SUPPORT You can find documentation for this module with the perldoc command. perldoc RDF::Generator::Void The Perl and RDF community website is at where you can also find a mailing list to direct questions to. You can also look for information at: * AnnoCPAN: Annotated CPAN documentation * CPAN Ratings * MetaCPAN ACKNOWLEDGEMENTS LICENSE AND COPYRIGHT Copyright 2012 Tope Omitola, Kjetil Kjernsmo, Toby Inkster. This program is free software; you can redistribute it and\/or modify it under the terms of either: the GNU General Public License as published by the Free Software Foundation; or the Artistic License. See for more information.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9882752895,
        "format_confidence":0.9538083673,
        "weight":0.031724915
    },
    {
        "url":"http:\/\/www.skf.com\/ca\/en\/products\/bearings-units-housings\/roller-bearings\/tapered-roller-bearings\/four-row-tapered-roller-bearings\/comparative-load-ratings\/index.html",
        "text":"Comparative load ratings\n\nFor rolling mill applications, load ratings are typically not calculated according to ISO\u00a0281:2007 but are calculated by a different method based on a rating life of 90\u00a0million revolutions (500\u00a0r\/min for 3\u00a0000 operating hours). As a direct comparison of these load ratings with ISO load ratings is not possible, even if they are converted for 1 million revolutions (ISO life definition) \"comparative\" load ratings calculated by the same non-ISO method are provided in the product tables.\nThese comparative load ratings may only be used together with the life and equivalent load equations specified below; they may not be used to calculate an ISO rating life.\n\nComparative life calculation\n\nThe comparative life is calculated using the comparative load rating CF as follows:\n\nLF10\u00a0=\u00a090\u00a0(CF\/PF)10\/3\n\n\nLF10h\u00a0=\u00a0(CF\/PF)10\/3\u00a0(1\u00a0500\u00a0000\/n)\n\n\ncomparative rating life, million revolutions\ncomparative rating life, operating hours\ncomparative dynamic load rating to give a rating life of 90\u00a0million revolutions, kN\nequivalent dynamic bearing load (for conditions please refer to table), kN\nconstant operating speed, r\/min\n\nFor load cases 1a) and 1b), see table, it is necessary to use the load rating for one roller row when using PFL. This load rating (for 1 row) can be obtained from\n\nCF(row)\u00a0=\u00a00,29\u00a0CF(bearing)\nSKF logo",
        "topic_id":15,
        "format_id":20,
        "topic_confidence":0.9462910295,
        "format_confidence":0.8789838552,
        "weight":0.0010092132
    },
    {
        "url":"https:\/\/www.fz-juelich.de\/ias\/jsc\/EN\/Expertise\/Support\/Software\/Nassi\/_node.html",
        "text":"Navigation and service\n\n\nThe nassi program developed at the J\u00fclich Supercomputing Centre (JSC) at the Forschungszentrum J\u00fclich is a tool to generate Nassi-Shneiderman diagrams under Unix\/X11. nassi is designed primarily for the creation of documentation, but may as well be used for source code analysis\/browsing.\n\nC and PASCAL programs and pseudo code that follows the conventions of either of these languages can be transformed into a graphical representation of the program flow. For representation and postprocessing nassi provides a convenient interface with which single diagrams can be selected, drawn on the screen, exported in several output formats and printed.\n\nA graphics editor allows layout changes of the whole diagram or of single statements or control structures. Statements and whole structures can be hidden or shifted as a block to a separate diagram. Such changes to the layout and structure of diagrams can be inserted into the source code via special comments and are then available for further treatment of the source code.\n\nFor output purposes, nassi also provides the option of generating source data of the Tgif and Xfig graphics editors in addition to screen output, Encapsulated PostScript graphics and printable PostScript files. This makes it possible to also change and extend diagrams far beyond the functionality of the built-in graphics editor. For the rapid generation of diagrams nassi provides a batch option which generates diagrams in the desired output format.\n\nFor details see the documentation or take a look at some screen shots in the tutorial.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.8772737384,
        "format_confidence":0.7546297908,
        "weight":0.0250999748
    },
    {
        "url":"https:\/\/www.adobe.com\/devnet\/svg1\/quickstart-1\/articles\/creating_non-rectangular_windows.html",
        "text":"by Joe Ward\n\nJoe Ward\n\n\n10 June 2010\n\nRectangular windows are fine, and appropriate for most applications. However, just because they are the easiest to draw doesn't mean that all your windows have to be curve impoverished. The !Square sample application creates a window based on the ellipse, rather than the rectangle. The window uses vector graphics for most of its chrome, so there are no issues with bitmap scaling to limit the window size or aspect ratio.\n\nThe !Square sample application, shown in Figure 1, illustrates how to extend the NativeWindow class to create windows with alternative visuals and behavior using the AIR APIs and the rich Flash graphics capabilities.\n\nNote: This is a sample application provided, as is, for instructional purposes.\n\nThis sample application includes the following files and classes:\n\n  \u2022 NotSquare-app.xml: The AIR application descriptor file\n  \u2022 A stub class that creates the main application window and closes\n  \u2022 !Square.fla: Flash document for use with Adobe Flash CS3 Professional.\n  \u2022 Extends the NativeWindow class\n  \u2022 Extends the Sprite class to create a content container that clips anything outside the window client area\n  \u2022 Implements the gripper controls for resizing the window\n  \u2022 Implements a button for minimizing or restoring the window\n  \u2022 Implements a button for maximizing the window\n  \u2022 Implements a button for closing the window\n  \u2022 Loads the images used for the dock and system tray icons\n  \u2022 A class providing some sample content for the window\n  \u2022 A visual object\n  \u2022 Simulates spring forces\n  \u2022 SourceViewer.js: Implements a source code browser using an HTML window\n  \u2022 Sample AIR icon files and bitmap graphics used by the window chrome\n\nTesting the application\n\nLaunch the !Square application (!Square.air). Resize the window using any of the eight grippers along the window drag bar. Move the window using the drag bar. Drag the white disk to the edge of the window to observe that any part of the disk that is outside the window client area is properly clipped.\n\nUnderstanding the code\n\nThe !Square application uses several graphics functions not specific to AIR. For more information about these functions, see the ActionScript 3.0 Language Reference.\n\nExtending the NativeWindow class\n\nThe RoundWindow class extends the AIR NativeWindow class to specialize the constructor and to define the methods and properties to draw its window chrome. Because you cannot use a custom class for the initial application window, the !Square example uses the initial window primarily to launch an instance of the RoundWindow class to serve as the main application window. The initial window created by AIR is never made visible and is closed once initialization is complete.\n\nThe constructor of the RoundWindow class creates its own NativeWindowInitOptions object and uses it to create the underlying native window. The constructor then creates the window chrome elements and activates the window to make it visible and active.\n\npublic function RoundWindow(title:String=\"\"){ var initOptions:NativeWindowInitOptions = new NativeWindowInitOptions(); initOptions.systemChrome = NativeWindowSystemChrome.NONE; initOptions.transparent = true; super(initOptions); this.minSize = new Point(350,350); bounds = new Rectangle(0,0,viewWidth,viewHeight); this.title = title; stage.align = StageAlign.TOP_LEFT; stage.scaleMode = StageScaleMode.NO_SCALE; stage.addChild(clippedStage); \/\/Put buttons on conventional side of window if(isWindows()){ closeButton = new CloseButton(287); minRestoreButton = new MinRestoreButton(283.5); maxButton = new MaxButton(280); } else { closeButton = new CloseButton(252); minRestoreButton = new MinRestoreButton(255.5); maxButton = new MaxButton(259); } addWindowDressing(); addEventListener(NativeWindowBoundsEvent.RESIZE,onBoundsChange); draw(viewWidth,viewHeight); activate(); }\n\nDrawing elliptical borders\n\nThe border of the window is drawn using standard Flash drawing functions provided by the Graphics class, which can be accessed through the graphics property of any display object. The border is composed of three rings. When you draw overlapping figures between the calls to beginFill() and endFill(), only the difference between the figures, in this case ellipses, will be filled. The following function draws the three rings by drawing two ellipses for each ring:\n\nwith({ clear(); beginFill(bevelColor,1); drawEllipse(0,0,viewWidth,viewHeight); drawEllipse(4,4,viewWidth-8,viewHeight-8); endFill(); beginFill(borderColor,1); drawEllipse(4,4,viewWidth-8,viewHeight-8); drawEllipse(16,16,viewWidth-32,viewHeight-32); endFill(); beginFill(bevelColor,1); drawEllipse(16,16,viewWidth-32,viewHeight-32); drawEllipse(20,20,viewWidth-40,viewHeight-40); endFill(); }\n\nDrawing a section of an ellipse\n\nBecause the Graphics class does not have a function for drawing just a piece of an elliptical arc, drawing the resizing gripper onto the border is actually more challenging than drawing the border itself. You could use the curveTo() method, but calculating the proper control point locations to match the elliptical border has its own mathematical challenges. For this task, !Square uses a simpler polyline technique based on the parametric equation of the ellipse. A point on the border is calculated based on the height and width and angle from the midpoint of the ellipse. Then the next point is calculated by increasing the angle a small amount and a line is drawn between them. This process is repeated until the desired arc is drawn.\n\nFigure 2 shows the formula that can be used to calculate a point along the ellipse, given the angle, and the width and the height of the ellipse.\n\nThe grippers are arranged around the border at preset angles, so the angle is known. The width and height of the ellipse is the width and height of the window for the outer edge of the window border (viewWidth and viewHeight), and the width and height minus the thickness of the border for the inner edge of the window border.\n\nThe drawing routine uses the following formula to calculate the x and y coordinates of the four corners of the gripper, points A, B, C, and D:\n\nvar A:Point = new Point(); A.x = Math.cos(startAngle) * viewWidth\/2; A.y = Math.sin(startAngle) * viewHeight\/2; var B:Point = new Point(); B.x = Math.cos(startAngle) * (viewWidth-40)\/2; B.y = Math.sin(startAngle) * (viewHeight-40)\/2; var C:Point = new Point(); C.x = Math.cos(stopAngle) * (viewWidth-40)\/2; C.y = Math.sin(stopAngle) * (viewHeight-40)\/2; var D:Point = new Point(); D.x = Math.cos(stopAngle) * viewWidth\/2; D.y = Math.sin(stopAngle) * viewHeight\/2;;\n\nThe start and stop angles are calculated by adding and subtracting half the angular width of the gripper from the preset gripper angle. Defining the length with angles rather than a distance makes the math a bit easier and also provides a more pleasing effect when the window is resized since the gripper length stays proportional to the border diameter.\n\nvar startAngle:Number = (angleRadians - spreadRadians); var stopAngle:Number = (angleRadians + spreadRadians);\n\nThe routine next draws the shape of the gripper between the four points (see Figure 3).\n\nFirst, a straight line is drawn from A to B:\n\nmoveTo(A.x,A.y); lineTo(B.x,B.y);\n\nNext, a series of line segments is drawn between B and C. If enough segments are used, then the visual effect is indistinguishable from an actual curve. !Square uses ten segments, which seems sufficient.\n\nfor(var i:int = 1; i < 10; i++){ lineTo(Math.cos(startAngle + i * incAngle) * (viewWidth-40)\/2, Math.sin(startAngle + i * incAngle) * (viewHeight-40)\/2); }\n\nAnother straight line is drawn from C to D and the shape is closed by drawing a polyline segment from D back to A. The shape is started with the beginBitmapFill() method so when endFill() is called the area defined by the drawing commands is filled with a bitmap texture.\n\nClipping the client area\n\nThe RoundWindow class uses a sprite with a clipping mask applied as the container for its contents. Any content objects that are drawn outside the border of the window are clipped. Window chrome elements are added directly to the stage so that they are not clipped.\n\nClipping can be implemented in Flash by setting the mask property of a Sprite object with another Sprite object. The masking sprite is not drawn, but only the parts in the first sprite that fall under the shape defined by the mask's graphics commands are visible.\n\nIn !Square, the clipped sprite is defined by the ClippedStage class. This class creates a clipping mask using the familiar commands for drawing an ellipse based on the width and height of the window. Any part of an object added as a child of the ClippedStage object that falls outside the ellipse are clipped.\n\nprivate function setClipMask(bounds:Rectangle):void{;,1);,0,bounds.width,bounds.height);; }\n\nThe class also listens for resize events from the parent window and responds by redrawing the clipping mask based on the new window dimensions.\n\nprivate function onResize(event:NativeWindowBoundsEvent):void{ setClipMask(event.afterBounds); }\n\nThis type of clipping is not limited to simple shapes, so the technique can be used for any window that has areas which should be masked.\n\nResizing an elliptical window\n\nTo resize the window, the border and grippers must be redrawn based on the new width and height of the window. The resize event object includes an afterBounds property that reports the new dimensions. The width and height from this property are passed to the window draw() method.\n\nprivate function onBoundsChange(boundsEvent:NativeWindowBoundsEvent):void{ draw(boundsEvent.afterBounds.width, boundsEvent.afterBounds.height); }\n\nHandling content in an elliptical window\n\nBecause the content in the !Square window is aware of its container (the springs are attached to the border), it must react to changes in window size and shape. It does this by listening for the window resize event and recalculating the dependent variables.\n\nprivate... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9879456758,
        "format_confidence":0.7974740267,
        "weight":0.0265250302
    },
    {
        "url":"https:\/\/www.kernel.org\/doc\/html\/v4.20\/admin-guide\/mm\/userfaultfd.html",
        "text":"\n\nFor example userfaults allows a proper and more optimal implementation of the PROT_NONE+SIGSEGV trick.\n\n\nUserfaults are delivered and resolved through the userfaultfd syscall.\n\nThe userfaultfd (aside from registering and unregistering virtual memory ranges) provides two primary functionalities:\n\n  1. read\/POLLIN protocol to notify a userland thread of the faults happening\n  2. various UFFDIO_* ioctls that can manage the virtual memory regions registered in the userfaultfd that allows userland to efficiently resolve the userfaults it receives via 1) or to manage the virtual memory in the background\n\nThe real advantage of userfaults if compared to regular virtual memory management of mremap\/mprotect is that the userfaults in all their operations never involve heavyweight structures like vmas (in fact the userfaultfd runtime load never takes the mmap_sem for writing).\n\nVmas are not suitable for page- (or hugepage) granular fault tracking when dealing with virtual address spaces that could span Terabytes. Too many vmas would be needed for that.\n\nThe userfaultfd once opened by invoking the syscall, can also be passed using unix domain sockets to a manager process, so the same manager process could handle the userfaults of a multitude of different processes without them being aware about what is going on (well of course unless they later try to use the userfaultfd themselves on the same region the manager is already tracking, which is a corner case that would currently return -EBUSY).\n\n\nWhen first opened the userfaultfd must be enabled invoking the UFFDIO_API ioctl specifying a uffdio_api.api value set to UFFD_API (or a later API version) which will specify the read\/POLLIN protocol userland intends to speak on the UFFD and the uffdio_api.features userland requires. The UFFDIO_API ioctl if successful (i.e. if the requested uffdio_api.api is spoken also by the running kernel and the requested features are going to be enabled) will return into uffdio_api.features and uffdio_api.ioctls two 64bit bitmasks of respectively all the available features of the read(2) protocol and the generic ioctl available.\n\nThe uffdio_api.features bitmask returned by the UFFDIO_API ioctl defines what memory types are supported by the userfaultfd and what events, except page fault notifications, may be generated.\n\nIf the kernel supports registering userfaultfd ranges on hugetlbfs virtual memory areas, UFFD_FEATURE_MISSING_HUGETLBFS will be set in uffdio_api.features. Similarly, UFFD_FEATURE_MISSING_SHMEM will be set if the kernel supports registering userfaultfd ranges on shared memory (covering all shmem APIs, i.e. tmpfs, IPCSHM, \/dev\/zero MAP_SHARED, memfd_create, etc).\n\nThe userland application that wants to use userfaultfd with hugetlbfs or shared memory need to set the corresponding flag in uffdio_api.features to enable those features.\n\nIf the userland desires to receive notifications for events other than page faults, it has to verify that uffdio_api.features has appropriate UFFD_FEATURE_EVENT_* bits set. These events are described in more detail below in \u201cNon-cooperative userfaultfd\u201d section.\n\nOnce the userfaultfd has been enabled the UFFDIO_REGISTER ioctl should be invoked (if present in the returned uffdio_api.ioctls bitmask) to register a memory range in the userfaultfd by setting the uffdio_register structure accordingly. The uffdio_register.mode bitmask will specify to the kernel which kind of faults to track for the range (UFFDIO_REGISTER_MODE_MISSING would track missing pages). The UFFDIO_REGISTER ioctl will return the uffdio_register.ioctls bitmask of ioctls that are suitable to resolve userfaults on the range registered. Not all ioctls will necessarily be supported for all memory types depending on the underlying virtual memory backend (anonymous memory vs tmpfs vs real filebacked mappings).\n\nUserland can use the uffdio_register.ioctls to manage the virtual address space in the background (to add or potentially also remove memory from the userfaultfd registered range). This means a userfault could be triggering just before userland maps in the background the user-faulted page.\n\nThe primary ioctl to resolve userfaults is UFFDIO_COPY. That atomically copies a page into the userfault registered range and wakes up the blocked userfaults (unless uffdio_copy.mode & UFFDIO_COPY_MODE_DONTWAKE is set). Other ioctl works similarly to UFFDIO_COPY. They\u2019re atomic as in guaranteeing that nothing can see an half copied page since it\u2019ll keep userfaulting until the copy has finished.\n\n\nQEMU\/KVM is using the userfaultfd syscall to implement postcopy live migration. Postcopy live migration is one form of memory externalization consisting of a virtual machine running with part or all of its memory residing on a different node in the cloud. The userfaultfd abstraction is generic enough that not a single line of KVM kernel code had to be modified in order to add postcopy live migration to QEMU.\n\nGuest async page faults, FOLL_NOWAIT and all other GUP features work just fine in combination with userfaults. Userfaults trigger async page faults in the guest scheduler so those guest processes that aren\u2019t waiting for userfaults (i.e. network bound) can keep running in the guest vcpus.\n\nIt is generally beneficial to run one pass of precopy live migration just before starting postcopy live migration, in order to avoid generating userfaults for readonly guest regions.\n\nThe implementation of postcopy live migration currently uses one single bidirectional socket but in the future two different sockets will be used (to reduce the latency of the userfaults to the minimum possible without having to decrease \/proc\/sys\/net\/ipv4\/tcp_wmem).\n\nThe QEMU in the source node writes all pages that it knows are missing in the destination node, into the socket, and the migration thread of the QEMU running in the destination node runs UFFDIO_COPY|ZEROPAGE ioctls on the userfaultfd in order to map the received pages into the guest (UFFDIO_ZEROCOPY is used if the source page was a zero page).\n\nA different postcopy thread in the destination node listens with poll() to the userfaultfd in parallel. When a POLLIN event is generated after a userfault triggers, the postcopy thread read() from the userfaultfd and receives the fault address (or -EAGAIN in case the userfault was already resolved and waken by a UFFDIO_COPY|ZEROPAGE run by the parallel QEMU migration thread).\n\nAfter the QEMU postcopy thread (running in the destination node) gets the userfault address it writes the information about the missing page into the socket. The QEMU source node receives the information and roughly \u201cseeks\u201d to that page address and continues sending all remaining missing pages from that new page offset. Soon after that (just the time to flush the tcp_wmem queue through the network) the migration thread in the QEMU running in the destination node will receive the page that triggered the userfault and it\u2019ll map it as usual with the UFFDIO_COPY|ZEROPAGE (without actually knowing if it was spontaneously sent by the source or if it was an urgent page requested through a userfault).\n\nBy the time the userfaults start, the QEMU in the destination node doesn\u2019t need to keep any per-page state bitmap relative to the live migration around and a single per-page bitmap has to be maintained in the QEMU running in the source node to know which pages are still missing in the destination node. The bitmap in the source node is checked to find which missing pages to send in round robin and we seek over it when receiving incoming userfaults. After sending each page of course the bitmap is updated accordingly. It\u2019s also useful to avoid sending the same page twice (in case the userfault is read by the postcopy thread just before UFFDIO_COPY|ZEROPAGE runs in the migration thread).\n\nNon-cooperative userfaultfd\n\nWhen the userfaultfd is monitored by an external manager, the manager must be able to track changes in the process virtual memory layout. Userfaultfd can notify the manager about such changes using the same read(2) protocol as for the page fault notifications. The manager has to explicitly enable these events by setting appropriate bits in uffdio_api.features passed to UFFDIO_API ioctl:\n\nenable userfaultfd hooks for fork(). When this feature is enabled, the userfaultfd context of the parent process is duplicated into the newly created process. The manager receives UFFD_EVENT_FORK with file descriptor of the new userfaultfd context in the uffd_msg.fork.\nenable notifications about mremap() calls. When the non-cooperative process moves a virtual memory area to a different location, the manager will receive UFFD_EVENT_REMAP. The uffd_msg.remap will contain the old and new addresses of the area and its original length.\nenable notifications about madvise(MADV_REMOVE) and madvise(MADV_DONTNEED) calls. The event UFFD_EVENT_REMOVE will be generated upon these calls to madvise. The uffd_msg.remove will contain start and end addresses of the removed area.\nenable notifications about memory unmapping. The manager will get UFFD_EVENT_UNMAP with uffd_msg.remove containing start and end addresses of the unmapped area.\n\nAlthough the UFFD_FEATURE_EVENT_REMOVE and UFFD_FEATURE_EVENT_UNMAP are pretty similar, they quite differ in the action expected from the userfaultfd manager. In the former case, the virtual memory is removed, but the area is not, the area remains monitored by the userfaultfd, and if a page fault occurs in that area it will be delivered to the manager. The proper resolution for such page fault is to zeromap the faulting address. However, in the latter case, when an area is unmapped, either explicitly (with munmap() system call), or implicitly (e.g. during mremap()), the area is removed and in turn the userfaultfd context for such area disappears too and the manager will not get further userland page faults from the removed area. Still, the notification is required in order to prevent manager from using UFFDIO_COP... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9611905813,
        "format_confidence":0.9870917797,
        "weight":0.0328319649
    },
    {
        "url":"https:\/\/docs.tenable.com\/tenableio\/vulnerabilitymanagement\/Content\/Analysis\/Classic\/DeleteAssets.htm",
        "text":"Delete Assets (Classic Interface)\n\nYou can delete assets as a standard or administrative user.\n\nWhen you delete an asset,\n\n  \u2022 removes the asset from the default view of the assets table.\n  \u2022 deletes vulnerability data associated with the asset.\n  \u2022 stops matching scan results to the asset.\n\nDeleting an asset does not immediately subtract the asset from your licensed assets count. Deleted assets continue to be included in the count until they automatically age out of your licensed assets count after 90 days.\n\nYou cannot reverse the deletion of an asset. If you mistakenly delete an asset, add it to the system by scanning the asset again.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.9300832748,
        "format_confidence":0.9597594738,
        "weight":0.0050298371
    },
    {
        "url":"http:\/\/www.sjgames.com\/errata\/gurps\/basic_3r.html",
        "text":"Steve Jackson Games GURPS \u2013 Generic Universal RolePlaying System\n\nERRATA \u2013 GURPS Basic Set (Third Edition Revised, First Printing) \u2013 Updated February 10, 1998\n\nCopyright \u00a9 by Steve Jackson Games Incorporated.\n\nAll these changes should also be marked in earlier copies, except as noted.\n\nP. 26. Under \"Good\" Disadvantages, the references to Honesty should be to Truthfulness instead.\n\nP. 44. Under the Skill Defaults: Using Skills You Don't Know sidebar, insert this paragraph in between the third and fourth paragraphs: \"Default levels in skills do not carry any of the skills' special benefits with them. Special benefits would include damage bonuses, dodge bonuses, 2\/3-skill parry, unpenalized off-hand use, move bonuses, and so on.\"\n\nP. 49. Battlesuit\/TL is Physical\/Average.\n\nP. 52. Under the Spear Thrower skill, delete the line that says to use ST+5 when using a Spear Thrower for determining damage and range. Spear Thrower now has a separate entry in the weapons tables.\n\nP. 63. Carousing defaults to HT-4.\n\nP. 65. Demolition defaults to Underwater Demolition-2, as well as the listed defaults.\n\nP. 68. Underwater Demolition should be listed as a TL skill.\n\nP. 73. In the Weapon Effects sidebar, delete \"(other attacks may do zero damage)\" under Thrusting Attacks.\n\nP. 74. Change the damage done by ST 4 or less in the Basic Damage Table. ST 4 has a basic damage of 1d-6 (thrusting and swinging); ST 3 has 1d-7; ST 2 has 1d-8; and so on.\n\nP. 74. The line at the bottom of the chart should read \"See p. 248 for a chart covering higher levels of ST.\"\n\nP. 90. We dropped a line from the top of the page in this and the previous printing. The first line should read \"Picking Things Up In Combat: In combat, a light item is picked up with the\"\n\nP. 97. Under Concentrate, the second sentence should read \"If the character is hurt, knocked down, forced to use an active defense, or otherwise distracted, he must make a Will-3 roll to maintain his concentration.\" Delete the third sentence.\n\nP. 98. In the attack penalties sidebar, the reference for strange position should be to p. 123, not p. 120.\n\nP. 101. Under Punching, add a note that its Reach is C,1. Under Kicking, add a note that its Reach is 1 without Karate training.\n\nThe first sentence under Kicking should read \"A kick is treated exactly like a punch, except your skill is DX-2, Brawling-2, or Karate-2, and you do straight Thrust\/Crush damage - or Thrust+1 if you are wearing heavy boots or something similar.\"\n\nP. 106. Under Step and Concentrate, the second paragraph should read \"If the character is hurt, knocked down, forced to use and active defense, or otherwise distracted, he must make a Will-3 roll to maintain his concentration.\"\n\nP. 109. In the Massive Damage sidebar, change the comment \"is lost and does not affect the victim\" to \"cripples the limb instead of dealing additional hits of damage.\"\n\nP. 111. Change the fourth sentence under Grapple to read \"Grappling does no damage, but the foe is at -4 DX and may not leave until he breaks free, or you let go.\"\n\nP. 119. Under Single-Shot Weapons change the last two sentences to \"This penalty is applied to a second or subsequent shot from the same gun, unless there is a minimum one-second pause between shots. The penalty is doubled if your ST is below the minimum listed for the weapon, and the minimum pause becomes two seconds.\"\n\nP. 119. In the Revised edition only, change the reference to GURPS Space in the Power Supplies sidebar to p. 247.\n\nP. 127. Under Crippling Injuries, change \"taking hits equal to\" to \"being hit for\". Similarly, under Knockdown, change \"Anyone who takes damage\" to \"Anyone who is hit for\".\n\nP. 131. Under Falling Objects change the line that begins \"For simplicity\" to read \"For simplicity, when dealing with falling inanimate objects. . . .\" Falling beings reach terminal velocity at 50 yards, as described under Falling, above.\n\nP. 139. In the first paragraph after the diagram, the reference to \"flying\" skill should be to Piloting skill.\n\nP. 159. In the description of Destroy Water, delete \"(but see the next spell)\" from the last sentence. The Dehydrate spell does not follow in Basic, as it does in Magic.\n\nP. 167. The One-Skill Cost for Psi Sense should be 1.\n\nP. 175-176. We intended to change the Limitations rules in the sidebar to match those in GURPS Psionics. We didn't. If you have GURPS Psionics, follow it in case of discrepancies.\n\nP. 195. In the Loyalty Checks sidebar, Rescue needs to be changed. When a hireling is rescued, make a Reaction Roll (not a Loyalty Check). This roll should be at +3 or more, depending on the nature of the rescue. The hireling's loyalty is the result of the Reaction Roll, or his original loyalty, whichever is higher.\n\nP. 201. Under Ranged Attack Modifiers: Opportunity fire, the penalty for watching one hex is -2. The penalty for watching two hexes is still -4.\n\nP. 207. Replace the Spear Thrower entry in the weapons table with this one:\n\nWeaponTypeAmountRangesCostWeightMin ST\nSPEAR THROWER (DX-4 or Spear Throwing-4)\nAtlatl$202 lbs.\nw\/Dartimpsw-1111STx3STx4$201 lb.none\nw\/Javelinimpsw+1113STx2STx3$302 lbs.7\nw\/Spearimpsw+3122STx1.5STx2$404 lbs.9\n\nP. 245. Computer Hacking defaults to Computer Programming-4, not Computer Operation-8. It is a \/TL skill.\n\nP. 245. In Cryptanalysis, first paragraph, last sentence should read: \".\u00a0.\u00a0.\u00a0to unsophisticated ciphers.\"\n\nP. 253. The entry for Disbelieve sends you to Free Actions, p. 107. Disbelieve is not a free action; see p. 106.\n\nP. 254. Add an index entry: Freight Handling skill, p. 46.",
        "topic_id":11,
        "format_id":20,
        "topic_confidence":0.9930095673,
        "format_confidence":0.9482047558,
        "weight":0.0007447467
    },
    {
        "url":"https:\/\/support.quest.com\/technical-documents\/metalogix-controlpoint\/8.2\/user-guide\/14",
        "text":"Chat now with support\nChat with Support\n\nMetalogix ControlPoint 8.2 - User Guide\n\nPreface Getting Started with ControlPoint Using Discovery to Collect Information for the ControlPoint Database Cache Searching for SharePoint Sites Managing SharePoint Objects\nAccessing SharePoint Pages Viewing Properties of an Object within the SharePoint Hierarchy Creating Dashboards for Monitoring Statistics within Your SharePoint Farm Setting Object Properties Managing Site Collection and Site Features Managing Audit Settings Creating and Managing SharePoint Alerts Setting ControlPoint Alerts Managing Metadata Copying and Moving SharePoint Objects (Version 8.0 and Earlier) Moving a Site Collection to Another Content Database Deleting Sites Deleting Lists Archiving Audit Log Data Before Moving or Deleting a Site Collection Duplicating a Workflow Definition from one List or Site to Others Removing a Workflow from One or More Lists (or Sites)\nUsing ControlPoint Policies to Control Your SharePoint Environment Managing SharePoint User Permissions Data Analysis and Reporting\nSpecifying Parameters for Your Analysis Analysis Results Display Generating a SharePoint Summary Report Analyzing Activity Analyzing Object Properties Analyzing Storage Analyzing Content Generating a SharePoint Hierarchy Report Analyzing Trends Auditing Activities and Changes in Your SharePoint Environment Analyzing SharePoint Alerts Analyzing ControlPoint Policies Analyzing Users and Permissions The ControlPoint Task Audit Viewing Logged Errors\nScheduling a ControlPoint Operation Saving, Modifying and Executing Instructions for a ControlPoint Operation Using the ControlPoint Governance Policy Manager (SharePoint 2010 and Later) Using Sensitive Content Manager to Analyze SharePoint Content for Compliance Using ControlPoint Sentinel to Detect Anomalous Activity Default Menu Options for ControlPoint Users About Us\n\nViewing Properties of an Object within the SharePoint Hierarchy\n\nFrom the SharePoint Hierarchy, you can access an at-a-glance summary of key properties for the farm, a Web application, a site collection, or a sitea site collection or a site.\n\nYou can also retrieve the following statistics for an object at any level of the hierarchy:\n\n\u00b7the total number of unique users who have permissions, and\n\n\u00b7the total number of users with activity over the past 30 days.\n\nTo view an object's properties:\n\n1From the SharePoint Hierarchy, select the object whose properties you want to view.\n\nNOTE: \u00a0You can only view properties for a single object at a time (that is, the multiple selection is unavailable).\n\n2Right-click and select Properties.\n\nFarm Properties\n\nFarm-level properties are maintained in Operations section of the SharePoint Central Administration site .\n\nProperties FARM\n\nWeb Application Properties\n\nWeb application-level properties are maintained in the Application Management section of the SharePoint Central Administration pages.\n\nProperties WEB APP\n\nSite Collection Properties\n\nSite Collection properties are maintained in the SharePoint Site Collection Administration area.\n\n\nSite Properties\n\nSite-level properties are maintained in SharePoint Site Settings pages.\n\nProperties SITE\n\nTo view user permissions and activity totals (for the selected level of the SharePoint Hierarchy):\n\nClick [Calculate Totals].\n\nWhen the values have been calculated, the following information displays at the top of the Properties dialog:\n\n\u00b7Total Users with Permissions\n\n\u00b7Total Active Users (last 30 days)\n\nNOTE: \u00a0These totals reflect unique users. (That is, any user who has more than one set of permissions to a site is only counted once).\n\nProperties USER TOTALS\n\n\u00b7Total Users with Permissions includes: \u00a0\n\n\u00a7Web application Service Accounts\n\n\u00a7Users granted permissions through Web application policies\n\n\u00a7Site Collection Administrators\n\n\u00a7Users within Active Directory groups to which the ControlPoint Service Account has access (that is, within the same domain or forest, in a different domain\/forest for which with a two-way trust exists, or in a different domain\/forest with a one-way outgoing trust that ControlPoint can authenticate via the ControlPoint Manage Forest Access feature. \u00a0Disabled Active Directory accounts are included in this total. \u00a0If an Active Directory user has been renamed but still has permissions in SharePoint under the old name, each name will be counted as a separate user.\n\n\u00b7Excluded from this total are built-in groups and special accounts, such as nt authority\\authenticated users (or any account that begins with \"nt authority\") and sharepoint\\system, and users granted permissions via augmented Claims or alternate authentication providers.\n\nNOTE: \u00a0Total Users with Permissions uses data recorded in the ControlPoint Service Database (xcAdmin), and is current as of the last Discovery run. (The actual number of users within Active Directory groups is counted in real-time, however.)\n\nTotal Active Users uses 30 days worth of activity data that is collected by the SharePoint usage job(s), and is as current as the last time the job(s) ran.\n\n\nCreating Dashboards for Monitoring Statistics within Your SharePoint Farm\n\nThe ControlPoint Configuration site contains the following custom lists that ControlPoint Application Administrators can use to create \"dashboards\" of Web Parts for monitoring statistics within a SharePoint farm:\n\n\u00b7Farm Statistics - Individual farm-wide metrics that are typically presented individually.\n\nFarm Statistics\n\n\u00b7Web Application Statistics - Rows of data about each Web application. \u00a0Depending on your needs, you can present aggregate data (column sums, maximums, standard deviation, and so on).\n\nWeb App Statistics\n\n\u00b7Site Collection Statistics - Rows of data about each site collection which, like Web Application Statistics, can be customized to present aggregate data as well as key performance indicators (KPIs).\n\nSC Statistics\n\nNOTE: \u00a0by default, the Site Collections Statistics list includes all site collections in the farm. \u00a0ControlPoint Application Administrators can, however, eliminate from the list groups of site collections that they do not want to monitor by excluding the Web applications that host them as described in the ControlPoint Administration Guide. \u00a0This may be useful, for example, if the number of site collections in your farm exceeds the SharePoint recommended maximum number of list items.\n\nYou can configure an additional list, the Web Statistics KPI\/Status \u00a0List, to track key performance indicators for selected statistics if:\n\n\u00b7for SharePoint 2010:\n\n\u00a7you have a SharePoint Server Enterprise-based environment, and\n\n\u00a7the feature \"SharePoint Server Enterprise Site Features\" has been activated on the ControlPoint Configuration root site.\n\n\u00b7for SharePoint 2007:\n\n\u00a7you have a MOSS Enterprise-based environment, and\n\n\u00a7the feature \"Office Services Enterprise Site Features\" has been activated on the ControlPoint Configuration root site.\n\nNOTE: \u00a0This list type has been deprecated as of SharePoint 2013.\n\nHow Statistics Lists are Populated\n\nStatistics lists are populated as part of the nightly Full Discovery timer job. \u00a0When this job runs, the lists are cleared and re-populated with current data.\n\nNOTE: \u00a0Because of the way in which they are populated, if any of these lists are copied or moved, statistics data will become static (that is, it will not be updated) in the new location.\n\nTo create a statistics dashboard on the ControlPoint Configuration root site main page:\n\nNOTE: \u00a0The steps below are intended to provide introductory guidelines for creating dashboards on the ControlPoint Configuration root site main page. \u00a0Consult your SharePoint documentation and other available resources for more detailed instructions and\/or guidance in using alternate tools and techniques.\n\n1From the ControlPoint Configuration site main page, choose Site Actions > Edit Page.\n\n2Click Add a Web Part, then choose the list(s) that you want to add as Web Parts to that area of the page.\n\nEXCEPTION: The Web Statistics KPI List is configured and added to the page using a different method. \u00a0See \"To configure the Web Statistics KPI, \" following.\n\nDashboardsADD WP\n\nYou can either use the default view that has been defined for the list or create a custom view.\n\nYou can, for example, change the display order of columns; display a subset of columns; display columns that are not included in the default view; add a calculated column; display column totals.\n\n\u00b7Example: \u00a0Site Collection Statistics with calculated columns to track size and storage utilization.\n\n\n\nSC Statistics CUSTOM\n\n\n\u00b7Example: \u00a0Farm Statistics filtered to show only \"high priority\" metrics.\n\n\nFarm Stats CUSTOM\n\nTo configure the Web Statistics KPI\/Status List (SharePoint 2007 and 2010):\n\nREMINDER: \u00a0To use KPI lists and Web Parts, you must have an Enterprise-based environment \u00a0with Enterprise Site Features activated on the ControlPoint Configuration root site. \u00a0This list type has been deprecated as of SharePoint 2013.\n\n1Create a new Status list (or KPI list \u00a0as it is known in SharePoint 2007) entitled \"Web Statistics KPI\" (or another name of your choosing).\n\n2Create indicators for the Web Statistics KPI List as follows:\n\na)In the ControlPoint Configuration root site, open the Web Statistics KPI List.\n\n\n\u00a7New > Indicator using data in SharePoint list (SharePoint 2007)\n\n\n\u00a7New > SharePoint List based Status Indicator (SharePoint 2010)\n\nDashboard KPI LIST\n\nc)Complete the appropriate fields on the New Item page (including the List URL of the list containing the statistic for which you want to create an indicator). \u00a0Consult your SharePoint documentation for complete instructions.\n\nDashboard KPI CREATE\n\nd)Complete substeps b and c for each indicator you want to create.\n\nOnce the list has been set up, add a KPI Web Part to the dashboard, specifying the Web Statistics KPI List as the Indicator List.\n\nDashboard KPI\n\n\nSetting Object Properties\n\nYou can use ControlPoint to set properties for site collections, sites, and lists.\n\nIn a multi-farm ... (truncated)",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.9215379357,
        "format_confidence":0.7677069306,
        "weight":0.0040233422
    },
    {
        "url":"https:\/\/www.genosphere-biotech.com\/technical-corner\/custom-genes\/sequence-optimization-consideration\/",
        "text":"Sequence Optimization Consideration\n\nSequence Optimization Consideration\n\nHeterologous protein expression\n\nProtein heterologous expression project can reveal to be a demanding process. Among the many factors that may affect successful expression, that of the actual coding sequence utilized has been shown in many instances to be a central issue. You may benefit from re-designing or creating de novo an expression-prone DNA sequence from he protein sequence.\n\nPoor or no expression, troncated proteins, amino-acids misincorporation are the most common consequences of codon bias and unbalanced tRNA pool.\u00a0\n\nThis issue has been extensively studied for E. coli expression and is perfectly exemplified by the arginine codons AGG and AGA -same tRNA- which are hardly ever found in E. coli highly expressed ORFs while found with significantly higher frequency in many other organisms. It would thus make better sense to replace the latter by codons used more frequently in your host organism e.g. CGT or CGC.\n\nFrequency Codons (AGG\n+ AGA) (% of all arg codons)\nExpression host\n(Highly expressed genes, Henaut and Danchin, Escherichia coli\nand Salmonella, Vol. 2, Ch. 114:2047-2066, 1996)\nE. coli\nRecombinant gene source\n(all genes frequencies, www.kazusa.or.jp\/codon\/)\nA. thaliana\nC. elegans\nD. melanogaster\nH. sapiens\nS. cerevisae\n\nGene design\n\nDesigning a gene that will express in a particular organism a recombinant protein boils down to choosing the most appropriate triplet for each amino acids. With a ratio of 64 codons to 20 aa plus termination there is quite some flexibility to include other constraints in addition to that of codon bias adjustment such as:\u00a0\n\n  \u2022 Gene & protein engineering: Addition or removal of specific motives (e.g. restriction sites), tags for purification, multiple stops, etc.\u00a0\n  \u2022 Gene manufacturing: Maintain average GC content, avoid long repeats, palindroms, etc.",
        "topic_id":19,
        "format_id":20,
        "topic_confidence":0.9899161458,
        "format_confidence":0.8583157063,
        "weight":0.0034887291
    },
    {
        "url":"https:\/\/hst-docs.stsci.edu\/stisihb\/chapter-12-special-uses-of-stis\/12-5-high-signal-to-noise-ratio-observations",
        "text":"12.5 High Signal-to-Noise Ratio Observations\n\nThe maximum achievable signal-to-noise (S\/N) ratio of STIS observations for well exposed targets is, in general, limited by the S\/N ratio and stability of the flat fields. CCD flat-field observations are obtained monthly. Ultimately, CCD reference flats in the pipeline should have an effective illumination of up to 106 e\/pix. Thus, it should be possible to achieve a S\/N ratio of several hundred over larger spatial scales given sufficient source counts. The limitation is the temporal stability of the CCD reference flats, which show variations of a few tenths of a percent. Dithering techniques can and should be considered for high S\/N CCD observations (see Section 11.3). The realizable S\/N ratio for spectroscopy will be less in the far red due to fringing, unless appropriate fringe flats are applied (see the caveats on long-wavelength spectroscopy in the red in Section 7.2.4).\n\nThe S\/N ratio of MAMA flat fields is limited by the long integration times needed to acquire them and the limited lamp lifetimes. (See Section 16.1). S\/N ratios of ~100:1 should routinely be achievable for spectroscopic observations of bright sources with the MAMAs if supported by counting statistics. If your program requires high S\/N ratios, we recommend using some form of dithering (described below) and co-adding the spectrograms to ameliorate the structure in the flat fields.\n\nKaiser et al. (1998, PASP, 110, 978) and Gilliland (STIS ISR 1998-16) reported quite high S\/N ratios for spectrograms of bright standard stars obtained during a STIS commissioning program. The realizable S\/N ratio depends on the technique used to correct for the flat-field variations, as shown in Table 12.2. The S\/N ratios quoted are for wavelength bins from an extraction box of 2 11 lowres pixels (2 in AXIS1 or dispersion, 11 in AXIS2 or across the dispersion). In the table, the Poisson limit is just the S\/N ratio that would be expected on the basis of counting statistics alone; \"No Flat\" means the realized S\/N ratio without applying any flat field at all to the data; \"Reference Flat\" means the realized S\/N ratio after applying the best available reference flat, and the \"Full FP-SPLIT Solution\" is discussed under Section 12.5.2 below. Clearly, S\/N ratios in excess of 100:1 per resolution element are well within the capabilities of the MAMAs for spectroscopy.\n\nTable 12.2: Results of S\/N Ratio Tests with STIS MAMAs in Orbit.\n\n\nPoisson Limit\n\nNo Flat\n\nReference Flat\n\nFull FP-Split Solution1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Results obtainable in echelle modes using the FP-SPLIT slits and an iterative solution for the spectrogram and flat field.\n2 Results obtained using the FP-SPLIT slits and simply shifting and co-adding the spectrograms after flat fielding.\n3 Results obtained using the FP-SPLIT slits and simply shifting and co-adding the spectrograms after flat fielding.\n\n12.5.1 Dithering\n\nIn first-order spectroscopic modes, improved S\/N ratios can be achieved by stepping the target along the slit, taking separate exposures at each location, which are subsequently shifted and added in post-observation data processing (PATTERN=STIS-ALONG-SLIT, see Section 11.3). This stepping, or dithering, in the spatial direction effectively smooths the detector response over the number of steps, achieving a reduction of pixel-to-pixel nonuniformity by the square root of the number of steps, assuming the pixel-to-pixel deviations are uncorrelated on the scale of the steps. In imaging modes, the same dithering can be done in two dimensions, i.e., the steps need not be along a straight line (see Section 11.3.5). For echelle modes, stepping along the slit is possible only with a long echelle slit (e.g., the 6X0.2 or 52X0.1 apertures, or one of the available but unsupported long-slit apertures), but see Section 12.2 above, and note the ameliorating effects of Doppler smearing as noted below. In practice, using the FP-SPLIT slits (see Section 12.5.2) provides a better means of dithering echelle observations.\n\nIn a slitless or wide-slit mode, stepping along the dispersion direction provides another method to achieve high S\/N ratio data. Data so obtained permit, at least in principle, an independent solution for spectrogram and flat field, but at a cost of lower spectral resolution and line profile confusion due to the wings of the LSFs transmitted through a wide slit (see Section 13.7). Such an approach for STIS data has not been attempted as of this writing.\n\nThe three scanned STIS echelle modes (E140H, E230H, and E230M), have a number of secondary wavelength settings defined with broad overlap in their wavelength coverage. Using two or more of these overlapping wavelength settings is another simple way to move a given feature to different parts of the detector.\n\n12.5.2 FP-SPLIT Slits for Echelle Observations\n\nA special kind of dithering in the spectral direction is possible for echelle mode observations with one of two sets of fixed-pattern (or FP-SPLIT) slits. These slit sets are each comprised of a mask with five apertures that are all either 0.2X0.2 or 0.2X0.06 in size. A schematic of the configuration is shown in Figure 12.4. During a visit, the target is moved from one aperture to another, and the slit wheel is repositioned, so that the spectrogram is shifted (relative to the detector pixels) along the dispersion direction only. The slits are spaced to place the spectrogram at different detector locations, so that flat-field variations can be ameliorated by co-adding many such spectrograms. The FP-SPLIT slits can be a good choice for obtaining high S\/N ratio echelle data, since it is usually not possible to dither in the spatial direction. However, since S\/N=100 is routinely achieved using the normal echelle apertures, the FP-SPLIT slits are rarely used.\n\nFigure 12.4: Schematic of the STIS Fixed-Pattern Slit Configuration.\n\nAXIS1 corresponds to the dispersion direction, and AXIS2 to the spatial direction. Dimensions are not to scale.\nWith echelle modes, Doppler-induced spectral shifts move the spectrogram on the detector. The STIS flight software automatically applies an onboard compensation for Doppler motion for echelle and MAMA medium resolution, first-order data taken in ACCUM mode (see Chapter 11). The MAMA control electronics correct (to the nearest highres pixel) the location of each event for the Doppler shift induced by the spacecraft motion prior to updating the counter in the image being collected. Thus, the flat-field correction for any image pixel would be an appropriately weighted average over a small range of nearby pixels and the effect of spacecraft-induced Doppler shifts is therefore to naturally provide some smoothing over the flat fields in the echelle modes.\n\nThe source of the Doppler-induced spectral shifts during an exposure is the variation of the projected HST spacecraft velocity along the line of sight to the target. Column\u00a02 of Table 12.3 gives the maximum shift in highres pixels that would apply, based upon an HST orbital velocity of ~7.5 km\/s during an orbit. The actual shift will of course depend upon the cosine of the target latitude, i, above or below the HST orbital plane, and upon the sine of the orbital phase at which the exposure is obtained. (Note that in general the observer can predict neither the latitude nor the orbit phase of the exposures in advance with any precision.) Column\u00a03 gives, for a target lying in the HST orbital plane, the maximum duration of an exposure for which the Doppler shift will be one highres pixel or less; the actual duration will scale as sec(i), so that targets near the CVZ are scarcely affected by Doppler motion. This information on Tmax is relevant only if you are trying to derive the flat-field response simultaneously with the source spectrogram (see below) and not for the straightforward flat field and shift-and-add methodology described above.\n\nTable 12.3: Effect of Doppler Shift on Exposure Times.\n\n\nMaximum Doppler Shift (hi-res pixels)\n\nTmax1 (minutes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 For inclination i = 0; actual duration will scale as sec(i). See text for details.\n\nObtaining the Highest S\/N Ratio with the FP-SPLIT Slits\n\nAs described above, the FP-SPLIT slits have been used with the echelles to provide signal-to-noise as high as ~350 with the direct shift-and-add method. Additionally, data obtained with the FP-SPLIT slits make it possible to solve independently for the fixed-pattern (i.e., the flat-field variation) and the source spectrogram. An iterative technique for combining FP-SPLIT data was applied successfully to data obtained with GHRS (see Lambert et al., ApJ, 420, 756, 1994), based on a method described by Bagnuolo and Gies (ApJ, 376, 266, 1991). This same technique was applied by Gilliland (STIS ISR 1998-16) to STIS observations of a standard star. The S\/N ratio that was achieved with these slits is summarized in the last column of Table 12.2, which shows that the FP-SPLIT slits can offer some advantage when one is attempting to achieve the highest possible S\/N ratio. In general, though, it may be difficult to improve upon the S\/N ratio that can be achieved by simply calibrating with the standard flat field and co-adding the spectrograms.\n\nThere are a number of caveats to the use of the FP-SPLIT slits to solve independently for the spectrogram and flat field. The most notable is that the targets must be relatively bright point sources. The restriction to bright targets results both from the need to limit the duration of individual exposures to keep the Doppler-induced spectral shifts to less than one highres pixel, and from the need to have appreciable counts in the individual exposures\u2014at least in the orders of interest. Very high counts in the sum of all exposures are essential for a good (and stable) solution to both the spectrogram and the underlying flat field.\n\nIf you are using the FP-SPLIT slits to distinguish the signature of the flat field from the target spectrogram,... (truncated)",
        "topic_id":19,
        "format_id":20,
        "topic_confidence":0.9959594607,
        "format_confidence":0.9280294776,
        "weight":0.0037720893
    }
]