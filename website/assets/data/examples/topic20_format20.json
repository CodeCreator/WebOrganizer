[
    {
        "url":"http:\/\/dcx.sap.com\/1201\/fr\/dbadmin\/backup-s-3422468.html",
        "text":"\n\nSQL Anywhere 12.0.1 \u00bb SQL Anywhere Server - Database Administration \u00bb Database maintenance \u00bb SQL Anywhere high availability \u00bb Database mirroring\n\n\nDatabase mirroring modes\n\nThree operational modes are provided for database mirroring:\n\nThese modes control when and how transactions are recorded on the mirror server, and you set them by using the SET MIRROR OPTION statement to set the value of the synchronization_mode option. Synchronous mode is the default.\n\nThe sending of log pages to the copy nodes is always done asynchronously, regardless of the mode chosen.\n\nWhen choosing a synchronization mode for your database mirroring system, you must determine whether recovery speed or the state of the data is more important when failover occurs.\n\nYou can check the database mirroring mode by querying the value of the MirrorMode database property:\n\n\u00a0Synchronous mode\n\u00a0Asynchronous mode\n\u00a0Asyncfullpage mode\n\u00a0See also\n\nSynchronization states\nState information files",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.5914620161,
        "format_confidence":0.9194540977
    },
    {
        "url":"http:\/\/docs.sophos.com\/esg\/smc\/7-0\/admin\/en-us\/webhelp\/references\/ConfigurationCorporateBrowserSophosContainerAndroid.htm",
        "text":"Corporate Browser configuration (Android Sophos container policy)\n\nWith the Corporate Browser configuration you define settings for the Corporate Browser feature of the Sophos Secure Workspace app.\n\nThe Corporate Browser allows you to securely access corporate intranet pages and other allowed pages. You can define domains and bookmarks within a domain.\n\nEvery bookmarks belongs to a certain domain. When you use Add bookmark to define a bookmark, the domain entry is created automatically if it does not exist.\n\nDomain settings\n\nSetting\/Field Description\nURL The domain that you want to allow.\nAllow copy\/paste Users can copy and paste text from the Corporate Browser to other apps.\nAllow open with Users can download attachments or pass them on to other apps.\nAllow save password Users can save their passwords in the Corporate Browser.\n\nBookmark settings\n\nSetting\/Field Description\nName The name for the bookmark.\nURL The web address for the bookmark.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.6606095433,
        "format_confidence":0.9775025249
    },
    {
        "url":"https:\/\/blogs.arubanetworks.com\/solutions\/securing-the-distributed-enterprise\/",
        "text":"Securing the Distributed Enterprise\u00a0\n\nShare Post\n\nSecurity is a critical component of an SD-Branch solution for both the WAN and the LAN. Customers using SD-WAN and SD-Branch (a superset of SD-WAN that includes the branch LAN) need to know that their network and services are well protected.\n\nThe goal of zero trust, according to NIST, is to provide \u201cnetwork security paradigms that narrow defenses from wide network perimeters to individuals or small groups of resources.\u201d This is in response to the ever-heightening emphasis on cloud-based assets and remote users, as opposed to enterprise-controlled network boundaries.\n\nOne of the key tenets of zero trust assumes that any entity on the internal network may be untrusted or at worst malicious. For instance, SD-Branch looks at communications between internal devices and users and Internet resources. Within the branch, intrusion detection and protection functionality (IDS\/IPS) inspects inter- and intra-branch traffic for attacks on the network, preventing malware (and other compromised traffic) from communicating with a home based of attack such as a command-and-control server.\n\nSecurity Layers\nThe security of the Aruba SD-Branch solution is built in layers, from the hardening of the operating system to the integration with best-of-breed security partners (see\u00a0Figure 1 below).\n\nSecurity Layers in the Aruba SD-Branch Solution\n\nFigure 1. Security Layers in the Aruba SD-Branch Solution\n\nAruba\u2019s SD-Branch portfolio provides a comprehensive solution across all aspects of WAN and LAN performance and branch security. With Aruba SD-Branch, built-in integrated security provides stateful firewall, intrusion detection and prevention, deep packet inspection (DPI), web content filtering, and other policy-based security, privacy, and compliance controls, all managed from Aruba Central.\n\nFurthermore, the solution implements and mandates very robust hardening policies, which is critical since branches are directly exposed to the Internet. Finally, Aruba-based branch networks benefit from \u201cbest-of-breed\u201d security.\n\nAruba branch gateways include rich firewall functionality that can be hardened to further ensure a secure branch.\n\nSolution Components\nZero-trust security in Aruba branch and headend gateways begins with ArubaOS: a tightly hardened operating system. The functionality of ArubaOS includes:\n\n  \u2022 Secure boot: Heavily restricting communications until the gateway has received its configuration from Aruba Central. Only trusted systems can boot the device.\n  \u2022 IPsec VPN: Aruba branch gateways and headend gateways support high-performance IPsec VPN for secure overlay networking across the Internet or other untrusted networks. Aruba uses AES-256 encryption (using trusted key exchanges) for all branch-to-hub tunnels. Notably, Aruba branch gateways and headend gateways support VPN termination from client endpoints directly. In a branch, this enables employees or contractors to access internal systems, such as security cameras or Internet of Things (IoT) sensors, based on their allowed role.\n  \u2022 Role-based Stateful firewall: The Aruba Policy Enforcement Firewall (PEF) is a full, stateful firewall able to tightly control what users and devices are permitted to do, enabling application-layer security and providing separation between user roles. Roles are trusted or not based on their profiles. This gives network administrators insight into the applications running on the network and who is using them.\n  \u2022 Deep Packet Inspection (DPI) module: Includes the capacity to identify close to 3,200 known and\/or trusted applications via application fingerprinting.\n  \u2022 Intrusion detection and prevention (IDS\/IPS): This is part of an advanced threat defense, which will be detailed more in an upcoming blog. This is preventing communications from untrusted systems.\n  \u2022 Web content and reputation filtering: The Aruba branch gateway uses Webroot cloud-based machine learning classification technology. Websites are classified for content-based filtering. The reputation of all public IP address space is monitored to detect and block threats such as spam, exploits, botnets, phishing, proxies, and mobile threats. Geolocation information allows you to block IP ranges based on country. Only safe or trusted domains and locations are permitted.\n  \u2022 Cloud security integration: This allows organizations that use cloud security services from third parties to have the same policy applied to user groups in the branch or at headquarters. Aruba delegates control to our third-party partners for trust authorization.\n  \u2022 Dynamic segmentation: Users can tunnel connections on Aruba wired switch ports to the branch gateway and apply consistent policy to the user or device the same way you apply policy to wireless users. Trust is based upon user and device roles.\n\nThe Aruba SD-Branch solution integrates with Aruba ClearPass (or other AAA servers) to form a zero-trust, policy-driven branch. This model dynamically assigns policies based on users and devices, as opposed to the traditional way of assigning these policies manually based on ports, VLANs, and IP addresses.\n\nFinally, the Aruba SD-Branch solution can integrate with best-of-breed third-party security infrastructure partners in the Aruba 360 Security Exchange Program. With these integrations, the Aruba SD-Branch architecture seeks to offer enterprise-grade advanced threat protection in a scalable manner.\n\nFor more information on Aruba\u2019s zero-trust security approach for SD-WAN and SD-Branch, follow the links in the article and check with you Aruba representative (account manager to systems engineer).",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.7086096406,
        "format_confidence":0.5932508111
    },
    {
        "url":"http:\/\/www.stylusstudio.com\/docs\/v2007\/d_start11.html",
        "text":"Getting Help\n\nAs you use Stylus Studio, you can press F1 at any time to obtain context-sensitive help. If you want, you can open the online help manually (and independent of the Stylus Studio application) by selecting Start > Programs > Stylus Studio XML Edition Name > Stylus Studio Documentation.\n\n\n\nThe online documentation is not installed with Stylus Studio. The first time you access the online documentation, you are prompted to download it from the Stylus Studio web site. By default, the online documentation is installed in the \\doc directory where you installed Stylus Studio.\n\nFree Stylus Studio XML Training:\nW3C Member",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.943975687,
        "format_confidence":0.8155457377
    },
    {
        "url":"https:\/\/access.redhat.com\/documentation\/en-us\/red_hat_gluster_storage\/3.4\/html\/administration_guide\/chap-platform_introduction",
        "text":"Chapter\u00a01.\u00a0Preface\n\n1.1.\u00a0About Red Hat Gluster Storage\n\nRed Hat Gluster Storage is a software-only, scale-out storage solution that provides flexible and agile unstructured data storage for the enterprise.\nRed Hat Gluster Storage provides new opportunities to unify data storage and infrastructure, increase performance, and improve availability and manageability in order to meet a broader set of an organization\u2019s storage challenges and needs.\nThe product can be installed and managed on-premises, or in a public cloud.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.8998281956,
        "format_confidence":0.8045548797
    },
    {
        "url":"http:\/\/jseshdoc.qenherkhopeshef.org\/doku.php\/doc:ar:newsigns",
        "text":"User Tools\n\nSite Tools\n\n\nExtending the sign list\n\n\nFrom version 2.0beta onward, users of JSesh can create their own signs. A sign editor and an elaborate database system are planned in the future, but, on a more practical sign, it was decided to allow one to import signs created with various softwares.\n\nSo : a) JSesh has currently no sign editor but b) JSesh can import signs from :\n\n  \u2022 true type fonts (see fontforge for a free editor) ;\n  \u2022 SVG files : SVG is a relatively recent format for vector graphics. It's very powerfull and complete. Currently, JSesh understands SVG files if the sign is drawn in black over white. You can edit SVG files with a number of programs; one of the best free ones is Inkscape, which has the advantage of being multi-platform.\n\nFor compatibility with my previous software (tksesh), it can also read:\n\n  \u2022 font files exported from tksesh (.tml files)\n  \u2022 font files from the GNU font utils (.bzr files). Both types of files can be edited with the fontedit software, included in tksesh. But this is of little interest for the generic user.\n\n\ndoc\/ar\/newsigns.txt \u00b7 Last modified: 2016\/10\/12 14:14 (external edit)",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.7951984406,
        "format_confidence":0.9634665251
    },
    {
        "url":"https:\/\/wiki.ucar.edu\/display\/confluencedocs\/Confluence+Documentation",
        "text":"Child pages\n  \u2022 Confluence Documentation\nSkip to end of metadata\nGo to start of metadata\n\n\nThe\u00a0Confluence wiki has an Office Excel macro that enables you to attach a spreadsheet to a wiki page and then show a specified worksheet and region of the Excel file on the page. To do this:\n\n  1. add a page to your wiki using the Add menu\n  2. save the page\n  3. add an attachment to that page via the Add menu\n  4. click the Insert\/Edit Macro icon in the toolbar\u00a0\n  5. Type \"excel\" into the search box of the Select Macro window and press enter\n  6. Click the Office Excel item\n  7. You'll see a dialog box come up (see screenshot in the right column). Select the spreadsheet you attached from the File Name popup menu\n  8. Optionally, if you'd like to show a specific worksheet or row\/column range, specify those values in the provided fields\n  9. Click Insert and then save your page\n\nThe macro editor will write the wiki markup for you which will look something like:\n\n\nTip: If you ever need to revise your spreadsheet, simply revise it and upload it to the wiki page using the same filename as before. The wiki will save the previous version of the file and use the new one to render the spreadsheet contents on the page.\n\n\nNote: The Office Excel macro does not support Excel 95 format and earlier. If you attach a spreadsheet and it's contents don't display, open it with Excel and confirm that it is saved in a format later than that such as Excel 97-2004 or the current xlsx format.\n\nExample of an Embedded Spreadsheet\n\nOverall Summary\n% of Available\nNUs Available41,776,550,159\nNUs Requested73,002,261,024174.7%\nNUs Allocated37,485,866,79589.7%\nNUs Delivered27,440,814,05365.7%\n% of Delivered\n\n\nThe {pagetree} macro is a handy hierarchical browser of all the pages in your wiki. It's a great way to provide an interactive table of contents. By default it presents a view of all the child pages of your home page. Clicking the plus sign next to an item expands it to show its children without leaving the page you're on. Just click a title to go to the page.\n\nThere is also an option in the macro for expanding or collapsing all children.\n\nUsing the Macro\n\nTo use the macro, edit a page and select the wiki markup tab. You can simply paste the {pagetree} macro anywhere in the page where you want the page browser to appear. The macro takes several parameters outlined below. The root parameter sets the root at the specified page. You can substitute a page name for @self here if you want. If you don't specify a root, it sets the home page as root by default. In addition, the expandCollapseAll parameter provides expand\/collapse all links which are handy.\n\nScreenshot of rendered {pagetree} macro\n\nMacro parameters\n\nHere are all the macro parameters you can control:\n\n  \u2022 root: - (optional) page where the tree would be rooted from. Meta root names @self, @parent, @home can also be used.\n  \u2022 sort: - (optional) sorts the tree node. It my be one of the following: bitwise, creation, modified, natural, position. Default sorting is position\n  \u2022 excerpt: - (optional) true\/false flag that indicate if a page excerpt would be included in the tree display (default is false).\n  \u2022 reverse: - (optional) true\/false flag that allows you to reverse the order of the display (default is false).\n  \u2022 searchBox: - (optional) true\/false flag that allows you to add a search box in the tree that would search from the root page (default is false).\n  \u2022 expandCollapseAll: - (optional) true\/false flag that allows you to add an expand all and a collapse all row (default is false).\n  \u2022 startDepth: - (optional) a number that indicates the initial depth that the tree would display (default value is 1).\n\nRelated Documentation\n\nFor more detailed documentation, see the official Pagetree Macro Docs.\n\nRelated macros\n\n  \u2022 {children}\n\nConfluence Documentation\n\nThe Confluence User Guide is available on the vendor's website.\n\nConfluence wiki markup notation is also available from any Edit Page form.\n\nUseful Confluence Docs\n\nUCAR Wiki Documentation\n\nUCAR Wiki Change Announcements\n\nTopic Tags",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.8829948902,
        "format_confidence":0.962279737
    },
    {
        "url":"https:\/\/www.adaptavist.com\/doco\/display\/SDJIRA\/Inserting+or+Editing+a+Diagram",
        "text":"Skip to end of metadata\nGo to start of metadata\n\nWithin a JIRA issue view screen choose the More \u2192 Insert\/Edit SmartDraw Diagrams from the\u00a0drop-down menu:\n\nChoose either to Edit an existing Diagram from the list of files already attached to the issue:\n\nOr - Select a\u00a0Template.\n\nCreate a diagram in the Work Area. Utilize the Ribbon and SmartPanel to build your diagram.\u00a0Click Save to Jira.\n\n  \u2022 No labels",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.8715215325,
        "format_confidence":0.6963761449
    },
    {
        "url":"http:\/\/www.novell.com\/documentation\/zenworks114\/zen11_adaptive_agent\/data\/package_locks_intro.html",
        "text":"17.0 Package Locks\n\nThe ZENworks Adaptive Agent provides information on how to configure package locks on managed devices. You can use these package locks to prevent the removal or upgrade of packages on the managed devices. This section is applicable only to ZENworks Configuration Management Linux Devices.\n\nThe following sections contain more information:",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.5957781076,
        "format_confidence":0.9605655074
    },
    {
        "url":"https:\/\/docs.rkeeper.com\/sh5\/Client-Installation.53084393.html",
        "text":"This article describes how to install the StoreHouse 5 client separately from the other components. The client can also be installed simultaneously with other components. Find more information in the article on server installation and configuration.\n\nThere are two types of client applications:\n\n  \u2022 SH5 Client is the main client application.\n  \u2022 RK7 Import to SH5 Client is a QUSH application for configuring integration with r_keeper 7 and other systems.\n\nThe installation of other components is described in a separate article.\n\nClient Installation\n\nThe StoreHouse 5 client installation is performed using the installer, as well as the server part.\n\nThe distribution kit of the current version of the system can be found on the FTP server at Use your Dealer ID for authorization.\n\nGoogle Chrome browser does not support FTP. So, copy the link and open it using the file explorer or an FTP client\n\n\nThe installer is located in an archive called SH5Setup_version_number. To open the archive, you need the 7-zip archiver. To install the system, unzip the file to the required folder and run the installer.\n\nSelect your language and click OK. The installer interface will change to the specified one.\n\nThe step-by-step process is presented below:\n\n1. After selecting the language and accepting the license agreement, specify the Store House 5 installation folder\n\n2. Select the required components\n\n3. Enter the address of the QUSH import server\n\n4. Specify the IP address of the server, port number after the letter \"a\".\nOptionally, specify the server name in quotation marks.\n\n5. Check the boxes for the applications that you want to create shortcuts for\n\n6. Check that the settings are correct and click Install.\n\nA folder with shortcuts will appear on the desktop:\n\n\nThe StoreHouse 5 client is ready for use.\n\nDouble-click on the SH.exe shortcut.\n\nThe authorization window will appear.\n\nEnter username and password to get started.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.6588149071,
        "format_confidence":0.8452064991
    },
    {
        "url":"https:\/\/datatracker.ietf.org\/wg\/ppvpn\/about\/",
        "text":"Provider Provisioned Virtual Private Networks (ppvpn) Concluded WG\n\nNote: The data for concluded WGs is occasionally incorrect.\n\nWG Name Provider Provisioned Virtual Private Networks\nAcronym ppvpn\nArea Sub-IP Area (sub)\nState Concluded\nCharter charter-ietf-ppvpn-01 Approved\nDependencies Document dependency graph (SVG)\nPersonnel Chairs Loa Andersson\u00a0\nRick Wilder\u00a0\nArea Director Alex Zinin\u00a0\nTech Advisor Thomas Narten\u00a0\nMailing list Address\nTo subscribe\n\nCharter for Working Group\n\nThis working group is responsible for defining and specifying a limited\nnumber of sets of solutions for supporting provider-provisioned virtual\nprivate networks (PPVPNs). The work effort will include the development\nof a framework document, a service requirements document and several\nindividual technical approach documents that group technologies\ntogether to specify specific VPN service offerings. The framework will\ndefine the common components and pieces that are needed to build and\ndeploy a PPVPN. Deployment scenarios will include provider-managed VPN\ncomponents located on customer premises.\n\nThe service requirement document will detail the requirements\nindividual PPVPN approaches must satisfy from a Service Provider (SP)\nperspective. Particular attention will be placed on SP requirements\nfor security, privacy, scalability and manageability considering such\nfactors as Service Provider's projections for number, complexity, and\nrate of change of customer VPNs over the next several years. The\nworking group will make specific efforts to solicit this information\nfrom SPs. The service requirements document is not intended to define\nthe requirements that all approaches must satisfy. Rather, it is\nintended to become a \"checklist\" of requirements, not all of which\nwill necessarily be required in all deployment scenarios. A goal of\nthe requirements document is to provide a consistent way to evaluate\nand document how well each individual approach satisfies the\nindividual requirements.\n\nThe effort will produce a small number of approaches that are based\non collections of individual technologies that already exist (see\nbelow for specifics). The goal is to foster interoperability among\nimplementations of a specific approach. Standardization of specific\napproaches will be gauged on (I)SP support. Note that it is not a\ngoal of this WG to develop new protocols or extend existing\nones. Rather, the purpose is to document and identify gaps and\nshortcomings in individual approaches with regards to the\nrequirements. In the case that specific work items are identified,\nsuch work will be done in an appropriate WG. Taking on specific\nprotocol work items in this WG will require rechartering.\n\nThe working group is expected to consider at least three specific\napproaches including BGP-VPNs (e.g. RFC 2547), virtual routers and\nport-based VPNs (i.e., where the SP provides a Layer 2 interface,\nsuch as Frame Relay or ATM, to the VPN customer, while using IP-based\nmechanisms in the provider infrastructure to improve scalability and\nconfigurability over traditional L2 networks). Multiple approaches\nare being developed as each approach has particular characteristics\nand differing scope of applicability.\n\nThe working group will consider inter-AS (SP) VPN interconnects so\nthat VPNs are able to span multiple ASs (SPs).\n\nEach technical approach document will include an evaluation of how\nwell it meets the requirements defined in the requirements\ndocument. In addition, technical approach documents will address\nscalability and manageability issues as well as their operational\naspects. Individual approach documents will also analyze the threat\nand security aspects of PPVPNs and include appropriate\nmandatory-to-implement technologies and management mechanisms to\nensure adequate security and privacy of user data in a VPN\nenvironment. This analysis will include cryptographic security from\ncustomer site to customer site using IPSEC.\n\nAn applicability statement will be developed for each approach that\ndescribes the environments in which the approaches are suitable for\ndeployment, including analysis of scaling impact of the approach on\nSPs and threat analysis.\n\nCoordination with the IETF PWE3 and ITU-T efforts will be ensured.\n\n\nDate Milestone\n1 Nov 2003 Charter update or WG disband\n1 Jun 2003 Begin submission of the candidate L2 approaches and related applicability statements to IESG for publication\n1 Apr 2003 Submit the layer 2 requirement and the layer 2 framework documents to the IESG for consideration as Informational RFCs.\n1 Mar 2003 Begin submission of the candidate L3 approaches and related applicability statements to IESG publication\n\nDone milestones\n\nDate Milestone\nDone Submit the layer 3 requirement and the layer 3 framework documents to the IESG for consideration as Informational RFCs.\nDone Begin discussion of applicability statements.\nDone Begin discussion (based on submitted IDs) on candidate approaches against the different service requirements.\nDone Formulate a plan and begin approaching SPs for input on scaling and other requirements\nDone Begin discussion of the framework and the service requirement documents. Identify a limited set of candidate approaches. Build appropriate design teams.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.534291923,
        "format_confidence":0.5438322425
    },
    {
        "url":"http:\/\/documentation.apple.com\/en\/finalcutpro\/usermanual\/chapter_54_section_2.html",
        "text":"Setting Proper Audio Levels\n\nWhen you work with audio, you need to make sure you set proper levels at each stage of your production:\n\nSetting Levels for Capture\n\nWhen you capture digital audio, you usually cannot make level adjustments because an exact copy of the digital information is transferred to your hard disk. However, if you are capturing analog audio using a third-party audio interface, make sure you set each input channel so the meters in the Clip Settings tab of the Log and Capture window match the audio meters on your video or audio device. For more information, see Capturing Audio from Tape.\n\nDetecting Audio Peaks\n\nWhen you capture audio, clipping occurs if any part of the audio signal goes over 0 dBFS. Because 0 dBFS is the maximum digital level possible, all levels that would have been above 0 dBFS are set (clipped) at 0 dBFS. Because of the nature of digital audio recording, such clipped audio typically results in a crackly, brittle sound. Excessive peaks indicate that your audio was recorded at unsuitable levels.\n\nIf your program has peaks in the audio, you can either recapture the audio at a better level or edit the audio appropriately to avoid the peaks. You can use the Mark Audio Peaks command to identify audio peaks in your clips. It\u2019s then up to you to decide whether to not use those sections of audio or rerecord them.\n\nNote: Final\u00a0Cut\u00a0Pro considers your clips\u2019 audio levels when analyzing levels. For example, if you set a clip\u2019s audio level to +12\u00a0dB, audio peaks may be detected. However, if you reset the audio level to 0\u00a0dB, audio peaks may no longer be detected.\n\nTo find and mark audio peaks\n  1. To detect peaks in a clip, do one of the following:\n\n    \u2022 Select one or more clips in the Browser.\n\n    \u2022 Open a sequence clip from the Timeline.\n\n  2. Choose Mark > Audio Peaks > Mark.\n\n    A status window appears with a progress bar showing how much of the process is complete. Markers are placed at each peak.\n\n    \u2022 If you selected a clip in the Browser: Markers appear for the clip and are labeled \u201cAudio Peak N,\u201d where N starts at 1 and increases, depending on how many audio peaks are detected. These markers also appear in the Viewer when the clip is opened there.\n      Figure. Browser window showing audio peak markers for a selected clip.\n      \u2022 If you selected a clip in the Timeline: Markers appear in both the Timeline and Canvas.\n        Figure. Timeline window showing audio peak markers for a sequence clip.\n\nYou can clear audio peak markers that were previously added, if you like.\n\nTo clear all audio peak markers in a clip\n  1. Select one or more clips in the Browser or Timeline.\n\n  2. Choose Mark > Audio Peaks > Clear.\n\nRaising Audio Levels Using Audio Normalization and Gain\n\nWhen you edit, your audio may come from a variety of sources, and the levels often vary. Final\u00a0Cut\u00a0Pro includes a Gain audio filter that allows you to amplify (or attenuate) the level of an audio clip far beyond the +12\u00a0dB gain available with audio level keyframes.\n\nTo amplify the audio level of clips whose levels are too low, you can manually apply the Gain filter. The only risk with manually applying gain adjustments to a clip is that you may amplify the audio too much, resulting in distorted audio. To guarantee that clips with low audio levels have the optimal gain, you can normalize your audio clips using the Apply Normalization Gain command.\n\nHow Normalization Gain Works in Final\u00a0Cut\u00a0Pro\n\nAudio normalization works by scanning audio for the peak (loudest) sample level and then applying a Gain filter that brings the peak level to the level you request. By default this value is 0 dBFS, the highest level possible before clipping occurs. The Gain filter raises the overall audio level.\n\nFigure. Diagrams showing an audio waveform with a peak, the same waveform with gain added so that the audio peak is set to 0 dBFS, and the waveform normalized with the peak level set at 0 dBFS.\n\nIn most applications, audio normalization is a destructive process because it permanently modifies audio files. Final\u00a0Cut\u00a0Pro applies normalization nondestructively by applying a Gain filter to a clip instead of affecting the clip\u2019s audio file. You can disable or remove the Gain filter and hear the original, unmodified audio file.\n\nTo apply normalization gain to audio clip items in a sequence\n  1. Select one or more audio clip items in a sequence.\n\n  2. Choose Modify > Audio > Apply Normalization Gain.\n\n    The Apply Normalization Gain dialog appears.\n\n  3. In the \u201cNormalize to\u201d field, enter the value you want to raise each audio clip\u2019s peak value to, then click OK.\n\n    The dialog displays a progress bar and Final\u00a0Cut\u00a0Pro begins calculating the peak value for each clip. After processing, each selected clip has its own Gain filter applied with a gain adjustment appropriate for that clip\u2019s normalization.\n\nHow Linked Mono and Stereo Clips Are Normalized\n\nThe Apply Normalization Gain command works differently depending on the type of clip items selected:\n\n  \u2022 Single mono clip item or multiple linked mono clip items: A separate Gain filter is applied to each clip item, and peaks for each clip item are calculated independently.\n  \u2022 Stereo clip items: A stereo Gain filter is applied to the stereo clip items, and the Gain setting is based on the peak value across both channels.\n\nReapplying Normalization Gain\n\nFinal\u00a0Cut\u00a0Pro searches for peak audio only between a clip\u2019s In and Out points, not for the entire duration of the clip\u2019s media file. If you trim a clip\u2019s In or Out point, new peaks may be introduced and the Gain adjustment may no longer be appropriate. In this case, you can easily reapply normalization gain to set an appropriate level.\n\nReapplying normalization gain is no different from applying normalization gain for the first time. The only difference is that no new Gain filters are added to clips that already have them. Instead, the values of the existing Gain filters are adjusted based on the current audio peaks of the clips.\n\nChoosing Normalization Gain Versus Audio Level Keyframing\n\nThe Gain filter and the Apply Normalization Gain command are best used for broad audio level adjustments, such as when you have clips with fairly low audio levels. For subtle level adjustments and more complex mixing, you should use audio level keyframes in the Viewer or Timeline.\n\nTroubleshooting Audio Normalization\n\nThere are a few issues to be aware of when you use the Gain filter and the Apply Normalization Gain command:\n\n  \u2022 Applying gain raises the level of an audio signal, including the noise. Very quiet audio, when normalized, may be very noisy. When possible, the best solution is to rerecord the audio. If this is not possible, you may be able to minimize the noise using Soundtrack\u00a0Pro.\n\n  \u2022 Loud peaks in audio clips that otherwise contain low audio levels make audio normalization more difficult to use. For example, suppose you have a clip containing dialogue that was recorded too quietly. At the beginning of this clip, there is a brief peak when the slate was clapped together. When you attempt to normalize the audio of this clip, the sound of the slate is so loud that very little gain is applied. To apply more gain, simply trim the clip until the audio peak from the slate is gone, then use the Apply Normalization Gain command again.\n\nWhat Reference Level Should You Use for Mixing and Output?\n\nThe dynamic range of your mix is dependent on the final viewing environment. For example, movie theaters have large, relatively expensive sound systems that can reproduce a large dynamic range. Television speakers are much smaller, and often the listening environment has more ambient noise, so very quiet sounds may not even be noticeable unless the overall signal is compressed and the level increased, reducing the dynamic range.\n\nFor example, television stations normally accommodate only 6\u00a0dB between the average loudness and the peaks. Dolby Digital feature film soundtracks, on the other hand, can accommodate up to 20\u00a0dB between average and peak levels. This is why loud sounds in a movie theater sound so loud: they are much louder than the average level.\n\nAcceptable amount of dynamic range\nTheatrical Dolby Digital\n20\u00a0dB\nAverage videotape\n12\u00a0dB\nTelevision broadcast\n6\u00a0dB\n\nWhen you mix your final audio, you choose a consistent reference for the average level. When you choose the average reference level, you are actually choosing how much additional headroom you have before your signal distorts. The higher you set the average level, the less safety margin you have for peaks in the signal. This means that the loudest sounds in your mix cannot be much louder than the average levels, so the mix is less dynamic.\n\nIf you set the reference level of the Final\u00a0Cut\u00a0Pro floating audio meters to -20 dBFS, you have nearly 20\u00a0dB of headroom because 0 dBFS is the digital limit for the loudest sound. If you set the reference level in your sequence to -12 dBFS instead, you have less headroom. Even though the average level of your audio is higher, there won\u2019t be as much dynamic range.\n\nFigure. Diagram of audio meters showing the available headroom when the reference level is set to -12 dBFS.\n\nHow much dynamic range you allow in your audio mix depends on its ultimate destination. If you\u2019re editing a program for TV broadcast, a reference level of -12 dBFS is fine, because you are only allowed 6\u00a0dB of dynamic range anyway. But if you\u2019re working on a production to be shown in movie theaters, consider using a reference level closer to -18 or even -20 dBFS (both of these are frequently used standards).\n\nRemember that the ultimate goal is to ensure that audio doesn\u2019t peak over 0 dBFS in your mix (as displayed in the Final\u00a0Cut\u00a0Pro audio meters) and won\u2019t peak over +3\u00a0dB or so on an analog meter.\n\nOutputting Bars and Tone at the Head of Your Tape\n\nWhen you output your program to a tape for duplication or delivery to a broadcast facility, you\u2019ll typically include a 1\u00a0kHz reference tone at th... (truncated)",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.9584741592,
        "format_confidence":0.856458962
    },
    {
        "url":"https:\/\/noisebridge.net\/wiki\/NoiseCal",
        "text":"From Noisebridge\nJump to: navigation, search\n\nThis page describes the specifications and implementation of NoiseCal, a calendaring solution that adheres to Noisebridge's practices and principles.\n\n\n[edit] Folks\n\n[edit] Specifications\n\n[edit] Mailing List Threads\n\n[edit] Requirements\n\n  \u2022 Publicly editable\n  \u2022 Unregistered editable\n  \u2022 Revert edits\n  \u2022 Publicly linkable\n  \u2022 Has the usual variety of calendar layouts (day, week, month, list)\n  \u2022 Events have a time, duration, description, recurrence, location\n  \u2022 Exports to iCalendar (RFC 5545)\n  \u2022 Widget on the wiki homepage\n  \u2022 Free, as in free speech and free beer\n  \u2022 Hosted locally\n\n[edit] Bonus points\n\n  \u2022 Can use wiki logins or some other kind of identification in addition to anonymous\n  \u2022 Events have a field for which room\/area of NB\n  \u2022 Calendars show which room\/area of NB\n  \u2022 open source or some other moral superiority\n  \u2022 easy publishing to email (for nb-announce, for instance)\n  \u2022 misc bells and whistles\n  \u2022 RSS feed\n  \u2022 Some kind of reputation system, likely usernames. Edits can be associated with usernames.\n  \u2022 Tagging\/Categorizing of events are useful so as to group similar events. There's a lot going on at NB! IE: \"5MoF\", \"Conlang\", \"Not Happening on a Regular Basis\", \"Reoccuring\"\n\n[edit] \u00dcberbonus Point\n\n  \u2022 Views based on tags. IE, view all room reservations for the next day\/week\/month on a calendar with the tag \"Dark Room\" or \"Project Euler Group\", or the word \"foo\" mentioned anywhere in the entry (this last one\n\nmight make searches expensive).\n\n  \u2022 Printable tag views. IE, I can print out all the room reservations for \"Dark Room\" this week and put it on the door (which will instantly be made out of date when the first edit comes along, phoey). This feature also lets conferences at NB self-schedule and you have an instantly printable\/viewable calendar.\n\n[edit] Implementation\n\nWe are still at the brainstorming and exploring phase.\n\nThere is some code on github but it's expected to be in great flux.\n\n[edit] Meeting Notes\n\n[edit] 2010-01-23 15:00\n\n  \u2022 We will base our work on the Calendar MediaWiki extension by Kenyu73.\n  \u2022 Replace PrefixSearch with a category search. (Fetch all events from Category:Event.)\n  \u2022 Parsing\n    \u2022 We shall use a human-readable wiki-ish format.\n      \u2022 Every section titled Event Details shall contain details of a NoiseCal event. The section shall contain a list of key-value pairs of the form: * Key: Value.\n      \u2022 Keys and values are case-insensitive.\n      \u2022 Valid keys, for now, are: \"Description\" (required), \"When\" (required), \"Where\", \"Recurring\", \"Organizer\". Values of \"When\" must be valid ISO8601 timestamps.\n    \u2022 We parse out the event on save, and add categories as a layman's index. (Does this make sense?)\n    \u2022 Add parsing for time.\n    \u2022 Add recurring events. (this needs more thought)\n    \u2022 Validate stuff (on ArticleSave). If you clicked save, but your NoiseCal-related edits do not validate, you will get a preview with an error message--as specific as possible. We could render events \"in the wild\" via ViewHook?.\n    \u2022 Let event description be wherever in the article (currently the first line of an event article is the description).\n  \u2022 Multiple events per wikipage.\n  \u2022 iCalendar (RFC 5545)\n    \u2022 iCalendar export\n    \u2022 Use ObjectCache for caching\n    \u2022 How does iCalendar deal with past events? Do iCalendar feeds always export the entire history of a calendar?\n    \u2022 We should have a way to restrict an iCalendar feed to the past n months and to the past m months in the future. Defaults should be n=1 and m=3.\nPersonal tools",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.7628839016,
        "format_confidence":0.9449991584
    },
    {
        "url":"https:\/\/docs.tenable.com\/tenableio\/vulnerabilitymanagement\/Content\/Analysis\/Classic\/DeleteAssets.htm",
        "text":"Delete Assets (Classic Interface)\n\nYou can delete assets as a standard or administrative user.\n\nWhen you delete an asset,\n\n  \u2022 removes the asset from the default view of the assets table.\n  \u2022 deletes vulnerability data associated with the asset.\n  \u2022 stops matching scan results to the asset.\n\nDeleting an asset does not immediately subtract the asset from your licensed assets count. Deleted assets continue to be included in the count until they automatically age out of your licensed assets count after 90 days.\n\nYou cannot reverse the deletion of an asset. If you mistakenly delete an asset, add it to the system by scanning the asset again.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.9300832748,
        "format_confidence":0.9597594738
    },
    {
        "url":"https:\/\/www.concrete5.org\/marketplace\/addons\/devoda-seo\/robots-meta-configuration",
        "text":"Robots meta configuration\n\nDashboardSEOTitles & Metas (\u201cOther\u201d tab)\n\nMeta Robots Index\n\nWhether you want to keep your whole site out of the index, just a single page or you prefer to keep specific page types of your site unfindable: the Devoda SEO plugin has your back. In the SEO section of a single page is a Meta Robots Index setting that allows you to choose the setting on a per page basis.\n\nOrder of importance\n\nIf a meta robots index setting is set on a per page basis, this value will be used. If none is set, it will look for the \u201cPage Type\u201d specific setting. Also no value set will mean it will use the sitewide \u201cMeta Robots Index\u201d value.\n\nYou can also choose to set a page type or page specific value to \u201csitewide value\u201d. In this case, it will use the sitewide value.\n\nMeta Robots Follow\n\nThe \u201cFollow\u201d value works the same way as the \u2018Meta Robots Index\u2019. The only difference is that it will have the values \u201cFollow\u201d and \u201cNofollow\u201d. Same order of importance applies here.\n\nMeta Robots Advanced\n\nNext to these robots, you can also enable\/disable the following sitewide & page specifically:\n\n  \u2022 Meta Robots NO ODP (NO Open Directory Project);\n    Prevents search engines from using metadata from the Open Directory project for titles or snippets. If you set a custom description for a page, it will have the noodp tag regardless of this setting.\n  \u2022 Meta Robots No Image Index;\n    Prevents search engines from indexing images.\n  \u2022 Meta Robots No Archive;\n    Prevents search engines from showing a 'Cached' link in search results.\n  \u2022 Meta Robots No Snippet;\n    Prevents search engines from showing a snippet in the search results.\n  \u2022 Meta Robots No Translate;\n    Prevents search engines from offering translation of pages in search results.\n\nAll of the 5 above can also be entered on a per page basis. Go to the SEO section of a single page and look for \u201cDevoda SEO\u201d. Within the \u201cAdvanced\u201d tab, you can change \u201cMeta Robots Advanced\u201d to \u201cCustom\u201d, in order not to use the sitewide settings. If you ever want to revert back to sitewide, just change the \u201cMeta Robots Advanced\u201d and you\u2019re done. This makes maintaining your site with specific settings really easy.\n\n\nRead all about Robots meta tags over here.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.8020557165,
        "format_confidence":0.6852165461
    },
    {
        "url":"http:\/\/answers.perforce.com\/articles\/KB_Article\/Perforce-Server-Trace-Flags",
        "text":"Perforce Public Knowledge Base - Perforce Server Trace Flags\nReset Search\n\n\n\nPerforce Server Trace Flags\n\n\u00ab\u00a0Go Back\n\n\n\nThis article describes the uses of server trace flags, the various configurations of the flags, and their individual outputs.\n\n\nTo turn on command tracing in the Perforce Server, add the -v server=1, -v server=2, or -v server=3\u00a0 (highest level) flag to the p4d startup command. Use P4LOG or the -L flag to name a log file. For example:\n\np4d -r \/usr\/perforce -v server=3 -p 1666 -L \/usr\/perforce\/log\n\nOr on Windows, or OSX use \"p4 set\" to create the P4DEBUG variable and restart Perforce. For example:\n\np4 set -S Perforce P4DEBUG=server=3\n\nTrace output appears in the log file, and shows the date, time, username, IP address, and command for each request the server processes.\n\nAs of 2005.2, the server automatically produces diagnostic output to help identify performance problems. The new support is on by default but can be turned off or adjusted with the -vtrack=x flag to the server. Any user commands that exceed certain thresholds for resource usage (CPU, lapse time, database I\/O, network I\/O, among other things) automatically get logged into the server error log P4LOG. The levels that can be set with -vtrack=x are:\n\n  \u2022 0 turn off tracking\n  \u2022 1 track all commands\n  \u2022 2 track excess usage for a server < 10 users\n  \u2022 3 track excess usage for a server < 100 users\n  \u2022 4 track excess usage for a server < 1000 users\n  \u2022 5 track excess usage for a server > 1000 users\n\nIf -vtrack is not provided on the server command line or set with P4DEBUG, the tracking level is computed from the number of users listed in the server license file.\n\nTo enable both, \"server\" and \"track\" flags at the same time, you can just do the following:\n\np4 set -S Perforce P4DEBUG=\"track=1 server=3\"\n\nAs of 2001.1, support has been added for -v server=2 and -v server=3. \u00a02011.1 adds -v server=4.\n\n  \u2022 -v server=1 ensures that the start information for each command is logged.\n  \u2022 -v server=2 extends server tracing to include command start and stop.\n  \u2022 -v server=3 adds a \"compute end\" message for certain commands.\n  \u2022 -v server=4 adds errors sent to the client to the server log.\nPlease note, a Perforce Server might log stop information for commands at -v server=1 if the command is long running. This differs from -v server=2, which ensures that the stop information is logged for all commands regardless of how long they take to complete.\n\nPrior to 98.1, you could not set this trace flag when you started p4d.exe as a service on Windows. You could only start p4d.exe from the command line on Windows when setting this flag.\n\nAs of 98.1 and later, you can set P4DEBUG as a registry variable to server=1 (use p4 set), so you can use this trace flag even with Perforce installed as a service on Windows.\n\nPrior to 97.3, this feature was not available.\n\n\n  \u2022 p4 set only works on Windows or OSX operating systems.\u00a0 It will not work with Cygwin.\u00a0 Either use p4d command line options, or environment variables elsewhere.\nRelated Links\n\n\n\nWas this article helpful?\n\n\n\nPlease tell us how we can make this article more useful.\n\nCharacters Remaining: 255",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.8583874702,
        "format_confidence":0.5467862487
    },
    {
        "url":"https:\/\/support.quest.com\/technical-documents\/metalogix-controlpoint\/8.2\/user-guide\/14",
        "text":"Chat now with support\nChat with Support\n\nMetalogix ControlPoint 8.2 - User Guide\n\nPreface Getting Started with ControlPoint Using Discovery to Collect Information for the ControlPoint Database Cache Searching for SharePoint Sites Managing SharePoint Objects\nAccessing SharePoint Pages Viewing Properties of an Object within the SharePoint Hierarchy Creating Dashboards for Monitoring Statistics within Your SharePoint Farm Setting Object Properties Managing Site Collection and Site Features Managing Audit Settings Creating and Managing SharePoint Alerts Setting ControlPoint Alerts Managing Metadata Copying and Moving SharePoint Objects (Version 8.0 and Earlier) Moving a Site Collection to Another Content Database Deleting Sites Deleting Lists Archiving Audit Log Data Before Moving or Deleting a Site Collection Duplicating a Workflow Definition from one List or Site to Others Removing a Workflow from One or More Lists (or Sites)\nUsing ControlPoint Policies to Control Your SharePoint Environment Managing SharePoint User Permissions Data Analysis and Reporting\nSpecifying Parameters for Your Analysis Analysis Results Display Generating a SharePoint Summary Report Analyzing Activity Analyzing Object Properties Analyzing Storage Analyzing Content Generating a SharePoint Hierarchy Report Analyzing Trends Auditing Activities and Changes in Your SharePoint Environment Analyzing SharePoint Alerts Analyzing ControlPoint Policies Analyzing Users and Permissions The ControlPoint Task Audit Viewing Logged Errors\nScheduling a ControlPoint Operation Saving, Modifying and Executing Instructions for a ControlPoint Operation Using the ControlPoint Governance Policy Manager (SharePoint 2010 and Later) Using Sensitive Content Manager to Analyze SharePoint Content for Compliance Using ControlPoint Sentinel to Detect Anomalous Activity Default Menu Options for ControlPoint Users About Us\n\nViewing Properties of an Object within the SharePoint Hierarchy\n\nFrom the SharePoint Hierarchy, you can access an at-a-glance summary of key properties for the farm, a Web application, a site collection, or a sitea site collection or a site.\n\nYou can also retrieve the following statistics for an object at any level of the hierarchy:\n\n\u00b7the total number of unique users who have permissions, and\n\n\u00b7the total number of users with activity over the past 30 days.\n\nTo view an object's properties:\n\n1From the SharePoint Hierarchy, select the object whose properties you want to view.\n\nNOTE: \u00a0You can only view properties for a single object at a time (that is, the multiple selection is unavailable).\n\n2Right-click and select Properties.\n\nFarm Properties\n\nFarm-level properties are maintained in Operations section of the SharePoint Central Administration site .\n\nProperties FARM\n\nWeb Application Properties\n\nWeb application-level properties are maintained in the Application Management section of the SharePoint Central Administration pages.\n\nProperties WEB APP\n\nSite Collection Properties\n\nSite Collection properties are maintained in the SharePoint Site Collection Administration area.\n\n\nSite Properties\n\nSite-level properties are maintained in SharePoint Site Settings pages.\n\nProperties SITE\n\nTo view user permissions and activity totals (for the selected level of the SharePoint Hierarchy):\n\nClick [Calculate Totals].\n\nWhen the values have been calculated, the following information displays at the top of the Properties dialog:\n\n\u00b7Total Users with Permissions\n\n\u00b7Total Active Users (last 30 days)\n\nNOTE: \u00a0These totals reflect unique users. (That is, any user who has more than one set of permissions to a site is only counted once).\n\nProperties USER TOTALS\n\n\u00b7Total Users with Permissions includes: \u00a0\n\n\u00a7Web application Service Accounts\n\n\u00a7Users granted permissions through Web application policies\n\n\u00a7Site Collection Administrators\n\n\u00a7Users within Active Directory groups to which the ControlPoint Service Account has access (that is, within the same domain or forest, in a different domain\/forest for which with a two-way trust exists, or in a different domain\/forest with a one-way outgoing trust that ControlPoint can authenticate via the ControlPoint Manage Forest Access feature. \u00a0Disabled Active Directory accounts are included in this total. \u00a0If an Active Directory user has been renamed but still has permissions in SharePoint under the old name, each name will be counted as a separate user.\n\n\u00b7Excluded from this total are built-in groups and special accounts, such as nt authority\\authenticated users (or any account that begins with \"nt authority\") and sharepoint\\system, and users granted permissions via augmented Claims or alternate authentication providers.\n\nNOTE: \u00a0Total Users with Permissions uses data recorded in the ControlPoint Service Database (xcAdmin), and is current as of the last Discovery run. (The actual number of users within Active Directory groups is counted in real-time, however.)\n\nTotal Active Users uses 30 days worth of activity data that is collected by the SharePoint usage job(s), and is as current as the last time the job(s) ran.\n\n\nCreating Dashboards for Monitoring Statistics within Your SharePoint Farm\n\nThe ControlPoint Configuration site contains the following custom lists that ControlPoint Application Administrators can use to create \"dashboards\" of Web Parts for monitoring statistics within a SharePoint farm:\n\n\u00b7Farm Statistics - Individual farm-wide metrics that are typically presented individually.\n\nFarm Statistics\n\n\u00b7Web Application Statistics - Rows of data about each Web application. \u00a0Depending on your needs, you can present aggregate data (column sums, maximums, standard deviation, and so on).\n\nWeb App Statistics\n\n\u00b7Site Collection Statistics - Rows of data about each site collection which, like Web Application Statistics, can be customized to present aggregate data as well as key performance indicators (KPIs).\n\nSC Statistics\n\nNOTE: \u00a0by default, the Site Collections Statistics list includes all site collections in the farm. \u00a0ControlPoint Application Administrators can, however, eliminate from the list groups of site collections that they do not want to monitor by excluding the Web applications that host them as described in the ControlPoint Administration Guide. \u00a0This may be useful, for example, if the number of site collections in your farm exceeds the SharePoint recommended maximum number of list items.\n\nYou can configure an additional list, the Web Statistics KPI\/Status \u00a0List, to track key performance indicators for selected statistics if:\n\n\u00b7for SharePoint 2010:\n\n\u00a7you have a SharePoint Server Enterprise-based environment, and\n\n\u00a7the feature \"SharePoint Server Enterprise Site Features\" has been activated on the ControlPoint Configuration root site.\n\n\u00b7for SharePoint 2007:\n\n\u00a7you have a MOSS Enterprise-based environment, and\n\n\u00a7the feature \"Office Services Enterprise Site Features\" has been activated on the ControlPoint Configuration root site.\n\nNOTE: \u00a0This list type has been deprecated as of SharePoint 2013.\n\nHow Statistics Lists are Populated\n\nStatistics lists are populated as part of the nightly Full Discovery timer job. \u00a0When this job runs, the lists are cleared and re-populated with current data.\n\nNOTE: \u00a0Because of the way in which they are populated, if any of these lists are copied or moved, statistics data will become static (that is, it will not be updated) in the new location.\n\nTo create a statistics dashboard on the ControlPoint Configuration root site main page:\n\nNOTE: \u00a0The steps below are intended to provide introductory guidelines for creating dashboards on the ControlPoint Configuration root site main page. \u00a0Consult your SharePoint documentation and other available resources for more detailed instructions and\/or guidance in using alternate tools and techniques.\n\n1From the ControlPoint Configuration site main page, choose Site Actions > Edit Page.\n\n2Click Add a Web Part, then choose the list(s) that you want to add as Web Parts to that area of the page.\n\nEXCEPTION: The Web Statistics KPI List is configured and added to the page using a different method. \u00a0See \"To configure the Web Statistics KPI, \" following.\n\nDashboardsADD WP\n\nYou can either use the default view that has been defined for the list or create a custom view.\n\nYou can, for example, change the display order of columns; display a subset of columns; display columns that are not included in the default view; add a calculated column; display column totals.\n\n\u00b7Example: \u00a0Site Collection Statistics with calculated columns to track size and storage utilization.\n\n\n\nSC Statistics CUSTOM\n\n\n\u00b7Example: \u00a0Farm Statistics filtered to show only \"high priority\" metrics.\n\n\nFarm Stats CUSTOM\n\nTo configure the Web Statistics KPI\/Status List (SharePoint 2007 and 2010):\n\nREMINDER: \u00a0To use KPI lists and Web Parts, you must have an Enterprise-based environment \u00a0with Enterprise Site Features activated on the ControlPoint Configuration root site. \u00a0This list type has been deprecated as of SharePoint 2013.\n\n1Create a new Status list (or KPI list \u00a0as it is known in SharePoint 2007) entitled \"Web Statistics KPI\" (or another name of your choosing).\n\n2Create indicators for the Web Statistics KPI List as follows:\n\na)In the ControlPoint Configuration root site, open the Web Statistics KPI List.\n\n\n\u00a7New > Indicator using data in SharePoint list (SharePoint 2007)\n\n\n\u00a7New > SharePoint List based Status Indicator (SharePoint 2010)\n\nDashboard KPI LIST\n\nc)Complete the appropriate fields on the New Item page (including the List URL of the list containing the statistic for which you want to create an indicator). \u00a0Consult your SharePoint documentation for complete instructions.\n\nDashboard KPI CREATE\n\nd)Complete substeps b and c for each indicator you want to create.\n\nOnce the list has been set up, add a KPI Web Part to the dashboard, specifying the Web Statistics KPI List as the Indicator List.\n\nDashboard KPI\n\n\nSetting Object Properties\n\nYou can use ControlPoint to set properties for site collections, sites, and lists.\n\nIn a multi-farm ... (truncated)",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.9215379357,
        "format_confidence":0.7677069306
    },
    {
        "url":"https:\/\/docs.vmware.com\/en\/Unified-Access-Gateway\/2207\/uag-deploy-config\/GUID-F1B90F5A-DC1A-4175-B345-A4984F9AF757.html",
        "text":"If you log in as root to the Unified Access Gateway console with the correct username and password and get a \"Login incorrect\" error, check for keyboard mapping issues and reset the root password.\n\nThere are several reasons why a login error occurs:\n  \u2022 the keyboard used does not map certain password characters correctly according to the keyboard definition of Unified Access Gateway\n  \u2022 the password expired. The root password expires 365 days after deploying the OVA file.\n  \u2022 the password was not set correctly when the appliance was deployed. This is a known issue with older versions of Unified Access Gateway.\n  \u2022 the password has been forgotten.\n\nTo test that the keyboard is mapping characters correctly, try entering the password in response to the \"Login:\" username prompt. This allows you to see each password character and may identify where characters are being misinterpreted.\n\nFor all other causes, reset the root password of the appliance.\nNote: To reset the root password you must:\n  \u2022 have login access to vCenter\n  \u2022 know the vCenter login password\n  \u2022 have permission to access the appliance console\n\nIf you have set up a Grub 2 boot loader menu password for the appliance, you will need to enter this as part of this procedure.\n\n\n  1. Restart the appliance from vCenter and immediately connect to the console.\n  2. As soon as the Photon OS splash screen appears, press e to enter GNU GRUB edit menu\n  3. In the GNU GRUB edit menu, change the line that starts with linux so that it contains linux \/boot\/$photon_linux root=$rootpartition rw init=\/bin\/bash. After changing this line, GNU GRUB edit menu should look exactly like this:\n    Note: For a FIPS appliance, the line should be linux \/boot\/$photon_linux root=$rootpartition rw init=\/bin\/bash fips=1\n  4. Press the F10 key and at the bash command prompt enter passwd to change the password.\n\n\n    New password:\n\n    Retype new password:\n\n    passwd: password updated successfully\n\n  5. Reboot the appliance reboot -f\n    \u2022 After the appliance boots, log in as root with the newly set password.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.8644683361,
        "format_confidence":0.6035205126
    },
    {
        "url":"https:\/\/docs.8x8.com\/8x8WebHelp\/VCC\/configuration-manager-vovcc\/content\/dialer-outbound-queue-setup.htm",
        "text":"Step 1: Set up an outbound queue for dialer\n\nEvery 8x8 Contact Center outbound campaign is directed via an outbound queue. The queue must define the dialing mode which determines how calls are handled when this queue is assigned to a campaign. It can be set to preview, progressive, or predictive mode.\n\nTo add an outbound queue and set up the dial mode:\n\n  1. Log into 8x8 Configuration Manager.\n  2. From the menu, go to Queues.\n  3. Select to add an outbound queue.\n  4. Define the following queue properties.\n  5. Enter a name, set a default priority, indicate the percentage of calls on this queue to be recorded.\n  6. Dial Mode (for campaigns): Select the dialing mode for campaigns from the drop-down list of choices. Dial mode defines how calls will be handled when this queue is assigned to a campaign.\n\n    Note: This progressive or predictive dial mode when applied to a queue impacts the dialing mode in call back scenarios via the API or the IVR.\n\n    \u2022 Preview dialer: Offers agents the preview of the customer record before dialing out, either automatically or via agent action.\n    \u2022 Progressive dialer: Reacts to agent state and uses available data to make calls on behalf of agents in the background. Dials at a 1:1 ratio (one call for every agent in available status), making calls until a live customer answers. There is a reduced risk of abandonment but higher agent wait times.\n    \u2022 Predictive dialer: Monitors all agents status for high-performance outbound dialing, predicting the availability of agents before they end their current call in order to reduce wait times as much as possible. This mode offers built-in compliance checks for abandonment rate.\n  7. If you select progressive or predictive dial mode, you will be presented with a Dialer compliance warning. At the prompt for dialer compliance, click accept. To learn more about the rules and regulations for dialer compliance click here.\n\n    Note: The compliance warning is presented only the first time you use that mode prompting for your acceptance.\n\n  8. Abandon percentage: When you set the dial mode to predictive, you need to specify an acceptable percentage for abandoned calls. In Predictive mode the dialer will automatically lower the dialing pace when this threshold is exceeded to decrease the number of dialed calls in order to decrease the percentage of abandoned calls. If the rate drops far enough below the target then the opposite is true and the dialer will continue to increase the pace until the maximum percentage is reached or all agents are busy.\n\n    Note: This field is applicable to predictive dialing mode only.\n\n  9. Select the audio file for agent whisper. This audio is played to the agent on accepting a call from the queue, typically used to quickly remind the agent about the intent or context of the call. Learn how to add an audio file for agent whisper.\n  10. Click Save to save the queue properties.\n  11. Proceed to the remaining steps to identify members serving the queue, to add rules for interaction priority, and to set an SLA for the queue.\n    A message displays that the queue is created successfully. Learn more about creating an outbound phone queue.",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.966417551,
        "format_confidence":0.7013947368
    },
    {
        "url":"https:\/\/www.ait-themes.club\/doc\/search-form-element-2\/",
        "text":"Search Form element\n\nUpdated: August 31, 2016\n\nAIT Toolkit plugin is required to use this Element.\n\nSearch Form\u00a0element displays search form\u00a0in the header of page. The search form searches for the items defined by the Items Custom Post Type. You can search by Keyword, Category, Location or by Radius from your current position. Results are shown both on the Header Map Element and also in the content like in a basic search.\n\nAll you need to do is to enable the Search Form Element on wanted page via Page Builder and fill out options defined by the element.\nAvailable options are:\n\n  \u2022 Type \u2013 type of search form to show (Search Form, Search Sentence)\n  \u2022 Radius Units \u2013 type of units to search items (kilometers \/ miles)\n  \u2022 Radius Help \u2013 help in the radius popup\n  \u2022 Search Sentence \u2013 sentence to display. Use the keywords to show the options. Available keywords are {search-keyword}, {search-category}, {search-location}, {search-radius}. You can decide which of them you will use in search sentence.\n\nSearch Sentence type\n\n\nSearch Form type",
        "topic_id":20,
        "format_id":20,
        "topic_confidence":0.8411461711,
        "format_confidence":0.8349158168
    }
]