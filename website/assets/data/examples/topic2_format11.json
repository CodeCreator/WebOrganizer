[
    {
        "url":"http:\/\/www.onlamp.com\/lpt\/a\/6042",
        "text":"Published on (\n\u00a0See this if you're having trouble printing code examples\n\nThe Virtual Referral: Mitigating Risk by Hiring Open Source Developers\n\nby Brian W. Fitzpatrick, coauthor of Version Control with Subversion\n\nHiring a new employee is almost always a risk, and hiring the wrong employee is one of the most costly mistakes that a manager can make. As a result, tech companies spend a small fortune recruiting, interviewing, testing, and analyzing candidates in an effort to minimize the risk of selecting a bad hire. The biggest problem with traditional interviewing and evaluation methods is that they are based on limited information about a candidate, such as resume, references, and contrived problem-solving scenarios in interviews. These can be effective evaluation methods, but they too are costly and time consuming, and still carry with them a high risk of making a bad hire.\n\nIf, however, you are choosing a candidate from the ever-growing pool of programmers who contribute to open source projects, you can get an inside look at a programmer's work on actual projects with actual team members by examining the open source projects he participates in. A small amount of research here can turn up a ton of real-world data (remember that \"the internet never forgets\") and may provide you with just the information you need to make the decision to hire or not to hire.\n\nO'Reilly Connection\n\nO'Reilly Media and Greenplum Team Up to Unite the Global Geekforce\n\nO'Reilly Media launched the beta version of O'Reilly Connection at its 2005 Open Source Convention. O'Reilly Connection is a tech-centric jobs and networking site for developers and those who want to hire them. The service was conceived and created by Greenplum.\n\nBut typically, how do you find candidates who are more than just a resume to you? Obviously, you hire people you know. If you've worked with someone before, then you know what she can and can't do, so there's not much of a risk at all if you think she'll be a good hire. But eventually, you run out of people you know, so you then have to rely on referrals from people you know and trust, typically your existing employees. You have less information about referrals than about people you know, but still have considerably more than you have with someone who walks in off the street.\n\nEventually, referrals aren't enough, and you need to start looking at resumes and interviewing people that you and your employees don't know. You're forced to rely on a resume that may be chock full of lies. You've got to trust that personal references will make an objective evaluation of your candidate and that corporate references aren't lying just to get the candidate out of their hair (although typically, for legal reasons, a corporate reference will only verify the start and end dates of a candidate's employment).\n\nO'Reilly Open Source Convention 2005.\n\nBrian W. Fitzpatrick\nSubversion Tutorial\n\nThis tutorial covers the details of using Subversion in open source development, including making the transition from CVS, Subversion's differences from CVS, and using Subversion features not found in CVS.\n\nBrian W. Fitzpatrick\nSwitching from CVS to Subversion: Case Studies in Migrating Your Team to a New Tool\n\nThis talk reviews best practices for migrating to Subversion, based on case studies of teams that have already made the switch.\n\nO'Reilly Open Source Convention\nAugust 1-5, 2005\nPortland, OR\n\nHow do you get more information about these people? You need to have a strenuous interview process, so that you and others in your company can grill a person on what he knows or doesn't know and how he thinks. Still, you're going to be hiring someone based on a series of brief interviews, and that means that you have a limited amount of information. In addition, this interview process is expensive--think about how much an hour of your time costs. Now think about the cost of that plus an hour each from another three or four more engineers at your company--it adds up pretty quickly.\n\nWhat you need is a way to get even more data about your candidate--a way to be a fly on the wall in your candidate's office to see how she works in her current job. If your candidate is involved in writing open source software, with a little research and resourcefulness, you can be that fly on the wall.\n\nThis doesn't mean that all open source participants are amazing programmers who will be a perfect fit for you and your company. What this does mean is that there's a wealth of information about your candidate available to you, because most open source development takes place in public.\n\nIf a candidate lists any open source projects on his resume, review his open source work (or assign the task to one of your engineers who you know will be interviewing the candidate). Knowing what projects a candidate has contributed to will enable you to review code that he's written (production code, mind you, not interview problems), review his docs, and even review his individual commits by trolling through the project's mailing archives and version control system. Even more importantly, you can get a firsthand look at how the candidate interacts with others in a team environment by reading the project's development mailing list. You can see how he or she designs new features, triages bugs, and deals with conflict and compromise.\n\nAnd if that's not enough, keep in mind that most programmers who work on open source projects do so because they love writing software and they're passionate about their work.\n\nThis is the kind of information that most hiring managers dream about, and it's yours for the taking. Whether it leads to a hire or a rejection, you're making a well-informed decision without placing all of your trust in just a resume, a referral, and an interview. When your prospective hire is an open source programmer, it means that you have more information about them, less risk involved in hiring them, and ultimately, a better team member.\n\nThanks to Karl Fogel and Greg Stein for reading drafts of this article.\n\nBrian W. Fitzpatrick is a member of the Apache Software Foundation and currently works for Google. He has been involved with Subversion in one way or another since its inception in early 2000.\n\nVersion Control with Subversion\n\nRelated Reading\n\nVersion Control with Subversion\nBy Ben Collins-Sussman, Brian W. Fitzpatrick, C. Michael Pilato\n\nReturn to\n\nCopyright \u00a9 2009 O'Reilly Media, Inc.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.7740411758,
        "format_confidence":0.6963773966
    },
    {
        "url":"https:\/\/insights.dice.com\/2017\/06\/07\/coreml-step-forward-machine-learning\/?ads_kw=apple+machine+learning",
        "text":"Apple\u2019s CoreML a Big Step for Machine Learning\n\nCoreML WWDC 2017\nCoreML WWDC 2017\n\nMachine learning is not a fad or distraction for major tech companies. It\u2019s been in use for quite some time, although tech giants such as Google have only recently made a big show of publicizing their efforts in it. Apple is finally letting its developer group tap into it via the new CoreML framework. Here\u2019s how.\n\nCoreML is, as you may guess by the name, dedicated to machine learning. It\u2019s positioned to serve as the go-to framework for machine learning on Apple platforms such as iOS (that\u2019s why Apple branded it with the \u2018core\u2019 moniker).\n\nStill officially in beta, CoreML is positioned as something that will be ready for use with iOS 11. An important note:\u00a0users don\u2019t need new hardware for an app to take advantage of CoreML; it\u2019s a framework, and not reliant on any specific hardware or on-device functionality.\n\nThe first thing developers will want to do is find a machine learning model. Apple doesn\u2019t have any restrictions here, per se, but does offer up some widely used models as a sort of \u2018best practices\u2019 for CoreML.\n\nBut all models touted\u00a0by Apple hint at a fundamental problem with CoreML, as well. Each model is meant for scene detection, which is how apps can scan images for faces, trees or other objects. As Apple states:\n\nSupported features include face tracking, face detection, landmarks, text detection, rectangle detection, barcode detection, object tracking, and image registration.\n\nCoreML Vision doesn\u2019t access machine learning models via an API. Instead, Apple has several classes for implementing the models. VNImageRequestHandler is \u201can object that processes one or more image analysis requests pertaining to a single image,\u201d for example. In lay-terms, it\u2019s the request to scan an image for a particular feature, such as a face. Scanning multiple images will require the VNSequenceRequestHandler class.\n\nWithin Vision, you can get quite detailed. Developers can scan for faces, but also facial landmarks like eyes or a mouth. In a crude way, this is how those apps that force a smile on a photographed face\u00a0work; they find your mouth, then present a layer that \u2018adds\u2019 a smile.\n\nCoreML Architecture WWDC 2017\nCoreML Architecture WWDC 2017\n\nCoreML Vision is deep, and will be attractive for simple-purpose apps. Developers who try to corral the entirety of this framework will have cumbersome codebases to support. An example: Apple has five classes dedicated to object detection and tracking, two for horizon detection, and five supporting superclasses for Vision. It\u2019s a lot to take in, and this is all before we convert the model.\n\nWhile pulling your model into the app\u2019s code is necessary, Apple also asks developers to convert it to a CoreML-friendly framework. It has a fairly easy-to-use conversion for most well-known models, but developers who use something not officially recognized may need to create their own conversion tool. Apple notes that developers will be \u201cdefining each layer of the model\u2019s architecture and its connectivity with other layers,\u201d but only says they should \u201cuse the conversion tools provided by Core ML Tools as examples.\u201d There\u2019s no strong guidance there.\n\nIf you need to create a Vision app that\u2019s a bit more robust, CoreML has an API. It\u2019s meant for custom workflows and \u201cadvanced use cases.\u201d\n\nVision is only one aspect of CoreML, though. Speech and text also play a big role, which is where Foundation comes into play. Foundation may have a lot of moving parts, but its Strings and Text structs and classes make an appearance in CoreML. Most notably, NSLinguisticTagger will help apps analyze speech and break it into readable chunks with localization.\n\nThe third part of all this is the subtlest player. GameplayKit helps evaluate \u201clearned decision trees\u201d in CoreML. It\u2019s basically the logic engine driving all of the Vision and Foundation modules. As the framework\u00a0is optimized for on-device performance, it takes a strong team to make it work. Vision, Foundation and GameplayKit are the stars, while frameworks such as Accelerate and Basic neural network subroutines (BNNS) pick up some of the slack.\n\nUsing CoreML isn\u2019t going to be easy. It has a lot of moving parts, and there\u2019s an exhaustive list of customizations that can be made. Still, if machine learning is something your app can take advantage of, it\u2019s likely worth the trouble. CoreML is a part of\u00a0Apple\u2019s universe now, and it\u2019s going to take an increasingly larger role as time goes on. Now\u2019s the best time to get acquainted with it.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9729292989,
        "format_confidence":0.7701945901
    },
    {
        "url":"http:\/\/techmenow.in\/the-evolution-of-machine-learning\/",
        "text":"\n\nThey\u2019re pouring resources and attention into convincing the world that the machine intelligence revolution is arriving now. They tout deep learning, in particular, as the breakthrough driving this transformation and powering new self-driving cars, virtual assistants, and more.\n\nDespite this hype around the state of the art, the state of the practice is less futuristic.\n\nSoftware engineers and data scientists working with machine learning still use many of the same algorithms and engineering tools as they did years ago.\n\nThat is, traditional machine learning models \u2014 not deep neural networks \u2014 are powering most AI applications. Engineers still use traditional software engineering tools for machine learning engineering, and they don\u2019t work: the pipelines that take data to model to result end up built out of scattered, incompatible pieces.\u00a0 There is change coming, as big tech companies smooth out this process by building new machine learning-specific platforms with end-to-end functionality.\n\nLarge tech companies have recently started to use their own centralized platforms for machine learning engineering, which more cleanly tie together the previously scattered workflows of data scientists and engineers.\n\nWhat goes into a machine learning sandwich\n\nMachine learning engineering happens in three stages \u2014 data processing, model building, and deployment and monitoring. In the middle we have the meat of the pipeline, the model, which is the machine learning algorithm that learns to predict given input data.\n\nThat model is where \u201cdeep learning\u201d would live. Deep learning is a subcategory of machine learning algorithms that use multi-layered neural networks to learn complex relationships between inputs and outputs. The more layers in the neural network, the more complexity it can capture.\n\nTraditional statistical machine learning algorithms (i.e., ones that do not use deep neural nets) have a more limited capacity to capture information about training data. But these more basic machine learning algorithms work well enough for many applications, making the additional complexity of deep learning models often superfluous. So we still see software engineers using these traditional models extensively in machine learning engineering \u2014 even in the midst of this deep learning craze.\n\nBut the bread of the sandwich process that holds everything together is what happens before and after training the machine learning model.\n\nThe first stage involves cleaning and formatting vast amounts of data to be fed into the model. The last stage involves careful deployment and monitoring of the model. We found that most of the engineering time in AI is not actually spent on building machine learning models \u2014 it\u2019s spent preparing and monitoring those models.\n\nThe meat of machine learning \u2014 and avoiding exotic flavors\n\nDespite the focus on deep learning at the big tech company AI research labs, most applications of machine learning at these same companies do not rely on neural networks and instead use traditional machine learning models. The most common models include linear\/logistic regression, random forests, and boosted decision trees. These are the models behind, among other services tech companies use, friend suggestions, ad targeting, user interest prediction, supply\/demand simulation, and search result ranking.\n\nAnd some of the tools engineers use to train these models are similarly well worn. One of the most commonly used machine learning libraries is scikit-learn, which was released a decade ago (although Google\u2019s TensorFlow is on the rise).\n\nThere are good reasons to use simpler models over deep learning. Deep neural networks are hard to train. They require more time and computational power (they usually require different hardware, specifically GPUs). Getting deep learning to work is hard \u2014 it still requires extensive manual fiddling, involving a combination of intuition and trial and error.\n\nWith traditional machine learning models, the time engineers spend on model training and tuning is relatively short \u2014 usually just a few hours. Ultimately, if the accuracy improvements that deep learning can achieve are modest, the need for scalability and development speed outweighs their value.\n\nAttempting to stick it all together \u2014 tools from data to deployment\n\nSo when it comes to training a machine learning model, traditional methods work well. But the same does not apply to the infrastructure that holds together the machine learning pipeline. Using the same old software engineering tools for machine learning engineering creates greater potential for errors.\n\nThe first stage in the machine learning pipeline \u2014 data collection and processing \u2014 illustrates this. While big companies certainly have big data, data scientists or engineers must clean the data to make it useful\u2014 verify and consolidate duplicates from different sources, normalize metrics, design and prove features.\n\nAt most companies, engineers do this using a combination SQL or Hive queries and Python scripts to aggregate and format up to several million data points from one or more data sources. This often takes several days of frustrating manual labor. Some of this is likely repetitive work, because the process at many companies is decentralized \u2014 data scientists or engineers often manipulate data with local scripts or Jupyter notebooks.\n\nFurthermore, the large scale of big tech companies compounds errors, making careful deployment and monitoring of models in production imperative. As one engineer described it, At large companies, machine learning is 80 percent infrastructure.\u201d\n\nHowever, traditional unit tests \u2014 the backbone of traditional software testing \u2014 don\u2019t really work with machine learning models, because the correct output of machine learning models isn\u2019t known beforehand. After all, the purpose of machine learning is for the model to learn to make predictions from data without the need for an engineer to specifically code any rules. So instead of unit tests, engineers take a less structured approach: They manually monitor dashboards and program alerts for new models.\n\nAnd shifts in real world data may make trained models less accurate, so engineers re-train production models on fresh data on a daily to monthly basis, depending on the application. But a lack of machine learning-specific support in the existing engineering infrastructure can create a disconnect between models in development and models in production \u2014 normal code is updated much less frequently.\n\nMany engineers still rely on rudimentary methods of deploying models to production, like saving a serialized version of the trained model or model weights to a file. Engineers sometimes need to rebuild model prototypes and parts of the data pipeline in a different language or framework, so they work on production infrastructure. Any incompatibility from any stage of the machine learning development process \u2014 from data processing to training to deployment \u2014 to production infrastructure can introduce error.\n\nMaking it presentable \u2014 the road forward\n\nTo address these issues, a few big companies, with the resources to build custom tooling, have invested time and engineering effort into creating their own machine learning-specific tools. Their goal is to have a seamless, end-to-end machine learning platform that is fully compatible with the company\u2019s engineering infrastructure.\n\nFacebook\u2019s FBLearner Flow and Uber\u2019s Michelangelo are internal machine learning platforms that do just that. They allow engineers to construct training and validation datasets with an intuitive user interface, decreasing time spent on this stage from days to hours. Then, engineers can train models with (more or less) the click of a button. Finally, they can monitor and directly update production models with ease.\n\nServices like Azure Machine Learning and Amazon Machine Learning are publicly available alternatives that provide similar end-to-end platform functionality but only integrate with other Amazon or Microsoft services for the data storage and deployment components of the pipeline.\n\nDespite all the emphasis big tech companies have placed on enhancing their products with machine learning, at most companies there are still major challenges and inefficiencies in the process. They still use traditional machine learning models instead of more-advanced deep learning, and still depend on a traditional infrastructure of tools poorly suited to machine learning.\n\nFortunately, with the current focus on AI at these companies, they are investing in specialized tools to make machine learning this work better. With these internal tools, or potentially with third-party machine learning platforms that are able to integrate tightly into their existing infrastructures, organizations can realize the potential of AI.\n\nA special thank you to Irving Hsu, David Eng, Gideon Mann, and the Bloomberg Beta team for their insights.\n\nFeatured Image: Bryce Durbin\n\n\nPlease enter your comment!\nPlease enter your name here",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.7995528579,
        "format_confidence":0.9602155089
    },
    {
        "url":"http:\/\/www.technewsworld.com\/story\/80634.html?rss=1",
        "text":"Safeguarding Corporate Information in the Cloud [Download Report]\nWelcome Guest | Sign In\n\nShould Everyone Learn to Code?\n\nShould Everyone Learn to Code?\n\n\"I do think that everybody should learn to code, at least on a basic level,\" said Linux Rants blogger Mike Stone. \"It would teach them to break down a problem into small, manageable portions and solve each of those parts logically.\" It's actually \"less about the code itself than solving a problem logically,\" he said. \"That's a skill that I think everybody should have.\"\n\nBy Katherine Noyes LinuxInsider ECT News Network\nJun 23, 2014 7:00 AM PT\n\nSo the dog days of summer are upon us once again here in the northern reaches of the Linux blogosphere, and for countless young people out there, that means it's time for camp.\n\nSome, of course, will take this time to pursue their sweaty fun in the great out-of-doors alongside our friends the ticks and mosquitoes. Linux Girl wishes those hearty souls well from the chilly confines of her arctic lair.\n\nMany others will go to code camp instead, and learn skills that will last them a lifetime.\n\n'It's Reasonably Specialized'\n\nLinux Girl\n\nCoding skills figure prominently in the news with surprising regularity these days, and the prevailing message is that anyone can -- and really should -- learn them.\n\nJust last week, it was Google's launch of Made with Code, an effort to convince girls that programming is cool. Even President Obama recently extolled the virtues of programming.\n\nYet is programming really something everyone should learn to do? That's a point worth pondering -- and none other than Linus Torvalds has reached a conclusion.\n\n\"I actually don't believe that everybody should necessarily try to learn to code,\" Torvalds said in a recent interview. \"I think it's reasonably specialized, and nobody really expects most people to have to do it. It's not like knowing how to read and write and do basic math.\"\n\nDown at the blogosphere's Punchy Penguin Saloon, Linux Girl couldn't resist taking a small poll to see how FOSS fans' opinions compare.\n\n'It Seems Absurd'\n\n\"I'm gonna go with Linus on this one,\" offered Google+ blogger Kevin O'Brien. \"It seems absurd to me that there should not be room for a little variety in what people do.\n\n\"I think everyone who has a desire to code should have that opportunity, and in this day and age I think a little basic literacy in computers is no bad thing,\" O'Brien added. \"But that needn't take the form of coding.\"\n\nLanguage and communications skills are the most essential ones to get out of school, O'Brien opined. \"It is hard to succeed in any vocation without that.\"\n\n'It Is Not a Life Skill'\n\nGoogle+ blogger Brett Legree saw it similarly.\n\n\"I'm with Linus on this one,\" he told Linux Girl. \"I mean, I wouldn't expect that everyone should become a nuclear engineer or a doctor or a financial analyst, so coding is the same thing, in my mind.\n\n\"It is a great choice for a career (or even a hobby!), but it is not a life skill like cooking and so on,\" Legree added.\n\n\"Why should everyone need to learn coding?\" SoylentNews blogger hairyfeet agreed. \"Most folks don't NEED to know coding, especially if the devs are doing their jobs and making software that works well and is intuitive.\n\n\"To say everyone should learn to code is as stupid as saying everybody should be able to do open-heart surgery, when a good 99.997 percent of people would NEVER be in a position to use it,\" hairyfeet added.\n\n'Is a Great Chef Made Greater?'\n\n\"Coding is a very useful skill, and I hope that at least one of my children learns to code,\" offered Chris Travers, a blogger who works on the LedgerSMB project. \"However, I do not think everyone should.\"\n\nCoding is \"a very generally applicable skill and it is also a skill which is useless by itself and outside of any context,\" Travers explained. \"Coding can help solve problems, but it is not the only factor in any solution. Context matters.\"\n\nFor those who want to pursue a career in a knowledge industry or any technical or scientific field, \"some coding experience would be extremely helpful,\" Travers concluded. \"Is a great chef made greater by knowing how to code? I doubt it.\"\n\nIndeed, \"many people don't want and\/or need to learn to code well enough as to write applications,\" Google+ blogger Gonzalo Velasco C. concurred.\n\n'Some Are Just Terrible'\n\nCoding is not for everyone, but school computer classes should include it in order to identify new talents and \"lead them into their real vocation before they choose the wrong career path and a bright prospect is lost,\" Google+ blogger Rodolfo Saenz suggested.\n\n\"Who knows how many Linus Torvaldses or Steve Jobses are lost because of lack of exposure to their real vocations?\" he added.\n\n\"I think everyone should be presented with the opportunity to code, but I don't even for a moment think that everyone has the knack,\" consultant and Slashdot blogger Gerhard Mack opined.\n\n\"I view it as the same as anything else: Some people are great at sports; some (like me) are terrible. Some people are great at music and can do amazing things with little effort and some are just terrible no matter how much effort they put in.\n\n\"I'll never forget one person in my high school computer science class who, despite being a 'straight A' student in everything else, never managed to learn to code,\" Mack recounted.\n\n'A Lifetime Process'\n\n\"Everyone 'should' learn to code, but coding should also learn to come to everyone,\" Hyperlogos blogger Martin Espinoza told Linux Girl.\n\n\"Where is the foolproof tinkertoy programming environment that has been prophesied by everyone and their brother in computing since time immemorial?\" Espinoza asked. \"The computer is vastly more useful if you know how to program it, and it has gotten easier in some ways over the years, but not really so much easier that everyone is going to take up programming.\n\n\"Programming well is a lifetime process, not something you can expect the average computer user to do unless the tools do most of it for them,\" he concluded.\n\n'Linus Is Naive'\n\nGoogle+ blogger Alessandro Ebersol had an even stronger view.\n\n\"Linus is naive,\" Ebersol told Linux Girl.\n\n\"A computer is not a light bulb one just turns on -- it's more like a car, where one needs a driver's license to ride,\" he explained. \"Everyone should learn to code, just as everyone needs to learn how to change a flat tire or else be at the mercy of strangers.\"\n\nSimilarly, \"I do think that everybody should learn to code, at least on a basic level,\" Linux Rants blogger Mike Stone agreed. \"It would teach them to break down a problem into small, manageable portions and solve each of those parts logically.\"\n\nIt's actually \"less about the code itself than solving a problem logically,\" he added. \"That's a skill that I think everybody should have.\n\n\"I don't expect everybody to be able to write their own OS kernel or anything, or even retain the ability to write 'Hello World' past their senior year, but I do hope they would at least retain the basic ideas, and that could only be a good thing,\" Stone concluded.\n\n'I Hate to Disagree With Linus'\n\nLast but not least, blogger and former educator Robert Pogson saw it similarly.\n\n\"I hate to disagree with Linus on this one, but he's not perfect,\" Pogson began. \"Arithmetic\/reasoning\/problem-solving these days requires coding even if it's just making a spreadsheet to plan a mortgage.\"\n\nWhen Pogson started teaching, \"many high school students dropped out around Grade 10 because they just didn't get algebra\/geometry,\" he recounted. \"When I finished teaching, those same students were some of the most adept at creating and using spreadsheets to solve real problems.\"\n\n'Every Student Should Know'\n\nSome people are just not abstract thinkers, and \"computers make maths real for them,\" he added. \"The same goes for collecting, analyzing, finding or presenting information of many kinds.\"\n\nEvery high school student should know \"one or more computer programming languages very well, even if it's just a spreadsheet,\" Pogson asserted. \"I recommend PASCAL because it's very easy to learn and within days ordinary students can solve amazing problems with it.\"\n\nIn short, \"computers are the fastest way to find, examine, modify and distribute all kinds of information in school and out, especially when the information is code,\" he concluded. \"Every student should know how it's done, even if all they do is hire others to do it.\"\n\nKatherine Noyes is always on duty in her role as Linux Girl, whose cape she has worn since 2007. A mild-mannered journalist by day, she spends her evenings haunting the seedy bars and watering holes of the Linux blogosphere in search of the latest gossip. You can also find her on Twitter and Google+.\n\nFacebook Twitter LinkedIn Google+ RSS\nTargeted advertising...\nGives me the creeps -- I hate it.\nIs really helpful -- I like the personalization.\nIs effective -- it makes business sense.\nIs irrelevant to me -- I use an ad blocker.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9035847783,
        "format_confidence":0.552342236
    },
    {
        "url":"https:\/\/adtmag.com\/articles\/2004\/10\/01\/process-fortifies-software-projects-against-failure.aspx",
        "text":"Process fortifies software projects against failure\n\nFor most developers, process is a bitter pill to swallow. But employing standard best practices in a repeatable way is critical to software organizations that want to build quality solutions delivered on time, within budget and to the requirements of the users.\n\nThe key to implementing process as an effective medicine against software project failure is determining the right dose for your organization. By customizing the process to fit the needs of your enterprise, you can effectively close the gap between stakeholders\u2019 requirements and what developers produce. The next hurdle is to gain acceptance from developers, which requires modifying behavior across the organization.\n\nMost developers consider process a bitter pill to swallow. In fact, many consider it a placebo that placates managers, but does nothing to help produce software. When we look at successful projects, however, process plays a critical role. Is there a magical, Mary Poppins-esque spoonful of sugar that helps the process medicine go down? Maybe, but let\u2019s examine whether developers have some justifiable grounds for their distaste.\n\nIf we want to change developers\u2019 attitudes, we need to understand their concerns about process. Developing software for more than 20 years, I\u2019ve found that it\u2019s common for developers to view their organization\u2019s software development process as onerous, inefficient and unnecessary.\n\nWhen speaking about process, developers conjure up the idea of three-ring binders containing volumes of required forms. I recently worked with a developer on filling out the required form that documented each software change for a quarterly release. The problem was the documentation\u2019s level of detail: The form required a table with entries for every SQL statement that changed. It\u2019s easy to see how a developer might call this onerous.\n\nSequential processes like the waterfall model can be inefficient. With it, the workflow can be understood and managed easily because each task follows in a certain, unchanging order. However, development is more efficient when it is collaborative and concurrent with other activities. For example, doing architectural tasks during requirements gathering raises technical issues early and avoids costly rework later.\n\nDevelopers see some process tasks as unnecessary. For example, one organization required all technical artifacts be reviewed by the quality management (QM) team at the end of every phase. Unfortunately, the QM team did not understand the technologies involved. So instead of a review of the technical content, the meetings would focus on document formats. In one case, the significant review discussion debated whether a document\u2019s section should use a table instead of a bulleted list. Developers walk away from these reviews frustrated and questioning their value.\n\nLet\u2019s be honest. Developers are notorious for only wanting to code. Most would be happy to skip planning, reviewing, talking to users and testing to sit at their computers and code. To many developers, everything else is a nuisance or doesn\u2019t seem important to producing software.\n\nCould they have a point?\nTo some extent, developers\u2019 gripes are justified. Many organizations have outdated processes that don\u2019t reflect current industry best practices. Viewed individually, each inefficiency in a process may not seem significant, but over the entire project life cycle they can become weighty baggage.\n\nWorse, when official processes are viewed as too cumbersome or useless, developers and even project managers just don\u2019t implement them. This presents a huge risk to the organization\u2019s ability to produce quality software on time -- and those three-ring binders just become dusty and old. An unused process is worse than no process at all because it seems to prove that process adds no value.\n\nSoftware production\u2019s illness requires medicine\nA common industry belief is that 70% of IT projects fail. This notion is a little dated and imprecise. The 2003 Standish CHAOS research suggests 15% of IT projects failed and 51% were \u201cchallenged.\u201d These results show that IT is hobbled by low project success rates. Of the top 10 identified success factors, most are within the domain of process, including user involvement, project management, scoping, methodology and requirements management.\n\nSometimes, we view process as the collection of stuff in the three-ring binders. However, I\u2019d like to explore another view of process -- one that thinks of it as not just the medicine, but the prescription.\n\nIn this view, the major goal of process within an organization is to apply best practices --those techniques and activities that have been proven to improve efficiency and quality in software production. By following a prescriptive process, an organization sees repeatable results. It\u2019s hard to argue against using process this way -- employing best practices in day-to-day software production.\n\nIn fact, implementing best practices is critical to repeating project successes. Producing software with teams is an inherently complex task. To repeatedly produce software that is reliable, on time, within budget and that meets or exceeds users\u2019 expectations requires dynamic workflows that foster collaboration, reveal risks and ensure consistent quality.\n\nUnfortunately, many organizational processes aren\u2019t designed this way. So what can be done?\n\nDesigning processes\nCuring software project ailments is similar to visiting a doctor. In the same way a doctor prescribes the most effective medicine for a patient\u2019s needs, companies need to design their prescriptive processes using best practices that match their organization\u2019s unique needs.\n\nThere are four key practices that are critical in designing any software development process:\n\n1. Develop iteratively. A process that uses iterative development will have a number of iterations through the life cycle. Unlike the waterfall methodology, each iteration will have a set of tasks involving the disciplines of requirements, analysis, design, coding, test and deployment. The amount of work within each discipline varies depending on where the iteration is within the life cycle. The main idea is to focus on addressing the highest project risks early. For example, early iterations focus on architecture. This is not the same as a design phase. Instead, the developers produce and test a software executable or release designed to address the architectural risks in the project. In a classic waterfall, the architecture is not tested until the test phase.\n\n2. Manage requirements. It\u2019s common wisdom that stating all requirements for a large-scale program is practically impossible. Yet many processes assume it can be done. A better approach is to have the process account for changes in requirements. This is not to say that anything goes until deployment. What we want is a systematic approach that not only captures requirements, but manages the agreement on requirement changes between the development team and project stakeholders.\n\nEfficient processes leverage the iterative approach by focusing on risky requirement tasks early. For example, a common goal of the first iteration is to set the scope of the project, going deep enough to uncover significant requirements, but leaving less-significant requirement details for later iterations.\n\n3. Manage change. Change management is critical for organizations building high-performance software teams. As tasking becomes more concurrent, and more releases and environment configurations are involved, the complexity of managing all these pieces increases dramatically. What may have worked in the past will not suffice. An effective process must include a well-defined approach to managing the changes in software, documents, environments, change requests and releases, as well as the ability to track these changes.\n\n4. Continuously verify quality. Verifying quality becomes more difficult in an iterative framework. A non-iterative process usually has completed deliverables due at specific times. In iterative development, documents and functionality progressively mature through the life cycle, so quality must be verified at multiple points for some deliverables. This step-wise verification becomes critical to realizing the final goal.\n\nThis iterative attention to quality can also help to accelerate development times. A practice I\u2019ve seen successfully employed on development projects is weekly quality builds. Each week, an integration build is created and tested, and defects are logged. This raises defects very early in the cycle. Developers can fix bugs before the next quality build, greatly reducing the time to do system testing and defect cycles.\n\nFitting organizational needs\nThe biggest key to designing an effective process is determining the correct dose of process medicine. It\u2019s easy to make a comprehensively large process that blankets all risks, but that is terribly inefficient. The trick is to apply the smallest dose of medicine that is still efficacious. Albert Einstein put it another way: \u201cEverything should be made as simple as possible, but no simpler.\u201d\n\nHow do you know what must be done in your process? Unfortunately, there\u2019s no quick answer. If we look at the goals of the organization, we\u2019ll be able to identify the risks that might prevent us from successfully achieving those goals. The process should be fitted to address the significant risks.\n\nLet\u2019s look at an example. One organization was designing its process and asked the question, \u201cWhat level of design work is needed for our organization?\u201d The organization had been using Macromedia\u2019s ColdFusion, a mature architecture framework, for some time and the application projects were similar to previously built projects. From an architectural perspective, the risk in new development was low vs. that of an organization tackling a complicated J2EE project for the first time. So their process required a lower level of de... (truncated)",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9876557589,
        "format_confidence":0.9602319002
    },
    {
        "url":"http:\/\/webzin.info\/should-the-web-have-a-universal-design-system.html",
        "text":"Home Should the Web Have a Universal Design System?\n\nShould the Web Have a Universal Design System?\n\n10\u00a0min read\nComments Off on Should the Web Have a Universal Design System?\n\nUnlike mobile applications, the web has no set design guidelines to which a designer can refer. Instead, each web project tends to be a blank canvas. There are frameworks like Material, Bootstrap, and others which provide a base, but no set guidelines which span the web as a whole.\n\nThe result is a wide-ranging and diverse web, but one with a lack of cohesiveness, particularly in terms of user experience. Navigations differ in placement, structure, and overall design. Layouts alternate in width. Text sizes and typographic scales vary wildly. And a wide range of differing components, interactions, and user interface elements are used.\n\nDesign systems ensure consistency between apps, resulting in a more cohesive product\n\nThe lack of a set design system for the web is due to its open source nature, and lack of ownership. No company or organization has the power to enforce guidelines or standards. The closest anything or anyone comes to impacting the way we design is Google, who can affect your search rankings based on factors such as user experience, responsiveness, and code structure. On the other hand, mobile operating systems like iOS and Android have the power to enforce certain application structures, user experience practices, and standards. Design systems ensure consistency between apps, resulting in a more cohesive product, and one that is easier to use and understand for the end user. It also enhances performance and optimization, as well as accessibility.\n\nDespite such a defined set of guidelines in both cases of iOS and Android, designers still find ways to differentiate through aspects like color, layout, and design details. In these circumstances it\u2019s still entirely possible to achieve outstanding \u00a0and unique designs which still fall within the guidelines.\n\nConversely, the web is an absolute blank canvas. There is the ability to take a design and user experience in any direction desired. On one hand, it\u2019s what makes the web so attractive, diverse, and abundant. On the other hand, it can lead to a confusing experience for many people: one that is highly inaccessible, inconsistent, and uses a variety of sub-optimal and dark user experience practices.\n\nThe case of iOS and Android show just how rich and diverse a digital product or ecosystem can be, even under such regulation and moderately-strict guidelines.\n\nThis poses the question of whether a set of open source guidelines should be introduced for the entire web. Whether it comes from W3C, is a unified effort between major browsers, or is devised by a group of designers, it could improve the web for all. There would still be great scope for producing unique designs, while ensuring the web reaches much more acceptable levels of accessibility and usability as a whole. Designers and user experience professionals could contribute to this as an open source project, pushing forward the progress of the entire web.\n\nIt\u2019s not just web applications this system should apply to. Whether it\u2019s a blog, portfolio, landing page, or wiki, they are all still usable products. They still require important user experience considerations such as accessibility, navigation, color practices, and typography scales. Many companies consider such aspects, while many ignore them either through choice, misjudgement, or lack of consideration. It\u2019s an area which is so fragmented under the current system, and does not work appropriately for everyone. That includes those with a disability, visual impairment, or lack of familiarity with computers and the web. These users should be designed for first.\n\nAs it stands, the primary consideration is often the design visuals: making something impressive, unique, and eye-catching. Often this desire to differentiate can lead to oversights with user experience, and design choices like unique navigation solutions which are confusing and unfamiliar to most.\n\nGoogle is a prime example of a company who have developed a set of guidelines and applied them with absolute consistency across mobile and web. Whether you switch from Google Keep on iPhone, to Google Drive on the web, the user experience and design elements remain consistent. Then when switching between products on the web like Play Store or YouTube, it again remains consistent.\n\nThis ease of use and transition from one product or site to another should be a model to follow for others. It puts the user first, making for an entirely accessible and understandable experience. Google is beginning to take this even a step further, as they introduce Android apps and web-based equivalents that work at desktop size. With products like Chromebooks, it makes the transition between devices even more seamless.\n\nThe closer we can get to a cohesive design system across the web\u2026the better it will be\u2026for all parties involved\n\nThe closer we can get to a cohesive design system across the web as a whole, the better it will be in the long run, for all parties involved. This means having systems span much further than just one company.\n\nIBM or Airbnb may perfect their design systems to the nth degree and apply them with excellent consistency. However, as soon as a user switches to another product or service, their design system is likely to be wholly different, from typography and layout, to navigational practices. That\u2019s why it needs to be looked at as an issue from further afar. And apps are the closest example we have to how successful this can be as a means to improve the everyday lives of users.\n\nEasily Whip Up Custom Videos for Your Brand with Videobolt \u2013 only $24!\n\nLoad More Related Articles\nLoad More By\u00a0Ben Bate\nLoad More In\u00a0\nComments are closed.\n\nCheck Also\n\nA Guide to Payment and Invoicing Tools for Designers\/Developers\n\nYou're reading A Guide to Payment and Invoicing Tools for Designers\/Developers, originally\u2026",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.807772398,
        "format_confidence":0.8990033269
    },
    {
        "url":"https:\/\/www.scrumalliance.org\/agile-resources\/agile-transitions-for-managers",
        "text":"Agile Transitions for Managers\n\nIdeally, a company's transition to an Agile environment is a welcome and positive experience for all managers and teams right from the start. But when reality sets in, some resist the change, particularly those in management.\n\nFrom their perspective, Agile would create problems by lowering their status and worth to the company and negatively impacting their teams\u2019 performance. Senior consultant and Certified Scrum Trainer\u00ae Arne \u00c5hlander finds that these concerns are not uncommon. A comment made by one such manager to \u00c5hlander during a company\u2019s Agile adoption typifies their skepticism: \u201cI'm afraid that this new way of working will result in lower quality products,\u201d he told \u00c5hlander. \u201cI cannot see how making the whole team responsible can really help us.\u201d\n\n\u00c5hlander has been helping companies in business development for over 20 years and is the founder of Aqqurite, a Lean and Agile product management company based in Sweden. He finds that gaining early confidence from such managers can be challenging but becomes easier when they see that Agile can actually deepen their worth and improve their teams\u2019 performance.\n\nOvercoming Objections\n\n\u00c5hlander recalls a recent transition he facilitated at a Swedish mobile company. The decision to adopt Agile \u2014 and Scrum in particular \u2014 had been made and pilot teams had been chosen. Implementation was underway when Stefan, a new R&D manager with a command-and-control style, was introduced to the project.\n\nHe immediately expressed wariness toward using Scrum but was willing give the pilot projects a try after speaking with a few mangers who were experiencing positive outcomes. It turned out that Stefan was not the only one with reservations, and a few reluctant project managers from one team approached him with the suggestion of abandoning Scrum and returning to their old way of working.\n\nBut by the time they had approached him, Stefan had observed so many positive effects from the other teams, his attitude had changed, and his response was to continue using Scrum. But it gets better. Stefan was so impressed, he expanded the initial plan from a few pilot teams to 15.\n\nWhile Stefan and managers like him are ready to support Agile after experiencing success, their initial buy-in on a pilot project is often based on a willingness to listen to others and be open to change.\n\nBut for a successful transition to take place, it is just as important for the ScrumMaster\u00ae or facilitator to listen to the managers\u2019 initial concerns and appreciate the academic and professional cultures that have shaped their perspectives.\n\nA Historical Perspective\n\nIn a three-part article series titled \"How the West Was Lost to Managerial Totalitarianism,\" Kurt Nielsen, Certified Scrum Trainer\u00ae and CEO at AgileLeanHouse A\/S, points out that our society has been actively promoting a culture of competition and performance rating for the past 70 years. In academic and professional environments, people are often forced to work against each other because it has been accepted as a means for making people work harder and produce results.\n\nNielsen refers to those who have totally bought into this thinking today as neo-Taylorists, named after Frederick W. Taylor, a management consultant from the late 19th and early 20th centuries. People, of course, want to survive, so current managers conditioned by the Taylorist model will be competitive and likely feel threatened by the thought of giving up their status as a command-and-control manager.\n\nIn politics, reality shows, the stock market and even the news, the junior person's role models have been people who focus on increasing and wielding their own power. Even worse, the educational system teaches young people that success is about being able to figure out the system.\n\nAs a result, the novice thinks this is the way to operate. Is it any wonder, challenges Nielsen, that companies attract job candidates whose main qualification is knowing how to tweak and game systems?\n\n\u201cIn companies, we now have a massive educational challenge in de-teaching people these characteristics and getting them back to true learning and value focus,\u201d says Nielsen.\n\nRecalling his early career days, Nielsen writes, \u201cI had lived my educational and professional life in this neo-Taylorist period, in fact I thought that was how things just were. I tried to be like that, with immense inner tensions as a result, because intuitively, I felt it was completely wrong, but I was always afraid that a more likely explanation was that I was not fit for the modern world.\n\n\u201cWhen I stumbled upon Scrum in 2005, it was a minor revelation to me. Ken Schwaber dared to call for a show of cards in his 2001 book \u2018Agile Software Development with Scrum.\u2019 It was really the emperor's new clothes; there was a better way and it was there a long time before this other stuff as well.\u201d\n\nAgile trainers and company ScrumMasters find that everyone benefits when they acknowledge the cultures that have influenced the reluctant managers and listen to their concerns. In turn, these managers are more willing to listen to others and give Agile a try. As they adopt this style of management, those at the executive level need to support middle managers as they integrate the expectations of upper management with the needs of their teams.\n\nThe Servant Leader\n\nWhile the Agile environment embraces collaboration, \u00c5hlander would not expect a manager to seek input from the team in every situation. In simple situations where a proven method is in place, the manager may elect to make the decision himself or keep in place the process that has been used successfully in the past.\n\nFor a deeper dive into this topic, \u00c5hlander recommends an article titled \"A Leader's Framework for Decision Making,\" by David J. Snowden and Mary E. Boone. The authors agree that exhaustive communication among managers and employees is not usually required in simple matters.\n\n\nHowever, the authors recommend that even in seemingly simple situations, managers should remain open-minded. At times, issues may be misclassified because of oversimplification, overemphasis on past experiences, and failure to notice that the context has changed.\n\nIn \u00c5hlander's experience working with companies internationally, he has found that teams are usually in a good position to make decisions. The manager's worth in moving a project forward is not a matter of telling others what to do but of creating the best circumstances for the team to work effectively and efficiently. In effect, he leads by serving.\n\n\"A servant leader is a leader is one who paves the way for teams and individuals at the same time as challenging [them] to reach higher levels and to become what they never thought was possible,\u201d \u00c5hlander says. This leader\u2019s expectations go beyond capabilities expressed in the past, to new potential ready to unfold.\n\nIn the words of a team member during an early retrospective \u00c5hlander recently facilitated: \"After introducing Scrum, it has been much easier to ask the other team members for help, and it has been much easier to receive help. We work more as a team than before even if we are responsible for the same component as before.\"\n\nGrowing Empowerment\n\nAndrew Dahl, who has helped lead Agile transitions at Zendesk and currently at as VP Product Development, finds that teams are most successful when managers empower their members. It\u2019s a matter of giving individuals enough autonomy to think for themselves and come up with solutions. \u201cWhen workers get that they are capable of understanding the problem and can make decisions, that's empowerment,\u201d says Dahl.\n\nSven Carlstedt, R&D manager at Assa Abloy Entrance Systems, puts it this way: \u201cThere is an inherent desire and energy in people to contribute.\u201d Carlstedt has led successful transitions at Ericsson Modems and ST-Ericsson, and is currently leading change initiatives within Assa Abloy.\n\nCarlstedt finds that people want to be a part of creating something, especially if they are engineers. \"There is a driving force within people,\" he says. From Carlstedt's perspective, the manager is not there to motivate his team, but to provide an environment where team members motivate themselves.\n\n\"Scrum is not about being nice to people,\" says Carlstedt. Rather, he finds, it is about creating a workspace that encourages team members to contribute. \u201cAgile has taught me to put more focus on the \u2018how\u2019 instead of \u2018what\u2019 [or]\u2019when\u2019 and thereby really put focus on growing the team or organization,\u201d he says. \u201cUncovering the underlying reasons to a problem with why we fail or have challenges has often led me to initiate coaching towards specific behaviors that the organization needs to master.\u201d\n\nClosing Thoughts\n\nWhen implementing an Agile transition company-wide, Nielsen finds that commitment from at least one senior executive is needed. He suggests starting the transition with one good project, a committed team, and someone in the company to serve as the ScrumMaster.\n\nWith that done, define a worthwhile purpose for your project; be transparent and visible by openly displaying issues and opportunities; institutionalize learning so everyone can contribute their thoughts and be heard; and promote self-organizing teams and respect for one another.\n\n\u00c5hlander recognizes that an Agile transition may not be an easy path for all managers initially, but it becomes much easier as they understand and demonstrate the values of Scrum: commitment, respect, openness, and courage. He finds that successful managers express an interest in people and support the growth of teams and individuals.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.5957568288,
        "format_confidence":0.5777492523
    },
    {
        "url":"https:\/\/www.computer.org\/csdl\/mags\/cg\/2005\/05\/mcg2005050006.html",
        "text":"The Community for Technology Leaders\nGreen Image\nIssue No. 05 - September\/October (2005 vol. 25)\nISSN: 0272-1716\npp: 6-7\nNahum Gershon , The MITRE Corporation\nJake Kolojejchick , General Dynamics C4 Systems\nIn late 2003, one of Steve Roth's largest projects was going to end in a month. His company, MAYA Viz (since acquired by General Dynamics C4 Systems), was small and a major project ending without follow-up work would put a strain on the whole organization.\nPrior to that time, Steve and his team at MAYA Viz had spent five years, on their own, and in the DARPA Command Post of the Future (CPOF) program building a research prototype of a distributed workspace called CoMotion. This prototype helped people share information, understand each other's views, and establish a common ground.\nOne month before CPOF's scheduled end, the team demonstrated a prototype to several senior US Army commanders in a series of war-gaming exercises. The results were astounding\u2014the prototype demonstrated a huge decrease in the time users spent on gathering data, freeing their time to do analysis, decision making, and sharing information throughout the team. One of the commanders saw a technology that could help transform how he commanded his organization. He successfully argued to the army leadership that they should fund an experiment where the CPOF software would be fielded to his new command, scheduled for deployment to Baghdad in early 2004.\nSteve and his small team now had a tight budget and only five months to transform their laboratory prototype into a functional product that could work with real users on tactical military networks. It was a stressful time. They had to transform not only themselves\u2014not a trivial task\u2014but also refocus the company from R&D to building production-quality software. The stakes were high\u2014mistakes could cost soldiers' lives.\nFinally, with the support of the team of CPOF companies, they did it. A number of large military units in combat use the system 24\/7. Moreover, this digital workspace system is revolutionizing military command and control. Now soldiers in different locations can collaborate simultaneously without the risk of traveling through dangerous environments to be physically together.\nLooking back\nAll of Steve's achievements were the result of his fundamental belief in the unique value of people in the decision-making process. To him, computers existed to empower human decision making, rather than replace it. Steve, a dreamer and visionary, started his career as a graduate student in cognitive psychology. Upon graduating from the University of Pittsburgh, he continued to research the use of computers in the learning process. He then moved to the Robotics Institute at Carnegie Mellon University. His project, System for Automated Graphics and Explanation (developed at CMU), allowed complex data to be represented visually in a way that humans could better see, use, and manipulate. SAGE was later commissioned by DARPA to help the military better manage its logistical operations on the battlefield.\nConvinced that their software and methods could drastically change the way humans interact with computers, Steve and his colleague, Jake Kolojejchick, launched MAYA Viz to change the world, have fun doing it, and make money (in that order).\nComing into alignment\nThe alignment of the stars that brought General Chiarelli to the CPOF exercise was never lost on Steve and Jake. The path from a laboratory exercise of a research prototype to the adoption by the army was a treacherous one. Only by having a firsthand perspective on problems and opportunities would they be able to keep from making a critical misstep that might keep the software from being adopted. Steve and Jake went to Iraq to support the fielding of the software, understand how it was being used, and expand the users' concepts for what they could do with the system.\nSteve cherished diversity and the gestalt that came from many different points of view. He enjoyed sharing his vision and giving his team the freedom to expand it from their own perspective. That a varied crew of designers and engineers who had joined a research company volunteered to go into a war zone to ensure the successful fielding of the system they created was a constant amazement to him. Steve instant messaged the team at all hours of the day and night trying to short-circuit misunderstandings and draw attention to potential problems\u2014all to facilitate the delicate transition from research to deployment.\nSteve believed that the potential applications of the CoMotion technology were in no way limited to the military. His vision was that future customers would primarily be from the private sector. But despite his success in fielding a working application, Steve dreamed that one day he would go back to research. He relished the challenge of the design problem. He relentlessly pursued architectural purity and user interface consistency, always with a loving disregard for the practical difficulties of implementing the ideal on time and under budget. He refused to accept that it could not be done. He knew that compromises could become entrenched and would erode the purity that made the work unique. Steve patiently and eloquently described his vision to everyone who worked with him and inspired them to think, dream, and change the world along with him.\n86 ms\n(Ver 3.3 (11022016))",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.5055576563,
        "format_confidence":0.979948163
    },
    {
        "url":"https:\/\/www.techworld.com\/tech-innovation\/its-raining-code-1702\/",
        "text":"The options for using open source have never been greater, and you owe it to yourself -- and your company -- to take a close look.\n\nIf you're in need of a software solution, the odds are good that you'll find an open-source project related to your problem. Free. No sales calls. No negotiations with vendors.\n\nGranted, no service contracts or tech-support numbers either, most likely. But given the low barrier to entry, it's easy to understand why thousands of companies are tempted to use open source for, at the least, those projects that fall shy of the mission-critical line.\n\nWhat's more, for companies not inclined to stray far from the warm comfort of packaged products, there are more and more open-source options offered by commercial vendors.\n\nThe next time you have a project in need of a software solution, try an experiment. Go to open-source collaboration site SourceForge, and spend five minutes running a search. The odds are good that you'll find an open-source project related to your problem. Free. No sales calls. No negotiations with vendors.\n\nGranted, no service contracts or tech-support numbers either, most likely. But given the low barrier to entry, it's easy to understand why thousands of companies are tempted to use open source for, at the least, those projects that fall shy of the mission-critical line. And for those CIOs nervous about the support and licensing issues that surround open source, well-known vendors are increasingly releasing some of their own code to the open-source community. IBM, for example, in January released 500 of its software patents to open-source software developers. Sun has announced that it will release its Solaris operating system under an open-source licence.\n\nOf course, if you're looking at open source precisely because you want to get away from those very vendors, maybe there's a better alternative: a co-operative of like-minded, open-source-loving CIOs just waiting for you to join. The options for using open source have never been greater, and you owe it to yourself -- and your company -- to take a close look.\n\nPower of co-operation\nOne of the challenges of using open source is simply finding a product that meets your needs and your quality standards. While many developers need an e-mail client or Web browser (hence, the rabid developer base for open-source projects such as Mozilla's Thunderbird and FireFox), finding a spontaneously developed tool to integrate your three retail-specific supply chain applications isn't as likely. And even if your SourceForge search uncovered such a tool, there's no guarantee that the developers wrote it with the care your enterprise requires. (To at least partially address this concern, VA Software offers SourceForge Enterprise Edition (SFEE), which helps developers create shared code environments among known collaborators, and the Tapestry portal, which lets Enterprise Edition users access information about other SFEE projects.) Plus there's the \"where did this code really come from\" legal question. To assuage these fears, some organisations have turned to co-operation with peers for their open-source development.\n\nOne of the most high-profile examples is the Avalanche Corporate Technology Co-operative, a collective development effort founded by a group of Minnesota-based companies, including Best Buy, Cargill and Jostens. Last spring, the group formed to identify opportunities for open-source development projects that could benefit all the members. The goal of this co-operative is to pool resources so that interested companies can jump into new projects while spreading out the risks and expenses. Some of the members, for instance, are too small to create full-blown development efforts on projects such as building an open-source desktop reference environment, but by combining efforts, they can all reap the rewards. Avalanche currently has a half-dozen projects under way, including one to create a reference design for an open-source desktop and another to build a business activity monitoring engine.\n\n\"We're relative novices in the whole open-source area,\" says Mike Thyken, senior vice president and CIO at bedding manufacturer and Avalanche member Select Comfort. \"We started looking at everything it was going to take to get a Linux\/open-source world put together, and we saw that it was very redundant between companies.\"\n\nSo Select Comfort joined Avalanche, contributing a membership fee of $30,000 as well as some time from technical architecture experts inside the company to Avalanche's Linux reference environment project. As a result, Select Comfort expected to receive deliverables on a Linux desktop and server architecture early in 2005, all for a \"fairly modest\" investment of money and time, Thyken says -- far less than if the company had attempted the project on its own.\n\nAside from the efficiencies of co-operative development, Avalanche members can receive other benefits as well. Avalanche's licensing agreement restricts the co-operative's code to members only, eliminating the fear that valuable code will simply be spread far and wide, without competitive benefit to the contributing members. And then there's the question of putting pressure on commercial software vendors.\n\n\"One of the original premises was that the balance of power had swayed quite heavily to the vendor side,\" says Jay Hansen, CEO of the co-operative. \"The IT consumer wanted to take back a little bit of the control over their own destiny.\"\n\nFinancial services companies are getting in on the open-source action as well. What began in 1997 as an internal effort to consolidate integration development at international investment bank Dresdner Kleinwort Wasserstein (DrKW) has become Openadaptor, a Java-based open-source integration platform. \"The most common thing you have in a bank is communication between systems,\" says Steve Howe, DrKW's VP and head of open-source initiatives. In the late 1990s, the company set out to create a standardised programming interface that would keep internal developers from having to reinvent basic code for every new project. The code behind the interface was shared with all of the company's developers, and each was able to make suggestions for -- and sometimes even changes to -- the source code. The result was a dramatic decrease in development time for connectors.\n\nThe system worked so well, in fact, that DrKW hired software development collaboration provider CollabNet, which hosts collaborative development environments for both commercial and open-source projects, to release its connector platform in open source as the organisation Openadaptor. DrKW wagered that if other organisations got involved, it would result in further enhancements to the platform. The idea worked. The core software currently sees more than 8,000 downloads a month of the organisation's site. And while no one knows exactly how many corporations are taking part, Howe says that Deutsche Bank, Halifax Bank of Scotland, HP, JP Morgan Chase and the Royal Bank of Scotland are all involved.\n\nHowe notes that Openadaptor is a case of an open-source product unlikely to see a commercial competitor. \"The really good thing about Openadaptor is that it's really lightweight. There's not much real impetus for the vendors to put that sort of thing out; they couldn't charge very much for it,\" he says. In other cases, the project could have a more dramatic impact on an existing vendor. Such is the case with Sakai.\n\nCosts and competition\nThe Sakai Educational Partners Program is a set of projects spearheaded by universities looking to provide common platforms for a variety of tasks. One of those tasks is the creation of an open-source course management system -- one aimed directly at a technology space currently occupied by Blackboard and other commercial competitors.\n\nAccording to Sakai Community Liaison James Farmer, a recent survey of 22 universities currently using Blackboard found that 100 per cent planned to drop the commercial product in favor of Sakai code. But Farmer is clear to point out that this project isn't about vendetta; it's about saving money. Having an open-source, co-operative development option would let schools reduce their costs by skipping commercial alternatives completely, or indirectly, by using the threat of open source to \"put some restraint on price,\" Farmer says.\n\nMalcolm B Brown, director of academic computing at Dartmouth College, agrees with that assessment, noting that he wouldn't be afraid to use Sakai as leverage with his vendor. But he says cost is secondary to functionality. An admittedly happy Blackboard customer (he had zero downtime on his course management system last term), he says that the school is nevertheless keeping an eye on Sakai to see if it can provide a better product than the commercial alternatives. \"The promise of Sakai is that it's written by universities for universities,\" Brown says, noting that university developers should be more aware of the complex relationships that can grow between educators and IT. \"It can be tough to make all [the instructors] conform\" to an application, Brown says. \"The platform needs to be customisable and nimble.\" And if Sakai can deliver on those needs, he says, Dartmouth would consider moving to its platform. But for now, he notes, \"It's too early to see how this is going to play out.\"\n\nGetting the bulk of educational institutions moved to a single platform would save money in another way as well. \"If you can interchange content, you will sharply reduce the cost of instruction,\" Farmer says. For instance, \"McGraw-Hill produces 24 versions of its materials to support all the electronic teaching systems,\" including Blackboard. \"If they could reduce to one standard, they could cut the cost of their media support by 80 per cent. Textbooks are expensive because faculty members are judging not just on textbooks themselves, but on the quality of the other materials available for their online courses.\" S... (truncated)",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9386200905,
        "format_confidence":0.8995754719
    },
    {
        "url":"https:\/\/www.agileconnection.com\/article\/critical-look-cmm-and-agile-through-gen-y?page=0%2C2",
        "text":"A Critical Look at CMM and Agile Through Gen Y\n\n\ndoes not preclude innovation and change. The process owners are the ones who are the gatekeepers to progress.\" [13]\n\nThe core skills needed to perform a technical job becomes secondary. These incapable managers and leaders who grew up quickly not because of their technical abilities will find it difficult to work with the programmers working for them. This in turn results in the mistrust between managers and teams.\n\nQuality groups, checklists and audits become the major activities in such organizations.\n\n\"The job of the quality department was felt to be policing rather than playing an important role in developing the system.\" [9]\n\nThe mistrust in the atmosphere results in manufacturing data, last minute documentation and an overall unhealthy atmosphere.\n\n\"The perennial problem is that of employees not being too keen on the processes and treating processes as a burden. New entrants still feel that documentation is a hell of work and the tendency for last minute documentation is a common phenomenon.\"[9]\n\nAgile by definition is a bottom-up approach. Team gets to decide what works for them. There is an added trust and responsibility on the team to retrospect and fix its own course... and most importantly this is not driven from the top or external departments. Trust leads to responsibility that leads to commitment. This contrast of \"we control our course\" vs \"we follow and work under a prescriptive process\/model\" becomes very significant differentiator.\n\nThe very first line in the Agile Manifesto states \"Individuals and interactions over processes and tools\"[14]. This being the major value definition, any and all the flavors of agile keeps this at the top of their priorities.\n\n\"Scrum believes that people, their skills and how they are harnessed are the most important factors for the success of a software development effort.\" [2]\n\n\"XP also emphasizes open workspaces, a similar\n\n\"people issue\" that is outside CMM's scope.\"[3]\n\nOnce Trust becomes the core of the working model, it results in giving more ownership to the team. The only expectation on the team would be delivering what they are good at, that is working software. \"Scrum Team: Responsible for developing functionality. Teams are self-managing, self- organizing, and cross-functional, and they are responsible for figuring out how to turn Product Backlog into an increment of functionality within an iteration and managing their own work to do so. Team members are collectively responsible for the success of each iteration and of the project as a whole.\" [1]\n\nAgile prescribes delivering in short and frequent intervals.\n\n\"Our highest priority is to satisfy the customer through early and continuous delivery of valuable software\" [15].\n\nThere are multiple objectives that get met through this principle. It gives an opportunity to realize the benefits quickly, the trivial one being reduced time to market. It gives frequent retrospections for the team to reflect on their own progress and collectively alter their course for betterment, and gives an opportunity to demonstrate what is being built for the customer and get necessary feedback to align their course to meet customer's expectations.\n\nThere is another aspect that is a direct result of these short cycles. The team gets to see the results of their hard work pretty quickly. This results in a team getting the sense of fulfillment at regular and short intervals. This makes a staggering contrast to ambiguous and abstract control charts under the supervision of a quality group that may not understand what the team is building.\n\nSuccess of a team in an Agile environment rests with team and not with Agile. The same cannot be said in a CMM atmosphere where the success is assumed\n\nAbout the author\n\nAgileConnection is a TechWell community.\n\nThrough conferences, training, consulting, and online resources, TechWell helps you develop and deliver great software every day.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9815440178,
        "format_confidence":0.9374960661
    },
    {
        "url":"https:\/\/www.linuxjournal.com\/article\/9625",
        "text":"At the Forge - Firebug\n\nby Reuven M. Lerner\n\nDuring the past year or two, Web developers have witnessed what we might call the JavaScript renaissance. That's because all the fancy Ajax, Web 2.0, mashup, interactive, collaborative, desktop-like applications that are being developed are written in JavaScript. This is possible today not only because the JavaScript language has improved, but also because browsers are increasingly compliant with Web standards. And, of course, the availability of cross-platform JavaScript libraries, such as Prototype, has added many features to the language, while simultaneously ensuring cross-platform compatibility.\n\nSo, with all the JavaScript development happening today, what is the most popular way to debug programs? That's right, it's the built-in alert function:\n\nalert(\"value of x = '\" + x + \"'\");\n\nAlerts might be unpleasant, ugly and downright annoying, but they have been the best and easiest way to debug JavaScript for several years. Sure, there have been a few JavaScript debuggers, but none of them has been all that exciting to use, let alone easy or productive.\n\nWell, I'm happy to say that the situation has now changed. Firebug is an open-source plugin for the Firefox browser that aims to be a one-stop debugging tool not only for JavaScript, but also for everything that a Web developer needs. Written by Joe Hewitt, one of the founders of the small startup Parakey, Firebug 1.0 was released in early 2007. It already has become wildly popular among Web developers, and for good reason.\n\nThis month, we look at Firebug, so that we can debug, inspect and optimize modern Web pages. Firebug already has improved my ability to debug modern Web pages dramatically, and I wouldn't be surprised if this turns out to be the case for many other Web developers.\n\nInstalling Firebug\n\nFirebug is distributed as an extension for the Firefox Web browser. It is most easily downloaded and installed from the Firebug site ( To install it, click on the download Firebug button. If you already have told Firefox this is an allowed download site, you will be able to download and install this Mozilla extension. If not, you need to add to your list of trusted download sites, and then repeat the download procedures. Once the extension is installed, restart Firefox.\n\nOnce you do this, your Web browser will look much the same as before, but with some small changes. First, there now will be an icon at the bottom of the screen in the status line. This icon will look like either a green V in a circle (to indicate that it is running) or a gray circle with a slash through it (to indicate that it is disabled). Firebug can be enabled all of the time, but you're probably interested in debugging only a small number of sites that you visit. Thus, it's useful that by clicking on the Firebug icon\u2014or by going to Tools\u2192Firebug in the Firefox menu\u2014you can indicate the sites for which Firebug should be active.\n\nYou can add a new site to this list by selecting open Firebug from that same Tools\u2192Firebug menu, or by adding it manually with the allowed sites dialog box from that same Firebug menu. In either case, the site you currently are visiting will be viewable in Firebug.\n\nNow that we have started Firebug, what can we do with it? Let's have some fun by going to the Linux Journal site ( Activate Firebug for this site, and your browser window will be cut in half, with the top half still showing the Web page and the bottom half containing Firebug. I generally prefer to work with Firebug in this way, but if you prefer to keep your browser window separate from your debugging window, you might want to choose open Firebug in new window, rather than simply open Firebug.\n\nThe main menu for Firebug contains the Firebug icon, which offers most of the same menu options as the icon in the status line and Tools\u2192Firebug menu, along with links to the Firebug documentation and home page. An Inspect button always sits next to that icon, and it lets you zoom in on a particular item on the page. The rest of that menu bar changes according to the context in which you are operating, which is determined by the second row of buttons, marked Console, HTML, CSS, Script, Dom and Net.\n\nDebugging HTML\n\nOne of the first, and easiest, tasks to take on with Firebug is debugging HTML. Click on the HTML button and choose Inspect. You immediately will see the HTML source code for the current page highlighted in the Firebug window.\n\nNow, here's where the magic begins: with HTML\/Inspect selected in Firebug, move your cursor over the Web page (the upper frame). As you move the cursor, the HTML element over which it is passing is highlighted in blue. In the Firebug frame, the HTML source corresponding to that rendered content also is highlighted.\n\nThis functionality is particularly useful when I know something is going wrong with the rendering of my Web page, but I'm not quite sure which part of the HTML source is to blame. A few clicks of the mouse later, and you easily can know which part of the file you need to edit.\n\nFirebug highlights the HTML source code that it displays, albeit using different colors than the View Source page that Web developers know and love. (I think that Firebug's color choices are better.) Moreover, Firebird displays the HTML source as a tree, including indentation. This, along with a display of all of the current element's parent tags (next to the edit button on the top row) provides a great sense of the current element's context in the document.\n\nInspecting the HTML certainly is useful and interesting, but it gets better. If you double-click on the text in the Firebug frame, you now can edit it. Obviously, your changes do not get saved back to the server, meaning you can return to the original content by refreshing the page. Nevertheless, it is quite useful (and fun!) to replace text on existing pages, right from your browser.\n\nReplacing text in an HTML element is a good start, but what if we want to modify the markup itself, rather than only its text or attributes? Right-clicking on the tag (or any of the text within the tag) displays a pop-up menu, letting you copy the HTML, innerHTML or XPATH of the selected tag. You can ask for the selected tag to be displayed in the top frame, scrolling if necessary in order to reach it.\n\nFinally, you can add new attributes to this element or ask to inspect the element in the DOM tab. And indeed, the DOM view provides another way of looking at the same document. While still inspecting one of the HTML elements, click on the DOM button in the second row of the Firebug frame. The frame changes its appearance, listing a large number of DOM attributes associated with the element. Thus, inspecting an image in the DOM tab shows that its type is IMG, while inspecting a link shows that it is of type A. As always, Firebug lets you edit any attribute you like by clicking on the value and replacing it.\n\nDebugging CSS\n\nSo far, we have seen how Firebug can help inspect and modify HTML. But, of course, HTML provides only the basic content and structure for a page; if you know what you're doing, you can inspect and modify the CSS definitions as well.\n\nOne of the many amazing and clever things about CSS is the way it handles inheritance. If you have set some attributes for <p> in css1.css and others in css2.css, both will apply to <p> tags in your file. Things can become more complicated than that, such as when you have a <p> tag with an id attribute, with conflicting style information. In these cases, the most specific style (that is, the id tag) applies.\n\nOn a large site with many different styles, it sometimes can become difficult to keep track of which styles are being applied. Fortunately, Firebug provides some wonderful capabilities for inspecting the CSS associated with a site, and even for editing it.\n\nTo use this functionality, click on the CSS button (next to the HTML button we were using earlier), and then on the Inspect button above it. Firebug continues to show its tree representation of the current document, but the right-hand frame displays all of the CSS styles that apply to whatever you're pointing to. Moreover, it indicates which CSS file, and which line of that file was applied. And, as if that isn't enough, it crosses out any styles that were overridden by more specific ones.\n\nAs with the HTML inspector, you easily can edit the CSS associated with any element by double-clicking on a style declaration. For example, if you want to change text from roman to italic type, you merely click on CSS\/Inspect, then point to the text you want to change. If there already is a font-style property in the CSS, you can change it to read italic. If not, you can right-click on the CSS pane, choose new property, and add the property and its value.\n\nFirebug knows what the legal property names are for CSS styles, so it is able to complete the style name automatically when you begin typing.\n\nThe right-hand frame offers more than merely a textual indication of styles. It also has a graphical representation of the CSS box model, showing the number of pixels used by the element, padding, border, margin and offset.\n\nDebugging JavaScript\n\nFinally, we get to the JavaScript debugger. As I mentioned earlier, JavaScript is becoming a mainstream application programming language, which means people are writing increasingly complex programs with it. And, although I've long used print statements for much of my debugging during the years, there's no doubt that a good, interactive debugging environment can make it easier to solve certain issues.\n\nFirebug provides several powerful tools for JavaScript debugging. It introduces a new logging system that makes it possible to produce debugging output without using the alert function. I still can't quite believe it took this long for someone to realize it would be helpful to have a method for logging that didn't produce a modal dialog box. Regardless of how long it took, we can now use ... (truncated)",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9563570023,
        "format_confidence":0.7969143987
    },
    {
        "url":"http:\/\/www.pcworld.com\/article\/149933\/SOA.html",
        "text":"Are You Being Played in the SOA Game Plan?\n\nIf you believe everything you hear, service-oriented architecture is taking over the world, and anyone not in the game is likely to be left on the sidelines. The move to SOA may indeed be inevitable, but it can be hard to accomplish in the current economic environment. Vendors are also struggling with heavy investments in SOA, and they, too, have to make a profit during this economic downturn. This begs the question of whether you truly need to score on the SOA front immediately-or are you just being played?\n\n\"Some vendors, those who were a bit too zealous regarding their expectations on volume and buy-in time lines, have ratcheted down the noise and focus on SOA,\" says Sandra Rogers, director of SOA, Web services and integration research at IDC Research (a sister company of \"Meanwhile, other vendors-those who remain committed-are becoming a bit more realistic and mature about the pace and extent SOA will advance. Now they are [better] articulating how it can fit into overall IT strategies.\"\n\nVendors who were looking for a fast buck are changing their tune entirely. \"Some vendors who promoted composite applications have just changed terminology to promote the same concepts under the banner of mashups,\" explains Rogers. \"While this gains some additional attention, in some cases it may stall interest from those enterprises looking for greater stability to road map, vision and messaging from their vendors.\"\n\nThis sales approach can backfire on vendors. \"They may appear to some businesses as more dispensable initiatives at this particular point in time,\" she says.\n\nRegardless of how SOA is pitched, the payoff is generally real. \"Reuse is easier to measure than strategic business benefits, especially in the early days, but the potential financial benefits of reuse are limited to cutting a percentage of your development budget, whereas the value of strategic business benefits is virtually unlimited,\" explains Larry Fulton, senior analyst at Forrester Research. \"So, reuse is a nice benefit and easy to quantify, but it is not the largest potential for SOA by any measure.\"\n\nUnfortunately, this message gets lost in the pitch and is often even left out of the sales brochure. \"SOA is marketed and sold totally different outside the U.S.,\" reveals Frank Kenney, research director at Gartner. \"Here, it's all about reuse; in South America, South Africa, the Middle East and Asia Pacific, it's about business processes. I have faith that the U.S. will eventually catch up.\"\n\nGiven the different pitches by time zone, is the interest in SOA globally high?\n\n\"Our current research finds approximately 50 percent of survey respondents, most of whom work for large companies, reporting that they have already deployed either SOA or Web services,\" reports Julie Craig, senior analyst at Enterprise Management Associates (EMA). \"Although the two design choices are technically very different, my experience has been that there is a lot of confusion among consumers as to which is which and how the two are different. So the 50 percent figure could be high, based on that confusion.\"\n\nIn other words, most people think they are into SOA. Funny thing is, most of them are-just not always in the way they think they are.\n\n\"One interesting trend is that software-as-a-service (SaaS) vendor products are virtually all SOA-based,\" says Craig. Just about everyone is using some form of SaaS.\n\nCraig says SaaS vendors are leveraging SOA for the same reasons that companies do: for service reuse, simplicity of deployment, abstraction from underlying infrastructure and ease of modification going forward. In addition, many established ISVs in the enterprise management space are investing considerable resources in rewriting products to run on an SOA, platform-based design. \"This is a nontrivial exercise, especially for those vendors with extensive legacy code sets. It is working, but it is slow-and expensive,\" she says.\n\nWith SaaS and other SOA-based vendors struggling to capture a return on investment post-haste, there should be some pricing wiggle room to help you save on your own SOA investments, shouldn't there?\n\n\"Absolutely. Even before the current downturn, vendors were experimenting with new pricing models to better match customer's longer ramp-up for SOA,\" says Fulton. \"This is particularly true in the design-time governance space, where vendors are trying things like pricing based on the number of governed services or developers, and I expect we'll see this trend continue in other areas as well.\n\n\"Also, this incremental adoption reality makes it very attractive for vendors to get a toe in the door, increasing the likelihood that you will come back for more down the road, so customers have a lot of leverage at this early stage,\" he adds.\n\nThere are other price negotiating advantages that can be leveraged by savvy buyers. \"Start with the vendors you already work with,\" advises Kenney. \"You might not have to buy as much to make SOA happen for you. Take, for example, enterprise service buses: Tell your vendors, 'I have problems making your widget work with my SOA plan, so get in here and help me fix it.'\"\n\nHowever, SOA architects aren't feeling any real pricing pressures, so don't expect to reap many savings there. \"TCS is not facing pricing pressure for services related to assessment, consulting and architecture definition services,\" confides Dr. Santosh Mohanty, head of Global Technology Excellence, Tata Consulting Service (TCS). \"However, some of the operational-level SOA initiatives have reduced annual budget and hence extended the program, but there is no pricing pressure.\"\n\n\"In case of enterprises that are convinced about the value that SOA will bring to them in terms of adaptability, efficiency and agility, there is no slowdown. And most of these enterprises have a top-down implementation of service oriented architecture,\" he further explains.\n\nAs to where SOA goes from here and whether you should bother with the whole shebang: \"If service orientation-even on a modest, initial scale-holds the promise of real competitive advantage for your organization, it follows that this is true for your competition,\" says Fulton. \"It might make sense to prioritize your SOA efforts and focus on those projects with the biggest competitive advantage, but I would be concerned about stopping SOA adoption entirely, since there is no guarantee that the competition will do the same thing, and that could put you at a serious disadvantage.\"\n\nrecommended for you\n\nWhat's Killing SOA?\n\nRead more \u00bb\n\nSubscribe to the Best of PCWorld Newsletter",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.8880728483,
        "format_confidence":0.6785295606
    },
    {
        "url":"http:\/\/searchsoa.techtarget.com\/tip\/Mashups-and-SOBAs-Which-is-the-tail-and-which-is-the-dog",
        "text":"Mashups and SOBAs: Which is the tail and which is the dog?\n\nIn this article, Jason Bloomberg talks about the collision of two distinct areas of discussion within the blogosphere, mashups and Service-Oriented Architecture.\n\nThis Content Component encountered an error\n\nNew buzzwords are one of the many side-effects of emerging markets, and into our buzzword-heavy world comes yet another doozie -- the mashup. According to Wikipedia, a mashup is a Web site or Web application that seamlessly combines content from more than one source into an integrated experience. Falling under the increasingly broad buzz-umbrella of Web 2.0, Mashups bear a more-than-passing resemblance to the service-oriented composite...\n\napplications ZapThink frequently speaks about -- known in analyst-speak as Service-Oriented Business Applications, or SOBAs. In fact, the overlap of mashups and SOBAs, or enterprise mashups, has recently become a hot topic du jour in the blogosphere.\n\nThe collision of two heretofore distinct areas of discussion within the blogosphere (in this case, mashups and Service-Oriented Architecture, or SOA) inevitably results a measure of consternation, because the people within each group bring a different context to the discussion. In this case, there is confusion over the question as to whether mashups are the critical \"new thing\" and SOBAs are simply a type of mashup, or vice-versa. That's why we ask: which is the tail and which is the dog?\n\nThe context for enterprise mashups\n\nThe reason for much of the chatter about mashups and SOBAs arises from the fact that mashups, and Web 2.0 in general, are primarily social phenomena, while SOBAs, and SOA generally, are primarily business phenomena. Yes, there is money to be made in some mashups, to be sure, but business motivators aren't generally driving mashup development. Instead, mashups are part of the broader social context of Web 2.0, leveraging the power of the Internet to augment communication and collaboration among individuals, not between companies and their customers. In contrast, the \"B\" in \"SOBA\" clearly indicates their business context: the point of SOBAs is to deliver flexible IT resources to meet continually changing business needs.\n\nGiven this difference in context, let's break down the discussion of poor Fido and his tail into two key questions:\n\nQuestion #1: Are mashups the result of bringing SOA to the Web 2.0 party?\n\n\nQuestion #2: Are SOBAs the result of bringing Asynchronous JavaScript and XML (AJAX) and other Web 2.0-related technologies to the enterprise's efforts with SOA? To explore question #1, it's important to realize that what SOA brings to the party is the loose coupling between the providers and consumers of services. Loose coupling means that businesses can change how they consume services without having to make changes to the implementation of those services, and vice-versa. Most of today's mashups care little about loose coupling. After all, if Google changes the implementation or location of their service interface, for example, then developers of the various mashups that leverage one of Google's services would have to update their mashups. Such changes might be annoying, but they're really not a big deal in the social context of Web 2.0.\n\nHowever, if the mashup is an enterprise mashup in that its creator intended it to solve a particular business problem, then tight coupling between provider and consumer software would be a serious concern. The last thing a business wants is to leverage mashups for a core business purpose, only to find that they fail capriciously depending upon the whims of the creators of the underlying services.\n\nMashups that meet business needs, therefore, will require SOA, and the SOA infrastructure necessary to guarantee loose coupling. Without that loose coupling, mashups are little more than toys from the enterprise perspective.\n\nQuestion #2 approaches the issue from the opposite direction. From the business perspective, mashups represent a new class of user-centric capabilities that enable a broad range of new uses for software, including the collaborative, social capabilities at the core of Web 2.0. In fact, ZapThink frequently discusses the Web services tipping point, where the focus on services will shift from providers to consumers of services. After the tipping point, enterprises will care just as much about how they use ervices as they do about creating and exposing services for consumption.\n\nWhat makes a service useful is not simply its machine-processable interfaces, but also interfaces that humans can use. Furthermore, services must be composable not only in theory, but in practice. Clearly, it doesn't matter how useful a SOBA appears to be if the consumers of that SOBA prevent users from taking advantage of the flexibility that service-orienting the application promised in the first place. It now seems that we're approaching (or even passing) that tipping point as service consumers with rich interfaces increasingly enable mashups and other collaborative capabilities of software.\n\nBut that doesn't mean that we're ready for enterprise mashups, however, or even that mashups will be the appropriate context for user interaction with SOBAs. In fact, there are several mitigating forces that may limit the \"SOBA as enterprise mashup\" equation:\n\nWhile it's true that SOBAs require rich user interface functionality to ensure their promised flexibility, Rich Internet Applications (RIAs) built with Web 2.0 technologies like AJAX are only one set of interface options. Desktop applications ranging from spreadsheets to accounting packages will also be likely SOBA consumers. Also, since SOBAs enable service-oriented processes, many SOBA consumers may simply be other SOBAs. Therefore, mashups will be at most only a part of the SOBA consumer story.\n\nToday's mashups largely reside within the world of techies. While mashup creators don't need the technical chops of, say, a seasoned Java developer, creating mashups is still out of reach of most businesspeople, even reasonably technically savvy ones. Of course, this situation will likely change as mashup tools mature.\n\nMost importantly, SOBAs require governance. Clearly, no business would risk allowing any of its employees to assemble and reassemble business processes willy nilly, with no controls in place to ensure that the resulting SOBAs followed corporate policies. Implementing identity and access management for the SOBAs is only the first step in providing the necessary governance. The problem is, today's mashups are inherently ungoverned -- that's what makes them so appealing to techies. The bottom line is, the more governed an enterprise mashup becomes, the less like a Web 2.0-style mashup it'll be.\n\nThe next generation of SOBA interfaces\n\nThe fact that mashups and SOBAs come from two different worlds with disparate contexts doesn't mean that there isn't some middle ground. For insight into how the exciting world of mashups should enhance the comparatively dull world of SOBAs, consider the various users of SOBAs and how they should be able to work within the enterprise environment. As SOBAs continue to gain prominence in the enterprise, many business users will do little more than consume the capabilities of the SOBA. The classic example of a high-value SOBA is in a call center, where the call center reps regularly interact with multiple systems in order to do their day-to-day activities. Through the use of SOBAs, they can now interact with one coherent user interface that composes services into a single view that provides all the visibility and control they require, where before their life was filled with complex, error-prone \"swivel chair\" integration of disparate, legacy application interfaces.\n\nUsers on the level of the call center rep, however, are not where the primary value of the SOBA comes through. After all, you're probably offshoring that rep anyway. On the contrary, it's the hands of the knowledge worker that can wring the most value out of SOBAs. These users not only consume the functionality of the SOBA, but also have some measure of responsibility over configuring the SOBAs as well, depending upon their role within the organization.\n\nRemember, configuration is how the business updates a SOBA, once the services that feed the SOBA are in place. SOBA configuration includes any change the business user would like to make, from the simplest adjustment of parameters to the most complex reorganization of business-critical processes. The user interface this knowledge worker uses, therefore, must leverage the combination of SOA, which enables the metadata-driven declarative nature of SOBAs, and a comprehensive governance framework that guarantees that nobody will make changes to SOBAs that violate corporate policies. It will not be sufficient for the user interface to offer nothing more than the call center rep's screens -- true SOBA interfaces must also include all the reconfiguration capabilities that the business requires from its applications. SOBAs with such interfaces are the true enterprise mashup -- governed, yes, but nevertheless putting great power into the hands of the business.\n\nThe ZapThink take\n\nZapThink's vision of the enterprise mashup, therefore, requires a special kind of business user: a knowledge worker who is adept at leveraging the power of SOBAs to meet ever-changing business needs. Indeed, while some knowledge workers have the technical capabilities necessary to construct today's mashups, the true promise of SOBAs depends upon user interfaces sophisticated enough for a broader business audience to use. Fortunately, enterprises already have knowledge workers at this level: people who are proficient with Microsoft Excel. Not only does Excel make a powerful SOBA consumer in its own right, but the technical skill people need to get value out of an application like Excel is the level SOBA tools vendors should target when building SOBA interface ... (truncated)",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.8242689371,
        "format_confidence":0.9429698586
    },
    {
        "url":"https:\/\/www.techcentral.ie\/customer-driven-open-source-is-the-future-of-software\/",
        "text":"Customer-driven open source is the future of software\n\nCredit: Fancycrave \/ IDG News Service\n\nThe best software is software that companies build to scratch their own itches and address their own needs\n\n\n\nRead More:\n\n6 January 2020 | 0\n\nBy some estimates there are roughly 190 million companies on earth today. Imagine if they were all contributing to open source. Of course, most of those companies are not in a position to contribute code, but if we want truly sustainable, customer-friendly open source, it is time to focus on the best possible source: companies that do not sell software.\n\nWhy? Because the more software is built to suit the needs of those who are running it day-to-day, the better that software will be, and the less we will need to worry about sustainability.\n\nMake someone pay for this\n\nAlthough open source has never been more broadly used, we are apparently in an \u201copen source sustainability\u201d crisis. It is the same \u201ccrisis\u201d we have been in for the past 20 years, with persistent warnings that this cannot last. I wrote about it\u00a0in 2008\u00a0(\u201cOpen source has the chance of becoming a non-renewable resource if enterprises consume it without contributing cash or code back\u201d), but\u00a0by 2013\u00a0my concern had faded: \u201c This dramatic improvement in the health of the open source ecosystem derives from two primary trends: a move toward more permissive, Apache-style licensing, coupled with an increase in open source contributions from web technology companies like Facebook.\u201d\n\n\n\n\nBy early 2019\u00a0I was calling open source sustainability concerns \u201cfake news\u201d because open source clearly \u201chas never been stronger.\u201d\n\nWhile I continue to believe open source is nowhere near an existential crisis, I do believe we have frittered away needless energy looking for sustainability in the wrong place: vendors. As I pointed out in 2013, the real innovation in open source stems from customers; that is, from enterprises who use open source to build their businesses and contribute code accordingly.\n\nSoftware of the people, by the people, for the people\n\nRed Hat CEO\u00a0Jim Whitehurst has been agitating for customer-driven open source\u00a0for well over a decade: \u201cUltimately, for open source to provide value to all of our customers worldwide, we need to get our customers not only as users of open source products but truly engaged in open source and taking part in the development community.\u201d\n\nThere are many reasons why such customer-driven (or user-driven) innovation might be best, but Linux veteran\u00a0Matt Wilson put it\u00a0this way: \u201cIf I can risk predicting the future, I think you\u2019ll see a lot more new open source software emerging from companies that are building it\u00a0and\u00a0using it to solve their business problems. And it will be better because of a positive feedback loop of putting the software into an applied practice.\u201d\n\nWilson said: \u201cIf you look backwards in time, [you\u2019ll see] that software vendors dominate the enterprise software space. And I think that is a reason why so much of it is\u00a0so so\u00a0bad. Because the people building the software are not\u00a0using\u00a0it to solve problems.\u201d\n\nNot software sold on a golf course. Software built to meet real-world needs by the companies experiencing those needs.\u00a0Fortunately, it is already happening:\n\n  \u2022 Lyft released\u00a0Envoy, a high performance C++ distributed proxy designed for single services and applications, as well as a communication bus and \u201cuniversal data plane\u201d designed for large microservice \u201cservice mesh\u201d architectures;\n  \u2022 Airbnb released\u00a0Airflow, a platform to programmatically author, schedule, and monitor data pipelines;\n  \u2022 Walmart released\u00a0Electrode, a React-based application platform (that the company used to run Walmart.com);\n  \u2022 Netflix released\u00a0Spinnaker, a multi-cloud continuous delivery platform for releasing software changes with high velocity and confidence;\n  \u2022 Intuit released\u00a0Argo, a collection of projects for working with Kubernetes that include Argo Workflows, a container-native workflow engine; Argo CD, for declarative continuous deployment; and Argo Events, an event-based dependency manager;\n  \u2022 Capital One released\u00a0Cloud Custodian, a tool used to deliver automated governance, security, compliance, and cost optimisation to enterprise cloud environments.\n\nThere are many more. Even a cursory review of the speakers at\u00a0OSCON 2019\u00a0reveals Uber, Bosch, The Home Depot, Comcast, and others, many of them talking about how their companies\u00a0both\u00a0use and build\u00a0open source software.\n\nThe future of software\n\nOf course, there is great software built by vendors, too, though the best software released by vendors tends to have more to do with how they are running their infrastructure than what they are selling. Take\u00a0Google and Kubernetes, for example: Google had been running containerised workloads for over a decade before releasing\u00a0Kubernetes. Kubernetes suddenly gave non-Google-like companies Google-like powers.\n\nThis is the future of open source. Vendors will continue to contribute to open source projects, and to release projects of their own. But Whitehurst and Wilson seem to be onto something: the best software is software that companies build to \u201cscratch their own itches,\u201d in true open source fashion, and address their own day-to-day needs.\n\nMatt Asay, IDG News Service\n\nRead More:\n\nComments are closed.\n\nBack to Top \u2191",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9064723253,
        "format_confidence":0.9332543612
    },
    {
        "url":"https:\/\/www.computer.org\/csdl\/mags\/it\/2008\/03\/mit2008030010.html",
        "text":"The Community for Technology Leaders\n\nGuest Editor's Introduction: Information and Quality Assurance\u2014An Unsolved, Perpetual Problem for Past and Future Generations\n\nJeffrey Voas, SAIC\nLinda Wilbanks, US Dept. of Energy\n\nPages: pp. 10-13\n\nEvery enterprise now lives and dies by the data stored on its computers and networks, and none can long survive if that data isn't accurate and reliable. In addition to the damage that can come from the actual alteration of data, the impact on a company's reputation can be substantial if customers are faced with unreliable, poor-quality products.\n\nQuality assurance (QA) and information assurance (IA) programs play crucial roles in protecting assets and developing trustworthy products and services. Many technical solutions and approaches are available to IT teams engaging in IA and QA, and certain broadly accepted principles apply across the board, but success generally depends on proper responses to issues that affect enterprises at local levels. Platform and application heterogeneity present one sort of challenge, and domain-specific requirements and legal issues add other layers of complexity throughout. A clear understanding of what you're trying to achieve is thus key up front. The following theme articles present case studies and experience reports in several QA and IA initiatives and describe lessons learned that could benefit others facing similar challenges.\n\nAssurance and Trust\n\nThis issue of IT Pro focuses on IA and QA in software development, but assurance is at least equally important for systems, security, and safety. Although people often use these concepts interchangeably, doing so is a mistake because they represent separate areas of concern, each with its own specific challenges. For example, systems assurance requires the ability to build assurance cases in which evidence and context are propagated upwardly to enable specific arguments that system-level assurance has been achieved.\n\nIA typically deals with confidence that certain data and information assets are secure and private. QA is a well-known field, particularly in areas such as the six-sigma manufacturing paradigm, which deals with defect-density rates (1 part in 1 million is defective, for instance). Although six-sigma can be applied to software with some success, it's better known in the physical, manufacturing arena, because software isn't mass produced, as with hardware parts. lists the following among its definitions of assurance:\n\n\n\n  \u2022 A declaration tending to inspire full confidence; that which is designed to give confidence.\n  \u2022 The state of being assured; firm persuasion; full confidence or trust; freedom from doubt; certainty.\n  \u2022 Firmness of mind; undoubting, steadiness; intrepidity; courage; confidence; self-reliance.\n  \u2022 Excess of boldness; impudence; audacity; as, his assurance is intolerable.\n  \u2022 Insurance; a contract for the payment of a sum on occasion of a certain event, as loss or death.\n  \u2022 Any written or other legal evidence of the conveyance of property; a conveyance; a deed (in England, the legal evidences of the conveyance of property are called the common assurances of the kingdom).\n\nUltimately, the goal for assurance is to build trust, which defines as:\n\n  \u2022 Confidence in or reliance on some person or quality.\n  \u2022 Dependence upon something in the future; hope.\n  \u2022 Confidence in the future payment for goods or services supplied; credit.\n  \u2022 Trustworthiness, reliability.\n  \u2022 The confidence vested in a person who has legal ownership of a property to manage for the benefit of another.\n  \u2022 A group of businesspeople or traders organized for mutual benefit to produce and distribute specific commodities or services, and managed by a central body of trustees.\n\nIT workers seek to leverage tools and processes to achieve such reliability and inspire confidence in users and customers. Several key elements come to bear on such efforts.\n\nFirst, it's important to note that software and information systems depend on facts. Those facts can be metrics, processes, standards, or other forms of evidence that a system graded by its behavior (thus using a nonphysical rather than a physical perspective) will operate in a manner that's consistent with the notion that it's \"fit for purpose.\"\n\nFit for purpose is one of three main ideas on how to certify software. In the first school of thought, you certify that you've satisfied a certain set of development, testing, or other processes applied during the prerelease phases of the life cycle. Of course, you're certifying that the processes were followed and completed\u2014demonstrating that they were applied correctly is a trickier issue.\n\nIn the second school, you certify that the developed software meets its functional requirements. Various types of testing and analysis, such as formal methods and specific operational profile testing, in large amounts, are available to accomplish this. The tricky part here is in demonstrating compliance with requirements: even if you can satisfy proof of confidence, you could unknowingly wind up with a false sense of accomplishment if the requirements prove to be incorrect, incomplete, or ambiguous.\n\nIn the third approach, you seek to certify that the software itself is fit for purpose. The term purpose suggests that two items are present: executable software and an operating environment. An environment is a complex entity that involves the set of inputs the software will receive during execution, as well as the probability that certain events will occur. It also involves the hardware on which the software operates\u2014the operating system, available memory, disk space, drivers, and other background processes that are potentially competing for hardware resources, and so on.\n\nTrust is a difficult issue to get a solid grip on. We all have a common idea as to what it means\u2014generally very much in line with the definition stated earlier. Where IA and software QA exacerbate the problem is that they seek to assure qualities of nonphysical systems, such as software, requirements, processes, and standards. For example, software reliability modeling tries to apply hardware reliability models (based on time, wear-out, and decay) to software. Yet, software is deterministic and static; if left untouched, it can't wear out or decay. This attempt to overlay a hardware model onto software led to the notion of \"software rot,\" although in reality, it's not the software that rots, but rather the world around it. This huge difference between physical and nonphysical systems continues to make IA and software QA challenging goals that will go one for years to come.\n\nThe Articles\n\nThe articles that follow address multiple aspects of IA and QA. Software QA is a superset of IA that addresses issues for all of the \"-ilities\" including reliability (such as mean-time-to-failure, mean-time-to-repair, mean-time-to-debug), as well as testing-stoppage criteria, compliance with standards, requirements elicitation, design-for-maintainability, and design for testability. IA deals mainly with the security and safety of data assets. As such, it's more concerned with protecting information, whereas software QA includes broader topics related more to the confidence that can be placed in the software. As you read through these articles, consider looking back to these definitions and how the content maps to them. It's also interesting to see what part of the overall puzzle each article is really addressing.\n\nThe first two articles are concerned with building the evidence in support of an assurance case via testing. Note that both deal with software. \"Beyond Brute Force: Testing Financial Software,\" by Mikhail Kharlamov, Alexey Polovinkin, Ekaterina Kondrateva, and Alexey Lobachev, presents the case for including domain experts as testers to increase efficiency and reliability in identifying faults. The authors use financial software as an example of how accounting for inherent complexities in a field can be difficult using standard testing approaches, which sometimes represent multiple parameters as single inputs. They suggest some specific issues to bear in mind in any test scenarios and point out ways in which integrated teams of IT and domain experts can uncover problems more time- and cost-effectively than domain-neutral testers.\n\nD. Richard Kuhn, Yu Lei, and Raghu Kacker's \"Practical Combinatorial Testing: Beyond Pairwise\" describes methods and tools for detecting failures that occur only when multiple components interact. Methods are widely available for pairwise testing, but recent advances in covering-array algorithms, integrated with model checking or other testing approaches, have made it practical to extend combinatorial testing. The authors argue that tests that cover all four-way or higher-strength combinations of parameter values are sufficient to identify all errors and provide high assurance.\n\nThe third article in our theme turns to the issue of quality as applied to the new IT paradigm of services, rather than simply software. The authors look at building a certification paradigm in which trusted third parties can ensure that the services you employ and build into your enterprise (via a SOA model, for example) provide the quality assurances that you require. In \"Toward Quality-Driven Web Service Discovery,\" Eyhab Al-Masri and Qusay H. Mahmoud demonstrate the need for mechanisms to let clients search for services that meet their quality of service requirements. The current absence of standards to define and regulate quality of Web services (QWS) complicates and undermines efforts to integrate QWS into the discovery process. The authors' proposal for trusted third-party service brokers that could measure quality-related metrics dynamically and provide reliable metrics for comparing among similar services presents a mechanism for improving QA in an increasingly service-oriented world.\n\n\nThe grand challenges for software QA and IA are many. Given that thi... (truncated)",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.8854204416,
        "format_confidence":0.5295298696
    },
    {
        "url":"http:\/\/www.infoworld.com\/article\/2670005\/application-development\/let-s-hear-it-for-screencasting.html",
        "text":"Let's hear it for screencasting\n\nNew medium offers unprecedented avenues for training, discussion\n\nLast January, when I first wrote about the medium that I\u2019ve since come to call screencasting, it seemed an odd-enough topic that I felt obliged to justify it to my editor.\n\nA year later it\u2019s clear that my instincts weren\u2019t leading me astray. I\u2019m now using screencasts \u2014 that is, narrated movies of software in action \u2014 to showcase application tips, capture and publish product demonstrations, and even make short documentaries. And I\u2019m seeing others around the Net starting to do the same. Now\u2019s a good time to explain why I think this mode of communication matters and will flourish.\n\nLet\u2019s start with the simplest form: a screencast that highlights a product feature, offers a tip, or teaches a technique. An example is the 90-second short movie I made to show how I use a Firefox extension called Linky to efficiently review a set of linked pages. Of course I could explain the procedure using the written word. As one viewer of the screencast noted, however, \u201cThere\u2019s nothing like seeing.\u201d\n\nWe all know that most people use only a tiny fraction of the power of applications such as Microsoft Office. Technologists like to think that the people we disparagingly call \u201cusers\u201d don\u2019t want to make better use of their software and couldn\u2019t, even if they tried. I don\u2019t buy that. As a species, we learn voraciously when we can imitate what we see others doing. And there\u2019s the rub.\n\nIf you think about it, we rarely get to observe in detail how other people use their software tools. Now that it\u2019s almost trivial to make and publish short screencasts, can we expose our software-tool-using behavior to one another in ways that provoke imitation, lead to mastery, and spur innovation? It\u2019s such a crazy idea that it just might work.\n\nMeanwhile, I\u2019m having a ball capturing product demonstrations, editing them, and publishing them to my blog as screencasts. I\u2019m privileged to see a lot of interesting things, and it\u2019s exciting to be able to share some of them with you.\n\nThese screencasts aren\u2019t product reviews and don\u2019t pretend to be. I think I\u2019m often able to dig down beneath the surface of a demo in a useful way. I\u2019m planning to do a bunch of demo screencasts this year, so you can judge for yourself.\n\nFinally, there\u2019s a sense in which screencasting can rise to the level of cinematic storytelling. I got a taste of that when I made a short documentary about the life of a Wikipedia page. It\u2019s not a training film but rather an exploration of Wikipedia\u2019s editorial process and its cultural commitment to accuracy, completeness, and neutrality.\n\nThe same week, a very different kind of screencast surfaced in the blogosphere \u2014 the ACLU-sponsored depiction of an Orwellian near future in which personal data is seamlessly joined and ruthlessly exploited.\n\nFor better and for worse, human experience is becoming intertwined with software systems of growing complexity. If we are going to make sense of our software-mediated lives and contemplate the values these software systems embody, we will need a means to tell one another stories: about how things are, how things might be, and how things ought to be.\n\nI think that screencasting is the right medium \u2014 arguably the only possible one \u2014 for Wikipedia\u2019s hopeful vision, for the ACLU\u2019s dark fantasy, and for a lot of other stories yet to be told.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.8726266623,
        "format_confidence":0.9254307747
    },
    {
        "url":"http:\/\/www.theguardian.com\/books\/2001\/apr\/10\/firstchapters.highereducation4",
        "text":"Rebel Code: Linus Torvalds, Open Source, and the War for the Soul of Software by Glyn Moody (V)\n\nOnce Stallman had decided on this new course of action - creating a free operating system - he soon made \"the major design decision - that we would follow Unix,\" he says. In retrospect, it might have seemed the obvious choice, but at the time this was by' no means the case because Stallman knew little about the system. \"I'd never used it,\" he says. \"I'd only read a little about it, but it seemed like it was a good, clean design [that had some] nice clean and powerful ideas.\"\n\nBut the main impetus for his choice grew once again out of his experiences during the diaspora of the hacker community at the AI Lab. \"Digital had discontinued the PDP - 1O\" - the last machine on which ITS had run - \"which meant that ITS was now completely unusable on any modern computer,\" Stallman says. \"And I didn't want that kind of thing to happen again. The only way to prevent it from happening again was to write a portable system.\"\n\nStallman wanted an operating system that could be easily transferred from one type of hardware to another. Because most operating systems had been conceived as the housekeeping software for one type of computer, portability was the exception rather than the rule. \"Unix was, at least as far as I knew, the only portable system that really had users on different kinds of computers. So it was portable not just in theory but in actual practice,\" Stallman says.\n\nAnother important reason why Stallman chose Unix as his model, he explains, was that he \"decided to make the system compatible with Unix so people who had already written programs for Unix would be able to run them on this system. People who knew how to use Unix would know how to use this system without being required to learn something new.\" Once again, as he had with Emacs, he was building on something that already existed, but making it better - in this case, by creating a Unix - like system that could be shared freely\n\nAlthough the project was born at this most unhappy time for Stallman, the name he chose for his new project showed a typical hacker humor. His Unix work-alike would be called GNU, an acronym that stood for \"GNU'S Not Unix,\" and which thus explained itself self referentially. This kind of recursion is often used as a programming technique, and applying it to words is highly popular amongst hackers.\n\nThe name GNU also proved a fruitful source of inspiration for similarly self-referential names for many of the programs that went to make up the overall GNU project. Another important virtue of Unix was that its design \"consists of a lot of small pieces, often fairly small pieces,\" Stallman explains; to create a Unix-like system, \"what you do is you just have to write each piece. So I sat down and started writing small pieces.\"\n\nAlthough this makes the entire process sound disarmingly simple, writing these \"small pieces\" involved reproducing work that had taken hundreds of people fifteen years - an even harder task than when Stallman matched the team of programmers at Symbolics. In his fight against that company, he was able to look at the source code - the underlying lines of programming - to help him write his own versions: hut with Unix this was not an option.\n\n\"I certainly never looked at the source code of Unix,\" Stallman says. \"Never. I once accidentally saw a file, and when I realized it was part of Unix source code, I stopped looking at it.\" The reason was simple: The source code \"was a trade secret, and I didn't want to be accused of stealing that trade secret,\" he says. \"I condemn trade secrecy, I think it's an immoral practice, but for the project to succeed, I had to work within the immoral laws that existed.\"\n\nOnce again Stallman had set himself an almost impossible task that would require an exhaustive dedication and sacrifice - which was perhaps part of the attraction. \"I wasn't certain I could finish the job,\" he recalls, \"but it wasn't important for me to know whether I could finish the job; the point was I was going to start it.\"\n\nThe GNU project formally began, in January 1984, when Stallman started working on a replacement for an obscure programmer's tool called Yacc. One reason he chose this was \"because there was a similar program available called Bison\" - another typical hacker pun - but it wasn't compatible with Yacc; it was missing some features.\" Stallman obtained permission from the author to make Bison compatible with Yacc, and in doing so, he put in place the first small stone of the GNU edifice.\n\nHaving limbered up with this relatively minor task, Stallman moved on to one of the most important. One of the key elements of a Unix system is the C compiler. Programs written in C are text files whose contents are lines of instructions following the rules of the language. Before a computer can obey the instructions contained in the source code, they must be converted into binaries - the sequence of Os and 1s that the processor chip can understand.\n\nThis is the job of the C compiler - a kind of mediator between the source code and binaries, which is therefore an indispensable tool for every C programmer. As such, it held a particular importance for Stallman in his endeavor to create a new, freely available version of Unix. A poor C compiler might seriously damage the chances of GNU's being taken seriously; equally, a great C compiler that was free would immediately make people sit up and take notice of the project.\n\nOnce more, Stallman tried to build on an existing program to minimize the amount of work that needed doing, and something he had heard about seemed to be perfect for the job. The Amsterdam Compiler Kit was a powerful compiler that ran on Unix and could compile not just C but most of the languages then in use. It had been written by the U.S. academic Andrew Tanenbaum at the Free University of Amsterdam. Perhaps slightly misled by another name for the software - the Free University Compiler Kit - Stallman wrote to Tanenbaum to ask whether he could use it for his GNU project.\n\nAs Stallman recalls, [Tanenbaum] said, \"The university is free; the compiler isn't. Why don't you give up that silly idea of a free system? I'd like to have some utilities to distribute with the compiler to boost sales. Contribute some utilities to me and I'll give you a share of the profits.\" What seemed eminently reasonable - generous even - to Tanenbaum, was an insult to Stallman, for whom proprietary software of the kind represented by the Amsterdam Compiler Kit was precisely what he was hoping to combat with his GNU project.\n\nTanenbaum doesn't recall what he said exactly, but he explains why he was unable to give permission for the Amsterdam Compiler Kit (ACK) to be used by Stallman as part of GNU: \"I think we may have already signed a contract to sell ACK commercially via a company in the U.S. and one in Europe,\" he says. \"Our research group needed that money to support graduate students, etc., and as I recall, we were already committed when he came around.\"\n\nDespite these laudable aims, Stallman saw only Tanenbaum's refusal to aid him in his free software crusade. His response was identical to that he had made to Symbolics a few years earlier: He determined to write his own rival software that would match or beat the offending code.\n\nIn September 1984, after an unsuccessful attempt to adapt yet another compiler, Stallman returned to his most successful program so far, Emacs. The code for this new Emacs, which became known as GNU Emacs, had \"nothing in common with the original Emacs, which was written in TECO,\" Stallman says. And in another symbolic act of return to his AI Lab roots, he decided to rewrite most of GNU Emacs in Lisp, that \"powerful and elegant language\" Stallman prized so highly.\n\nBy early 1985, GNU Emacs was able to edit files, and Stallman made it available publicly. This was an important step because it represented the first time people could use and judge GNU software. Even though the free GNU operating system was almost nonexistent, people could still use Emacs: The Unix compatibility that was Stallman's constant guide meant that GNU Emacs could run on any Unix and provide him with a huge potential constituency of users.\n\nFirst, \"people started asking for and writing improvements, and the result was I kept working on Emacs a lot, much longer that I'd expected to, and it ended up much better than I'd originally planned on its being,\" recalls Stallman. This approach of releasing code and encouraging feedback and modifications from users, although not new - it had formed an implicit part of the entire Unix culture for year - became central to the rapid improvement of free software over the next fifteen years. Its adoption in an extreme form for the development of Linux was one of the key factors in its rapid growth and robustness.\n\nThe second consequence of making Emacs available was that Stallman could sell it on tape, the standard distribution medium before floppies; this was alongside free distribution on the Internet because at that time few people had access to this fledgling medium. As well as serving users who might otherwise find it difficult to obtain copies, selling tapes bad the even more beneficial effect of giving Stallman an income. \"See,\" he says, \"I had quit my job at MIT to start the GNU project. And the reason is I wanted to make sure that MIT would not be in a position to interfere with making GNU software free in the way I wanted to do it. I didn't want to have to beg the MIT administration for permission to do this and then maybe they would insist on spoiling it in some way.\"\n\nTo its credit, the AI Lab allowed Staliman to continue to use its computers, and to sleep in his former office at 545 Tech Square for twelve years. He was even registered to vote at that address. Stallman did retain one significant moment of those happy times: his log - in ID. To this day he signs himself \"rms,\" his origina... (truncated)",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9636676311,
        "format_confidence":0.9613091946
    },
    {
        "url":"http:\/\/www.embedded.com\/electronics-blogs\/other\/4414216\/The-care-and-feeding-of-compilers",
        "text":"The care and feeding of compilers\n\nMay 12, 2013\n\nBernard Cole-May 12, 2013\n\nA critical component in every embedded system developer's repertoire of firmware and software tools is the compiler. It transforms source code, generally written in a language such as C or C++, into a machine or object code form that a target microprocessor or microcontroller can understand and execute.\n\nAs simple as this sounds, the process is fraught with complexity, for the compiler must perform a range of operations - lexical analysis, preprocessing, parsing, semantic analysis, code generation and optimization - to be sure that what goes in is what comes out.\n\nBecause of the critical importance of this process, a lot of effort is spent on getting good how-to articles on the effective use of compilers. An early (2000) design article on was \u201cEmbeddeding with GNU: the compiler and linker,\u201d which focused on the importance of good compiler practices in then leading edge 8 and 16 bit microcontrollers.\u00a0\n\nAs noted in the collection of recent design articles, webinars, white papers and Insight blogs in this week\u2019s Tech Focus newsletter on \u201cFirmware compiler optimization strategies,\u201d 32-bit processors and multicore designs are becoming more common, though 8 and 16 bit MCUs have definitely not disappeared.\n\nAnd with the larger word lengths, increased direct memory access, and larger memory sizes, program code lengths of a million lines or so are not uncommon. This represents at least an order of magnitude increase in the firmware optimization challenges facing programmers \u2013 and their compilers.\n\nIn addition to the recent articles included in this week\u2019s newsletter, is the repository of extensive design resources on every aspect of compiler use in embedded software code development over a broad range of processor platforms and applications.\n\nAt one end of the spectrum are back-to-the-basics articles by authors such as Jakob Engblom on writing compiler friendly C code, and using C-compilers to reduce code size, Wayne Wolff on basic MCU compilation techniques, and Greg Davis on the details of programming with modern compilers.\n\nAt the other end are articles for experienced programmers on such topics as using compilers to effectively allocate memory, the use of whole program code compilation on 32-bit MCUs, architecture-oriented C compiler code optimization, compiler optimization for DSP and compiler support for multiprocessor SoC designs. Below are my Editor\u2019s Top Picks\n\nTuning C\/C++ compilers for optimal parallel performance in multicore apps\nCompiler optimization for smaller, faster embedded application code\nProviding memory system and compiler support for MPSoc designs\nOptimizing compilers and embedded DSP software\u00a0\n\nAlthough most embedded developers feel most comfortable using the traditional per module, \"compiland\" approach typical of the GCC compiler, the broader range of applications in which embedded systems are being used is dictating developers take a closer look at such approaches as whole program compilation by companies such as IAR Systems and Microchip Technology. Also getting some attention is LLVM and dynamic compilation techniques being considered by companies such as Apple that are aimed at replacing - or at least complementing - the traditional GNU compiler.\u00a0\n\nAs you explore these new alternatives - or find new ways to use old compiler techniques - I look forward to hearing from you about communicating your experiences and ideas with the broader embedded developer community. Site Editor Bernard Cole is also editor of the twice-a-week newsletters as well as a partner in the TechRite Associates editorial services consultancy. He welcomes your feedback. Send an email to, or call 928-525-9087.\n\nSee more articles and column like this one on Sign up for the newsletters. Copyright \u00a9 2013 UBM--All rights reserved.\n\nLoading comments...\n\nParts Search\n\nSponsored Blogs",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9066580534,
        "format_confidence":0.8272138238
    },
    {
        "url":"https:\/\/blog.radware.com\/applicationdelivery\/wpo\/2015\/12\/were-not-there-yet-images-scripts-and-redirects-are-slowing-down-the-top-mobile-travel-sites\/",
        "text":"We\u2019re Not There Yet: Images, Scripts and Redirects Are Slowing Down the Top Mobile Travel Sites\n\nDecember 16, 2015 \u2014 by Kent Alstad3\n\nIt was inevitable: travelers are utilizing their smartphones to research and make arrangements for their trips, as smartphone and data network penetration continues its inexorable climb toward establishing a truly \u201cmobile-first\u201d world.\n\nThis will increase as a shared experience among the world\u2019s travelers, with a quarter of the globe\u2019s online bookings for the $2.36 trillion dollar travel and hospitality industry expected to occur via mobile devices by 2019, according to projections by U.K-based market research firm Euromonitor. The United States is ahead of the curve for mobile-based bookings, with half expected to originate on smartphones by 2016.\n\nIt makes sense: travel is, by its very definition, already a mobile experience, and now you can research your trip and handle bookings from hotels to planes, trains and automobiles. All this can be done from a device you always have on your person.\n\nBut is the Performance of Mobile Travel Sites Going Anywhere?\n\nAnd yet, while the industry is working to facilitate travelers\u2019 desires to be able to handle details on their smartphones, the user experience is suffering due to poor mobile website performance practices.\n\nWe wanted to gauge the real-world experience of modern travelers, so utilizing some of the most popular smartphones, we tested in November 2015 the performance of the top 100 mobile travel sites as ranked by information technology company SimilarWeb, calculated by data aggregation and based on traffic and engagement levels.\n\nTested sites were accessed using actual mobile devices connected to the AT&T 4G\/LTE data network. Each tested site was loaded three times on the iPhone 5 and 6 as well as the Samsung Galaxy S5 and S6. The data from the median of the three test runs was used to report and compare site performance for the following metrics for comparison:\n\n  \u2022 Load Time (Fully Loaded)\n  \u2022 Time to First Byte\n  \u2022 Start Render\n  \u2022 Requests (Fully Loaded)\n  \u2022 Bytes In\/Total Size in KB\n  \u2022 Content Breakdown by MIME Type\n  \u2022 Redirects\n\nAs we\u2019ve discovered in testing leading ecommerce sites for our last annual State of the Union for Mobile Ecommerce Performance, the usual suspects were in play: page bloat, increasing complexity, image size and too many redirects. These all have an effect on latency and slow down page load times.\n\nKey Findings from our Report\n\nCompared with typical ecommerce sites, travel sites for mobile typically are more form-heavy, meaning the initial elements presented are fields for data entry, as opposed to rotating pictures of destinations.\n\nTherefore, the page composition of these mobile travel sites will generally differ from the mobile retail sites we\u2019ve previously tested, although there are plenty of opportunities to clean up the slow sites on an internal basis to increase the performance and improve the user experience.\n\nIMPORTANT: These tests are not intended to be taken as performance reviews of the devices we tested on. Rather, they demonstrate how broadly performance can vary across mobile devices. Site owners need to be aware that this degree of variance exists among their users so that they can prioritize performance testing their pages across a range of devices and connection types.\n\nAmong the top 100 mobile travel websites:\n\n  \u2022 On all four devices, median load times exceeded the four-second target: The median page took 6.7 seconds to load on the iPhone 6 and 5.5 seconds to load on the iPhone 5. On the Samsung Galaxy S5, the median load time was 5.7 second, with the S6 scoring the lowest median load time at 4.1 seconds.\n  \u2022 While the median page had 62 requests (such as images, CSS, and JavaScript files), 18 percent of the mobile pages we tested contained 100+ resource requests. Each of these requests incurs latency, which adds up to slower load times.\n  \u2022 As a percentage of the sites\u2019 total size, images accounted for about a quarter of a page\u2019s weight on average (22.8 percent). Ten percent of the sites had images making up over half their size, going up as high as 86 percent.\n  \u2022 The mobile travel sites exceeded the ideal total payload of under one megabyte, averaging 1.2 MBs.\n  \u2022 The median number of redirects was 8, with 43 percent of the sites including over 10 redirects.\n  \u2022 In general, the slowest sites had more JavaScript requests than the fastest sites, taking a performance hit from third-party trackers. The average JavaScript payload was around 368k.\n\n10 Slowest Sites (anonymized)\n\nLooking at the slowest sites, they generally (though not in every case) had both more requests and larger footprints.\n\nAnother general area of woe? Third-party JavaScripts and trackers\/analytics tools.\n\nWhile it\u2019s understandable that the industry would want as much customer data as possible in order to be responsive in its marketing efforts, data trackers increase a site\u2019s weight and up the number of data fetches tasks, leading to slowdown.\n\nConsider this section from the waterfall of one of the slowest sites:\n\nSource: All waterfall charts from\n\nThe items in yellow are highlighted as \u201cwarnings.\u201d They are related to ad networks and analytics with undesired redirects. The sum total of all the JavaScript elements (with 110 requests), is 60.2 percent of the mobile page\u2019s weight.\n\n(Note: \"MIME types\" are used to denote the type of information that a file contains, such as .html meaning an HTML page) Source:\n\nConclusion and Takeaways\n\nFor mobile travel and hospitality site owners, the performance hit taken by third-party trackers and analytics tools needs to be weighed against the benefit of the insights gained. Is it worth it?\n\nThe abandonment rate for the industry as a whole is around 41 percent across all websites, desktop and mobile included. Anything that can be done to smooth out the experience for users can help combat that.\n\nWe recommend you:\n\n1. Consolidate JavaScript and CSS\n\nConsolidating JavaScript code and CSS styles into common files that can be shared across multiple pages should be a common practice. This technique simplifies code maintenance and improves the efficiency of client-side caching. In JavaScript files, be sure that the same script isn\u2019t downloaded multiple times for one page. Redundant script downloads are especially likely when large teams or multiple teams collaborate on page development.\n\n2. Minify Code\n\nMinification, which is usually applied to scripts and style sheets, eliminates non-essential characters such as spaces, newline characters, and comments. A correctly minified resource is used on the client without any special processing, and file-size reductions average about 20%. Script and style blocks within HTML pages can also be minified. There are many good libraries available to perform minification, often along with services to combine multiple files into one, which additionally reduces requests.\n\n3. Defer Loading and Executing Non-Essential Scripts\n\nMany script libraries aren\u2019t needed until after a page has finished rendering. Downloading and parsing these scripts can safely be deferred until after the onload event. Defer as much as possible until after onload instead of needlessly holding up the initial rendering of the important visible content on the page.\n\nThe script to defer could be your own or, often more importantly, scripts from third parties. Poorly optimized scripts for advertisements, social media widgets, or analytics support can block a page from rendering, sometimes adding precious seconds to load times.\n\nFollowing these and other best practices can help increase the user engagement for your site. Remember, even a basic-looking page of entry fields can be too slow for users.\n\nYou don\u2019t want them asking, \u201cAre we there yet?\n\nGet the Report: 2015 State of the Union: Mobile Performance of the Top Travel Industry Sites\n\nKent Alstad\n\nKent is an entrepreneur, software architect, and technology innovator. Before taking his former role of VP Acceleration at Radware, Kent was CTO at Strangeloop Networks, where he was instrumental in authoring all of Strangeloop\u2019s issued and pending patents. Prior to helping create Strangeloop, he served as CTO at IronPoint Technology. Kent also founded Eclipse Software, a Microsoft Certified Solution Provider, which he sold to Discovery Software in 2001. In more than 25 years of professional development experience, Kent has served as architect and lead developer for successful production solutions with The Active Network, ADP, Lucent, Microsoft, and NCS. \u201dPort View\u201d, an application Kent architected for the Port of Vancouver, was honoured as \u201dBest Administrative System\u201d at the 1996 Windows World Open Competition.\n\n\nLeave a Reply\n\nYour email address will not be published. Required fields are marked *",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.5958439708,
        "format_confidence":0.7884078026
    },
    {
        "url":"https:\/\/www.thoughtworks.com\/en-us\/insights\/blog\/hypothesis-driven-innovation-lab-exposing-our-assumptions",
        "text":"\nBlogs Banner\n\nThe Hypothesis Driven Innovation Lab - Exposing our Assumptions\n\nBresicWhitney is a lifestyle property group serving \u00a0modern buyers across inner Sydney, Australia. They saw an opportunity to shake up the real estate industry in Australia. They had an idea that would give buyers the information they needed - instantly. Partnering with us, we designed and tested a document delivery system.\n\nThe webapp gives potential buyers access to building condition reports amongst others, free of the standard $500 cost, at any time online or via their mobile device. Something no other competitor is offering their buyers. The platform then closes the loop by sending property agents a report notifying them of the download - giving them an opportunity to get in touch with the potential buyer.\n\nHere, I outline the process from the definition of a problem through to the output of a solution in what appears to be a linear fashion. This is misleading, in that we employed an approach which allowed us to move backwards and forwards through the process at anytime. The key was to ensure we were learning, then delivering value\u00a0from that learning at all times. This included research, design, and technology build, but also applied to the way in which we engaged the client and progressed the project in step with the relationship.\n\nThe Situation\n\nThe real estate market in Australia is dominated by realestate.com.au. \u00a0A property listing site that effectively holds a monopoly over buyers, sellers and real estate agents as the only significant market place to buy and sell property. REA offer an excellent service, with a broad reach - BresicWhitney offer the same excellent service, for a niche.\n\nForces in Australia\u2019s intense love of property ownership and a culture of auctions over private treaty as the method of sale, make competition fierce. The sheer demand puts a strain on the relationship between buyers and agents, and has resulted in a less-than-optimal reputation for most realtors nationwide.\n\nBresicWhitney have been grappling with this issue for some time and realised that if they don\u2019t change the way they look at the business they will fade into irrelevance. When staring down the barrel of your own demise, you are prompted to really get to know yourself, and what value you bring to your clients. In this case, BresicWhitney consider buyers and sellers to be their clients, not just another vendor on their books.\n\nThe commonly practiced way to run an open-house (a viewing of the property for sale open to interested buyers) is for the agent to stand at the door and almost demand that the potential buyers provide their name and contact information - kind of like a gate-keeper. Most people just go with it and provide their details, but they generally don\u2019t like it. It\u2019s not a nice experience. This interaction immediately set\u2019s up the agent for an adversarial relationship with any buyer; which is really not good for business. Further, it provides a full day of follow up phone calls that the agent will need to do on Monday usually, since almost all open-homes are conducted on Saturdays.\n\nA new vision emerged. More focussed on empowering the buyer. This was when we were asked to join the conversation. The BresicWhitney stakeholders understood the importance of changing the way they operated, and had articulated a vision which was an ideal situation for success. A clear vision of the kind of interactions they needed to remain relevant in the market and industry, but with flexibility to explore different ways to support that vision. Conversely a vague vision, with its infinite flexibility, would have been much harder to define ways to deliver value.\n\nBeginning the Engagement in an Agile Way\n\nWe met with the BresicWhitney team for a discovery session to understand the journey up to that point, and began to define what the engagement would look like. Whilst not being mature in agile practices, BresicWhitney did value the benefits and were open to taking our lead - provided we could demonstrate value.\n\nOur proposal consisted of a set timeline, an estimated team of four Thoughtworkers, and some value based deliverables. We set out for a specified amount of time, with a goal to validate (or reject) a set of hypotheses, and to also to explore the problem space a bit ourselves.\n\nThere were no deliverables in the traditional sense and no set processes we would follow. We would however learn about the problem, confirm or deny the things we thought we knew, and define opportunities for potential solutions. We used a selection of the many items in our toolkit, some of those would be technology, many of them not.\n\nWe were allowed to go ahead on a toolkit and a promise because we were able to articulate the importance of eliminating risks as early as possible. It was agreed that this timeboxed project would focus on that very task: identifying and eliminating risks.\n\n\nWe set aside the hypotheses and aimed to learn as much about the key actors in the buying relationship as we could. This was important in that it allowed us to follow what we saw and learnt rather than simply following assumptions or common beliefs about the customers. One issue that presented was the Burden of Knowledge which meant that even though we wanted to ignore the hypotheses presented by the client, it was difficult to do so once we are aware of them. We dealt with this by engaging a researcher to observe buyer behaviors separate from us to ensure that the findings would not be tainted by our knowledge of the client\u2019s problem hypotheses.\n\nThe research team consisted of two designers, plus the additional researcher addressing the burden of knowledge problem.\n\nOur research began with qualitative methods aimed at understanding the kinds of experiences and journeys buyers and agents take when going through the process.\u00a0We conducted interviews with agents and buyers, aiming to understand their tasks and activities over the course of their week. This was captured as conversations, rough sketches, and notes for later review. We augmented our research with a series of phone interviews with recent buyers who were both successful and unsuccessful in their property search.\n\nWe also began to craft the \u201cas-is\u201d journey map which provided the rich material to begin thinking about potential solutions.\u00a0We needed to really understand the problem in order to create the right solution.\n\nWe followed this up with a contextual inquiry to better understand customers direct experience, and to unearth insights that they may not have been aware of themselves. This included observations at private showings and open-homes on a Saturday. It\u2019s important to point out that there are two types of empathy at play here. Cognitive Empathy and Emotional Empathy.\n\nCognitive Empathy is the kind of information we gathered when discussing the different views, needs, and opinions of buyers with the key stakeholders. This is important as it helps to foster the idea that there are points of view other than your own, and that we ought to seek them out.\n\nEmotional Empathy is the kind of understanding you get by feeling it yourself. Through observing, or discussion you can truly feel what it would be like to experience life as that person. This is often the most radical in its effect on executives and as a source of insight.\n\nThe output of the various observations and conversations was expressed as an Empathy Map. This guided us further in our discovery and is a helpful resource when attempting to articulate problem statements we were trying to solve.\n\nUsing some of the empathy maps and insights generated, along with some of the hypotheses put forth by the client, we were able to articulate the core problems, and if solved would have an impact. They were:\n\n  \u2022 Buyers did not have confidence in buying guides, as they were an estimate by the agent of how much the property would sell for.\n  \u2022 Buyers we reluctant to purchase property inspection reports at around $500 per report because they could not be sure the property would be within financial reach at the auction (due to lack of confidence in the buying guide).\n  \u2022 Buyers didn\u2019t have trust in their interactions with agents, and asked for more transparency.\n\nTo further validate these problem areas we conducted an email survey to 4000 recently engaged buyers who had a range of experiences with BresicWhitney. Our questions for this kind of qualitative research were more directed, and the goal was to confirm at scale whether we were on the right track with our problem insights.\n\nWith our new findings, we consolidated and refined the \u201cas-is\u201d journey maps to ensure it reflected the situation accurately. We then tested our understanding by playing this back to the stakeholders, something we also did at the end of our conversations with buyers.\n\nTesting is not something you do at the end, it is an integral part along the way to ensure that the the problem you are planning to solve is valid. It\u2019s vital that the design team properly understands the constraints, amongst other things.\n\n\nSo how do we build something awesome?\n\nBut we still had to test our idea with the buyers, and this is where the Innovation Lab is most obvious. Really the Innovation process started way back when we had some ideas which we explored with buyers, but this is where it starts to take form.\n\nThe Innovation Lab is not a specific place, it\u2019s more of a way to describe the critical activities that allow us rapidly validate assumptions of all kinds.\n\nWith a limited amount of time - and a high level of confidence that we were solving the right problem - we proceeded to developing a solution. We were however, acutely aware that we were in fact testing two things; the problem and the solution.\n\nWe agreed to work on the following hypotheses:\n\n#1 Providing potential buyers with information will focus the conversation on buying.\n\n#2 If we provide information early in the process, buyers will prefer \/ seek out Bre... (truncated)",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.5171951056,
        "format_confidence":0.7469841242
    }
]