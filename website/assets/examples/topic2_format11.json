[
    {
        "url":"https:\/\/sumatosoft.medium.com\/minimum-loveable-product-building-products-users-will-love-15397a975243?source=post_internal_links---------4-------------------------------",
        "text":"Minimum Loveable Product: Building Products Users Will Love\n\nRecently we\u2019ve been talking about the advantages of the development of a minimum viable product can bring to companies. But is it enough for a product to be viable? Can technology be adorable and endearing? Can it \u2014 and should it \u2014 be loveable? You may feel confused with all these obscure abbreviations that are used in describing some popular trends the technology world faces today. There is an MCR \u2014 a minimum credible release, an MBP \u2014 a maximally buyable product, and \u2014 finally \u2014 an MLP which is a minimum lovable product. While an MVP is treated by companies and their development teams as a marketing strategy holy grail, what can an MLP boast of?\n\nThe term \u201cminimum lovable product\u201d was originally coined in 2013 by Henrik Kniberg in his overview of the product development process at Spotify:\n\n\u201c\u2026Perhaps a better term is Minimum Lovable Product. A bicycle is a lovable and useful product for somebody with no better means of transport, but is still very far from the motorcycle that it will evolve into\u201d.\n\nFurther, Henrik Kniberg expressed the idea that the term \u201cminimum lovable product\u201d (further \u2014 MLP) is the same as MVP, because both terms describe the simplest versions of the product that are able to solve the major problem, satisfy the principal requirement or need, and are capable of delighting users not being features-complete. In other words, the MVP approach is striving for being \u201cbarely enough\u201d, and never \u201cgreat and superb\u201d, opposite to the development of an MLP which is targeted at creating a wonderful product people would praise and tell their friends about.\n\nMLP characteristics\n\n  \u2022 The planning product is total innovation. It means there will be no competitors, or at least their number will be not that big. The startup is in a half-won position because a customer has no analogs to compare to the planning product.\n  \u2022 When potential customers and people who have already tested the product show off positive reactions when they hear about it, it means that the company managed to gain their respect and interest. And \u2014 by no means important \u2014 love.\n  \u2022 Customers, potential customers, CEOs of other companies, mass media representatives start contacting the company to learn more about the thing it is building, the release date, the set of available features, etc.\n\nMLP as a first product release\n\nThere is one more serious pitfall of underinvestment in the MLP. The reality is that underinvestment can make people lose interest in the product, which will lead to the gradual fading of the opportunity of creating the product people will love. Let us introduce these statements with the help of the graphics:\n\nmlp meaning\n\nWhat features should be implemented in the MLP? Will three main features be enough? Or will it be better to provide 10 and more significant options? The choice must be well elaborated and reasonable, so we suggest resorting to the Kano model developed in the 80s by Japan\u2019s professor Noriaki Kano.\n\nIt is an essential tool helping to communicate 5 universal categories of customer\u2019s needs. Also, this tool helps companies understand their customers\u2019 needs better than their customers to understand their needs themselves.\n\nGraphically, this idea can have the following appearance:\n\nThus, this value proposition canvas will be a good aid in deciding which features and options will create value to build the product around further. Genuine excitement is guaranteed as a result.\n\nIt\u2019s hard to compare the importance of MVP to MLP\u2019s significance. If the necessity of an MVP development has been experimentally proven by lots of bigger companies before, the MLP development option is a bonus that you can obtain from a detailed inspection of your potential customers\u2019 needs and things they are ready to pay for. The approach of launching a product generating a profit and meeting the basic requirements of people has remained in the past. The new trend is coming forward which is the creation of a delightful and enjoyable product \u2014 a lovable one.\n\nOriginally published at\n\nWe are an IT products development company. Our team are experienced professionals who are ready to share their expertise with Medium readers.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.5919764638,
        "format_confidence":0.7056789994
    },
    {
        "url":"http:\/\/www.infoworld.com\/article\/2670005\/application-development\/let-s-hear-it-for-screencasting.html",
        "text":"Let's hear it for screencasting\n\nNew medium offers unprecedented avenues for training, discussion\n\nLast January, when I first wrote about the medium that I\u2019ve since come to call screencasting, it seemed an odd-enough topic that I felt obliged to justify it to my editor.\n\nA year later it\u2019s clear that my instincts weren\u2019t leading me astray. I\u2019m now using screencasts \u2014 that is, narrated movies of software in action \u2014 to showcase application tips, capture and publish product demonstrations, and even make short documentaries. And I\u2019m seeing others around the Net starting to do the same. Now\u2019s a good time to explain why I think this mode of communication matters and will flourish.\n\nLet\u2019s start with the simplest form: a screencast that highlights a product feature, offers a tip, or teaches a technique. An example is the 90-second short movie I made to show how I use a Firefox extension called Linky to efficiently review a set of linked pages. Of course I could explain the procedure using the written word. As one viewer of the screencast noted, however, \u201cThere\u2019s nothing like seeing.\u201d\n\nWe all know that most people use only a tiny fraction of the power of applications such as Microsoft Office. Technologists like to think that the people we disparagingly call \u201cusers\u201d don\u2019t want to make better use of their software and couldn\u2019t, even if they tried. I don\u2019t buy that. As a species, we learn voraciously when we can imitate what we see others doing. And there\u2019s the rub.\n\nIf you think about it, we rarely get to observe in detail how other people use their software tools. Now that it\u2019s almost trivial to make and publish short screencasts, can we expose our software-tool-using behavior to one another in ways that provoke imitation, lead to mastery, and spur innovation? It\u2019s such a crazy idea that it just might work.\n\nMeanwhile, I\u2019m having a ball capturing product demonstrations, editing them, and publishing them to my blog as screencasts. I\u2019m privileged to see a lot of interesting things, and it\u2019s exciting to be able to share some of them with you.\n\nThese screencasts aren\u2019t product reviews and don\u2019t pretend to be. I think I\u2019m often able to dig down beneath the surface of a demo in a useful way. I\u2019m planning to do a bunch of demo screencasts this year, so you can judge for yourself.\n\nFinally, there\u2019s a sense in which screencasting can rise to the level of cinematic storytelling. I got a taste of that when I made a short documentary about the life of a Wikipedia page. It\u2019s not a training film but rather an exploration of Wikipedia\u2019s editorial process and its cultural commitment to accuracy, completeness, and neutrality.\n\nThe same week, a very different kind of screencast surfaced in the blogosphere \u2014 the ACLU-sponsored depiction of an Orwellian near future in which personal data is seamlessly joined and ruthlessly exploited.\n\nFor better and for worse, human experience is becoming intertwined with software systems of growing complexity. If we are going to make sense of our software-mediated lives and contemplate the values these software systems embody, we will need a means to tell one another stories: about how things are, how things might be, and how things ought to be.\n\nI think that screencasting is the right medium \u2014 arguably the only possible one \u2014 for Wikipedia\u2019s hopeful vision, for the ACLU\u2019s dark fantasy, and for a lot of other stories yet to be told.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.8726266623,
        "format_confidence":0.9254307747
    },
    {
        "url":"https:\/\/blog.opengroup.org\/2016\/09\/27\/the-role-of-enterprise-architecture-in-platform-3-0-transformation\/",
        "text":"The Role of Enterprise Architecture in Platform 3.0 Transformation\n\nBy Stuart Macgregor, CEO, Real IRM and The Open Group South Africa\n\nOur transition to the highly-connected realm of Platform 3.0 will radically disrupt the way that we approach Enterprise Architecture (EA).\n\nThe current architectures and methodologies will simply not hold up in the era of Platform 3.0 \u2013 characterised by the forces of big data, mobility, the Internet of Things, and social media colliding.\n\nIn the Platform 3.0 era, power shifts to the customer \u2013 as we choose from a range of services offered conveniently via digital channels. By embracing Platform 3.0, organisations can respond to newly-empowered customers. New entrants can scale at unprecedented rates, and incumbents can pivot business models rapidly, while entering and exiting new markets as opportunities emerge.\n\nEA plays an essential role in making these possibilities a reality. EA infuses IT into the DNA of the business. No longer is it about \u2018IT\u2019 and \u2018business\u2019. Technology is absolutely integral to the entire business, and business leaders are quickly realising the fundamental truth that \u2018if you can\u2019t change the system, you can\u2019t change the business\u2019.\n\nA new and exciting Platform 3.0 architectural reality is emerging. It\u2019s composed of microservices and platforms that are combined in radical new ways to serve point-in-time needs \u2013 powering new-found business opportunities and revenue streams, dramatically transforming your organisation.\n\nPlatform 3.0 refers to radically different ways for the organisation to securely engage with partners, suppliers, and others in your value chain or ecosystem.\u201d\n\nManaging volatile change\n\nBut, while driven by an urgent need to transform, to become faster and more agile, large organisations are often constrained by legacy infrastructure.\n\nWith an EA-focused approach, organisations can take a step back, and design a set of architectures to manage the volatile change that\u2019s inherent in today\u2019s quickly-digitising industries. EA allows business systems in different departments to be united, creating what The Open Group (the vendor-neutral global IT standards and certifications consortium) aptly describes as a \u201cboundaryless\u201d flow of information throughout the organisation.\n\nPlatform 3.0 refers to radically different ways for the organisation to securely engage with partners, suppliers, and others in your value chain or ecosystem. For a retailer, stock suppliers could access real-time views of your inventory levels and automatically prepare new orders. Or a factory, for example, could allow downstream distributors a view of the production facility, to know when the latest batch run will be ready for collection.\n\nIn almost every industry, there are a number of new disruptors offering complementary service offerings to incumbent players (such as Fintech players in the Banking industry). To embrace partnerships, venture-capital opportunities, and acquisitions, organisations need extensible architectural platforms.\n\nMore and more transactions are moving between organisations via connected, instantaneous, automated platforms. We\u2019re seeing the fulfilment of The Open Group vision of Boundaryless Information Flow\u2122 between organisations and fuels greater efficiencies.\n\nArchitecting for an uncertain future\n\nWe need to architect for an uncertain future, resigning ourselves to not always knowing what will come next, but being prepared with an architectural approach that enables the discovery of next-generation digital business opportunities.\n\nBy exploring open standards, this transformation can be accelerated. The concept of \u2018openness\u2019 is at the very heart of Platform 3.0-based business transformation. As different business systems fall into and out of favour, you\u2019ll want to benefit from new innovations by quickly unplugging one piece of the infrastructure, and plugging in a new piece.\n\nOpen standards allow us to evolve from our tired and traditional applications, to dynamic catalogues of microservices and APIs that spark continuous business evolution and renewal. Open standards help up to reach a state of radical simplicity with our architecture.\n\nThe old-world view of an application is transformed into new applications \u2013 volatile and continually morphing \u2013 combining sets of APIs that run microservices, and serve a particular business need at a particular point-in-time. These APIs and microservices will form the basis for whatever application we\u2019d like to build on top of it.\n\nArchitects need to prepare themselves and their organisations for an uncertain future, where technology\u2019s evolution and businesses\u2019 changing demands are not clearly known. By starting with a clear understanding of the essential building blocks, and the frameworks to re-assemble these in new ways in the future, one can architect for the uncertain future lying in wait.\n\nPlatform 3.0 requires a shift towards \u201chuman-centered architectures\u201d: where we start acknowledging that there\u2019s no single version of the truth. Depending on one\u2019s role and skill-set, and the level of detail they require, everyone will perceive the organisation\u2019s structure and processes differently.\n\nBut ultimately, it\u2019s not about the user, or the technology, or the architecture itself. The true value resides in the content, and not the applications that house, transmit or present that content. Human-centered architectural principles place the emphasis on the content, and the way in which different individuals (from inside or outside the organisation) need to use that content in their respective roles.\n\nAs the EA practice formalises intellectual capital in the form of business models and rules, we create an environment for machine learning and artificial intelligence to play an essential role in the future of the organisation. Many describe this as the future of Platform 3.0, perhaps even the beginning of Platform 4.0?\n\nWhere this will eventually lead us is both exciting and terrifying.\n\n\nStuart MacGregor CEO, Real IRM, South Africa\nStuart MacGregor CEO, Real IRM, and The Open Group South Africa\n\nStuart Macgregor is the CEO,\u00a0Real IRM Solutions\u00a0and\u00a0\u00a0The Open Group South Africa. Through his personal achievements, he has gained the reputation of an Enterprise Architecture and IT Governance specialist, both in South Africa and internationally.\n\nMacgregor participated in the development of the Microsoft Enterprise Computing Roadmap in Seattle. He was then invited by John Zachman to Scottsdale, Arizona to present a paper on using the Zachman framework to implement ERP systems. In addition, Macgregor was selected as a member of both the SAP AG Global Customer Council for Knowledge Management, and of the panel that developed COBIT 3rd Edition Management Guidelines. He has also assisted a global Life Sciences manufacturer to define their IT Governance framework, a major financial institution to define their global, regional and local IT organizational designs and strategy. He was also selected as a core member of the team that developed the South African Breweries (SABMiller) plc global IT strategy.\n\nStuart, as the lead researcher, assisted the IT Governance Institute map CobiT 4.0 to TOGAF\u00ae, an Open Group standard. This mapping document was published by ISACA and\u00a0The Open Group. He participated in the COBIT 5 development workshop held in London in 2010.\n\nOne comment\n\nComments are closed.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.541772902,
        "format_confidence":0.6700543165
    },
    {
        "url":"http:\/\/blog.sdelements.com\/is-penetration-testing-a-waste-of-time\/",
        "text":"Suppose you hire a consultancy to perform a black-box assessment of your software. After executing the test, the firm produces a report outlining several vulnerabilities with your application. You remediate the vulnerabilities, submit the application for re-testing, and the next report comes back \u201cclean\u201d \u2013 i.e. without any vulnerabilities. At best, this simply tells you that your application can\u2019t be broken into by the same testers in the same time frame. On the other hand, it doesn\u2019t tell you:\n\n  \u2022 What are the potential threats to your application?\n  \u2022 Which threats is your application \u201cnot vulnerable\u201d to?\n  \u2022 Which threats did the testers\u00a0not\u00a0assess your application for? Which threats were not possible to test from a runtime perspective?\n  \u2022 How did time and other constraints on the test affect the reliability of results? For example, if the testers had 5 more days, what other security tests would they have executed?\n  \u2022 What was the skill level of the testers and would you get the same set of results from a different tester or another consultancy?\n\nIn our experience, organizations aren\u2019t able to answer most of these questions. The tester doesn\u2019t understand application internals and the organization requesting the test doesn\u2019t know much about the security posture of their software. We\u2019re not the only ones who acknowledge this issue: Haroon Meer discussed the\u00a0challenges of penetration testing\u00a0at 44con. Most of these issues apply to every form of verification: automated dynamic testing, automated static testing, manual penetration testing, and manual code review. In fact a\u00a0recent paper\u00a0describes similar challenges in source code review.\n\nThe opaque nature of verification means effective\u00a0management of software security requirements\u00a0is essential. With requirements listed, testers can specify both whether they have assessed a particular requirement and the techniques they used to do so. Critics argue that penetration testers shouldn\u2019t follow a \u201cchecklist approach to auditing\u201d because no checklist can cover the breadth of obscure and\u00a0domain-specific vulnerabilities. Yet the flexibility to find unique issues does not obviate the need to verify well understood requirements. The situation is very similar for standard software Quality Assurance (QA): good QA testers both verify functional requirements AND think outside the box about creative ways to break functionality. Simply testing blindly and reporting defects without verifying functional requirements would dramatically reduce the utility of quality assurance. Why accept a lower standard from security testing?\n\nBefore you perform your next security verification activity, make sure you have\u00a0software security requirements\u00a0to measure against and that you define which requirements are in-scope for the verification. If you engage manual penetration testers or source code reviewers, it should be relatively simple for them to specify which requirements they tested for. If you use an automated tool or service, work with your vendor to find out what requirements their tool or service cannot reliably test for. Your tester\/product\/service is unlikely to guarantee an absence of false negatives (i.e. certify that your application is not vulnerable to SQL injection), but knowing what they did and did not test for can dramatically help increase the confidence that your system does not contain known, preventable security flaws.\n\nAbridged from this article on InfoQ\n\n5 Steps to Starting a Software Security Requirements Program\n\n5 Steps to Starting a Software Security Requirements Program",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9108562469,
        "format_confidence":0.6337821484
    },
    {
        "url":"https:\/\/anchore.com\/blog\/more-than-just-security-updates\/",
        "text":"More Than Just Security Updates\n\nIn our last blog, we talked about how quickly different repos respond to updates to their base images. Any changes made by the base image will need to be implemented in the application images built on top of it, so updates to popular base images spread far and, as we saw from the last blog, quickly.\n\nThe only type of update we have covered so far in this series of blogs is security updates. However, that is only one part of the picture; package updates may contain non-security bug fixes and new features. To gain some insight into what is being changed in these updates, we have broken down exactly what packages change for a few of the more popular operating system images.\n\nOne interesting time to look at package differences is when the operating system gets updated to a new version.\n\nLooking at the overview tab for library\/centos:latest, when it just got updated to version 7.4, the Navigator shows in the chart on the righthand side that there were many changes with this update. Shown below is a breakdown of which packages have been updated since last September. Only a portion of the packages are shown, you can find the rest in the link below.\n\nFocusing in on just that most recent update, we see that 80 of the 145 packages have been updated. The image from Sep 13th was CentOS 7.3, while the one from Sep 14th is CentOS 7.4. Looking into some of the changes, bash, like many others, received backports of bug fixes. Other packages were new additions, such as elfutils-default-yama-scope, while one, pygobject3-base, was removed from the image. In terms of CVE\/Security updates, this was an ineffectual update; a quick check of the security tab of both versions (7.3, 7.4) shows that there were no changes in CVEs between the two.\n\nClick the button below to access the full spreadsheet with all package updates for 6 popular operating systems.\n\nView the Full Spreadsheet\n\nIn the spreadsheet, you\u2019ll see Alpine stands out in terms of image size and reduced package count. Having more packages means having more packages to maintain. Even if Alpine were to update almost all of its 11 packages, as it did on May 25th, there would not be as many changes as a standard Debian update, such as the one on June 7th, where 25 of 81 packages were updated. There is a trend towards lightweight images, and the appeal of simpler updates might be one reason behind it. Among public repositories, Alpine is growing its share of usage as a base image. Other base operating systems are beginning to including slim versions of their containers, such as Debian, which has a slim version of each of its releases as well as Oracle and Red Hat.\n\nComparing the sizes of the two Debian tags included in the spreadsheet, stretch and stretch-slim, we see that the slim version is about half the size of the original, 95 MB vs 53 MB. The trend goes across releases too; Debian Stretch (Debian 9) images are around 90 MB while Jessie (Debian 8) images are around 120 MB. Ubuntu 16.04 is around 120 MB while 17.04 is around 90 MB. One repository not slimming its images is CentOS. It does not currently include slim versions, even though Red Hat Enterprise Linux, from which CentOS is based, has a slim image known as RHEL Atomic.\n\nPart of slimming down containers is removing packages that are not necessary. In some instances, packages are included that are arguably not required in the context of a container, such as device-mapper or dracut. This harkens back to a previous blog, where we discussed how containers are often being used as micro-VMs rather than microservices. The packages listed above, among others, lend themselves to running a full machine, rather than running just a single application. Removing these extra packages is not as simple as it initially appears. For example in the CentOS 7 image dracut, which builds initramfs images to boot an OS, is pulled in as a requirement by kmod, which provides an infrastructure for kernel module loading, which is pulled in by systemd. We see many similar examples in the traditional Linux vendors\u2019 images, where the package management system was designed before the advent of containers. This is a topic we will revisit in a future blog.\n\nEven though smaller base images require less maintenance and storage, having fewer packages means less functionality. Most application images built on top of Alpine require that users add many more packages onto the base image so application images are often significantly larger than the 4MB base image. But having the choice to add to an image rather than working out how to remove packages certainly simplifies maintenance for the developer and allows for a reduced attack surface. In our next blog, we will look at some popular application images that are available based on multiple distributions to see how the image size, bug count and update frequencies compare.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.5071045756,
        "format_confidence":0.6606304049
    },
    {
        "url":"http:\/\/webzin.info\/should-the-web-have-a-universal-design-system.html",
        "text":"Home Should the Web Have a Universal Design System?\n\nShould the Web Have a Universal Design System?\n\n10\u00a0min read\nComments Off on Should the Web Have a Universal Design System?\n\nUnlike mobile applications, the web has no set design guidelines to which a designer can refer. Instead, each web project tends to be a blank canvas. There are frameworks like Material, Bootstrap, and others which provide a base, but no set guidelines which span the web as a whole.\n\nThe result is a wide-ranging and diverse web, but one with a lack of cohesiveness, particularly in terms of user experience. Navigations differ in placement, structure, and overall design. Layouts alternate in width. Text sizes and typographic scales vary wildly. And a wide range of differing components, interactions, and user interface elements are used.\n\nDesign systems ensure consistency between apps, resulting in a more cohesive product\n\nThe lack of a set design system for the web is due to its open source nature, and lack of ownership. No company or organization has the power to enforce guidelines or standards. The closest anything or anyone comes to impacting the way we design is Google, who can affect your search rankings based on factors such as user experience, responsiveness, and code structure. On the other hand, mobile operating systems like iOS and Android have the power to enforce certain application structures, user experience practices, and standards. Design systems ensure consistency between apps, resulting in a more cohesive product, and one that is easier to use and understand for the end user. It also enhances performance and optimization, as well as accessibility.\n\nDespite such a defined set of guidelines in both cases of iOS and Android, designers still find ways to differentiate through aspects like color, layout, and design details. In these circumstances it\u2019s still entirely possible to achieve outstanding \u00a0and unique designs which still fall within the guidelines.\n\nConversely, the web is an absolute blank canvas. There is the ability to take a design and user experience in any direction desired. On one hand, it\u2019s what makes the web so attractive, diverse, and abundant. On the other hand, it can lead to a confusing experience for many people: one that is highly inaccessible, inconsistent, and uses a variety of sub-optimal and dark user experience practices.\n\nThe case of iOS and Android show just how rich and diverse a digital product or ecosystem can be, even under such regulation and moderately-strict guidelines.\n\nThis poses the question of whether a set of open source guidelines should be introduced for the entire web. Whether it comes from W3C, is a unified effort between major browsers, or is devised by a group of designers, it could improve the web for all. There would still be great scope for producing unique designs, while ensuring the web reaches much more acceptable levels of accessibility and usability as a whole. Designers and user experience professionals could contribute to this as an open source project, pushing forward the progress of the entire web.\n\nIt\u2019s not just web applications this system should apply to. Whether it\u2019s a blog, portfolio, landing page, or wiki, they are all still usable products. They still require important user experience considerations such as accessibility, navigation, color practices, and typography scales. Many companies consider such aspects, while many ignore them either through choice, misjudgement, or lack of consideration. It\u2019s an area which is so fragmented under the current system, and does not work appropriately for everyone. That includes those with a disability, visual impairment, or lack of familiarity with computers and the web. These users should be designed for first.\n\nAs it stands, the primary consideration is often the design visuals: making something impressive, unique, and eye-catching. Often this desire to differentiate can lead to oversights with user experience, and design choices like unique navigation solutions which are confusing and unfamiliar to most.\n\nGoogle is a prime example of a company who have developed a set of guidelines and applied them with absolute consistency across mobile and web. Whether you switch from Google Keep on iPhone, to Google Drive on the web, the user experience and design elements remain consistent. Then when switching between products on the web like Play Store or YouTube, it again remains consistent.\n\nThis ease of use and transition from one product or site to another should be a model to follow for others. It puts the user first, making for an entirely accessible and understandable experience. Google is beginning to take this even a step further, as they introduce Android apps and web-based equivalents that work at desktop size. With products like Chromebooks, it makes the transition between devices even more seamless.\n\nThe closer we can get to a cohesive design system across the web\u2026the better it will be\u2026for all parties involved\n\nThe closer we can get to a cohesive design system across the web as a whole, the better it will be in the long run, for all parties involved. This means having systems span much further than just one company.\n\nIBM or Airbnb may perfect their design systems to the nth degree and apply them with excellent consistency. However, as soon as a user switches to another product or service, their design system is likely to be wholly different, from typography and layout, to navigational practices. That\u2019s why it needs to be looked at as an issue from further afar. And apps are the closest example we have to how successful this can be as a means to improve the everyday lives of users.\n\nEasily Whip Up Custom Videos for Your Brand with Videobolt \u2013 only $24!\n\nLoad More Related Articles\nLoad More By\u00a0Ben Bate\nLoad More In\u00a0\nComments are closed.\n\nCheck Also\n\nA Guide to Payment and Invoicing Tools for Designers\/Developers\n\nYou're reading A Guide to Payment and Invoicing Tools for Designers\/Developers, originally\u2026",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.807772398,
        "format_confidence":0.8990033269
    },
    {
        "url":"https:\/\/insights.dice.com\/2017\/06\/07\/coreml-step-forward-machine-learning\/?ads_kw=apple+machine+learning",
        "text":"Apple\u2019s CoreML a Big Step for Machine Learning\n\nCoreML WWDC 2017\nCoreML WWDC 2017\n\nMachine learning is not a fad or distraction for major tech companies. It\u2019s been in use for quite some time, although tech giants such as Google have only recently made a big show of publicizing their efforts in it. Apple is finally letting its developer group tap into it via the new CoreML framework. Here\u2019s how.\n\nCoreML is, as you may guess by the name, dedicated to machine learning. It\u2019s positioned to serve as the go-to framework for machine learning on Apple platforms such as iOS (that\u2019s why Apple branded it with the \u2018core\u2019 moniker).\n\nStill officially in beta, CoreML is positioned as something that will be ready for use with iOS 11. An important note:\u00a0users don\u2019t need new hardware for an app to take advantage of CoreML; it\u2019s a framework, and not reliant on any specific hardware or on-device functionality.\n\nThe first thing developers will want to do is find a machine learning model. Apple doesn\u2019t have any restrictions here, per se, but does offer up some widely used models as a sort of \u2018best practices\u2019 for CoreML.\n\nBut all models touted\u00a0by Apple hint at a fundamental problem with CoreML, as well. Each model is meant for scene detection, which is how apps can scan images for faces, trees or other objects. As Apple states:\n\nSupported features include face tracking, face detection, landmarks, text detection, rectangle detection, barcode detection, object tracking, and image registration.\n\nCoreML Vision doesn\u2019t access machine learning models via an API. Instead, Apple has several classes for implementing the models. VNImageRequestHandler is \u201can object that processes one or more image analysis requests pertaining to a single image,\u201d for example. In lay-terms, it\u2019s the request to scan an image for a particular feature, such as a face. Scanning multiple images will require the VNSequenceRequestHandler class.\n\nWithin Vision, you can get quite detailed. Developers can scan for faces, but also facial landmarks like eyes or a mouth. In a crude way, this is how those apps that force a smile on a photographed face\u00a0work; they find your mouth, then present a layer that \u2018adds\u2019 a smile.\n\nCoreML Architecture WWDC 2017\nCoreML Architecture WWDC 2017\n\nCoreML Vision is deep, and will be attractive for simple-purpose apps. Developers who try to corral the entirety of this framework will have cumbersome codebases to support. An example: Apple has five classes dedicated to object detection and tracking, two for horizon detection, and five supporting superclasses for Vision. It\u2019s a lot to take in, and this is all before we convert the model.\n\nWhile pulling your model into the app\u2019s code is necessary, Apple also asks developers to convert it to a CoreML-friendly framework. It has a fairly easy-to-use conversion for most well-known models, but developers who use something not officially recognized may need to create their own conversion tool. Apple notes that developers will be \u201cdefining each layer of the model\u2019s architecture and its connectivity with other layers,\u201d but only says they should \u201cuse the conversion tools provided by Core ML Tools as examples.\u201d There\u2019s no strong guidance there.\n\nIf you need to create a Vision app that\u2019s a bit more robust, CoreML has an API. It\u2019s meant for custom workflows and \u201cadvanced use cases.\u201d\n\nVision is only one aspect of CoreML, though. Speech and text also play a big role, which is where Foundation comes into play. Foundation may have a lot of moving parts, but its Strings and Text structs and classes make an appearance in CoreML. Most notably, NSLinguisticTagger will help apps analyze speech and break it into readable chunks with localization.\n\nThe third part of all this is the subtlest player. GameplayKit helps evaluate \u201clearned decision trees\u201d in CoreML. It\u2019s basically the logic engine driving all of the Vision and Foundation modules. As the framework\u00a0is optimized for on-device performance, it takes a strong team to make it work. Vision, Foundation and GameplayKit are the stars, while frameworks such as Accelerate and Basic neural network subroutines (BNNS) pick up some of the slack.\n\nUsing CoreML isn\u2019t going to be easy. It has a lot of moving parts, and there\u2019s an exhaustive list of customizations that can be made. Still, if machine learning is something your app can take advantage of, it\u2019s likely worth the trouble. CoreML is a part of\u00a0Apple\u2019s universe now, and it\u2019s going to take an increasingly larger role as time goes on. Now\u2019s the best time to get acquainted with it.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9729292989,
        "format_confidence":0.7701945901
    },
    {
        "url":"https:\/\/www.gadgetdaily.xyz\/kernel-3-19-development-the-kernel-column\/",
        "text":"\n\nNotice: Undefined index: post_link_target in \/nas\/content\/live\/gadgetmag\/wp-content\/plugins\/smart-scroll-posts\/smart-scroll-posts.php on line 195\n\nNotice: Undefined index: posts_featured_size in \/nas\/content\/live\/gadgetmag\/wp-content\/plugins\/smart-scroll-posts\/smart-scroll-posts.php on line 196\n\nKernel 3.19 development \u2013 the kernel column\n\nJon Masters summarises the latest happenings in the Linux kernel community as work continues on the 3.19 series kernel\n\nLinus Torvalds, freshly returned from speaking at Linux Conf AU (LCA) 2015, announced 3.19- rc5 saying \u201c[a]nother week, another -rc\u201d. His announcement mail included his usual opening about his desire for less churn late in the development cycle (Linux kernels typically have up to 8 RCs \u2013 or Release Candidates \u2013 in the two months of the average release). Overall, Linux 3.19 is shaping up to be a normal sized release \u2013 though there\u2019s still well over 10,000 individual commits or patches, each with many lines, which isn\u2019t bad when you consider how the development largely aligned with the end of year holiday period. The new kernel will add a few exciting features, including support for Intel\u2019s MPX processor extensions, and the nios2 embedded system microprocessor architecture from Altera.\n\nLooking further out beyond 3.19 and into the new year, it is interesting to consider that we have another 3 or 4 kernel development cycles coming up over the next few months. That\u2019s where we would like to hear from you. Drop us a line and let us know what exciting kernel development projects you have in store for the year and what you would like to see featured in these pages. Linux User & Developer aims to provide you with fresh and original content, and we are always willing to take on deeply technical topics in our quest to help you learn more about how Linux works inside.\n\nIntel MPX\n\nPerhaps the most exciting feature merged for Linux 3.19 is support for the emerging Intel MPX technology. Memory Protection Extensions (MPX) are an ISA (Instruction Set Architecture) addition to the venerable x86 instruction set that can be used to increase software security by automatically guarding against certain buffer overflow and other generic \u2018bounds checking failure\u2019-type security flaws. In other words, MPX isn\u2019t particularly sexy in terms of end users having something shiny to play with, but it stands to provide a significant improvement in security that users will certainly appreciate.\n\nMPX works by introducing some new system registers and special instructions, that supporting compilers (including the GCC) can emit during code compilation to cause those registers to be programmed with additional information about a program, such as the ranges of memory to which it intends to have access. As an instruction set extension, MPX will have no impact on existing applications, but new or existing code can be recompiled to take advantage of this feature. Since many software applications are routinely rebuilt and upgraded, and MPX can be enabled with very little work on the part of the programmer (the compiler takes care of most of the details), it is likely that we will see a growing adoption of this technology over the next few years. This is a good thing, because it takes years for new hardware features to make it into a market that can provide such capabilities. It\u2019s certainly time security improved.\n\n\u201cOut out damn perl\u201d\n\nRob Landley posted a patch that (re)removed a build dependency upon the Perl scripting language being required to compile a Linux kernel. As a complex software project, Linux has dependencies upon various external tools like compilers and assemblers, but is generally (surprisingly) lightweight in terms of its requirements \u2013 at build time, at least. You can compile a kernel on a Raspberry Pi (if you have a lot of time to kill) \u2013 it\u2019ll just take a lot longer than it would on your desktop or laptop. It was this flexibility to compile Linux in many places that Rob was seeking to preserve in mailing.\n\nAt first blush his patch, which was entitled \u201cout out damn perl\u201d, might seem overly harsh upon our obfuscated Perl-coding friends, but Linux has a good track record of keeping its build requirements modest (and free of much more than a shell for scripting purposes). The resulting discussion centered around separation of build from development tools. It is reasonable to require Perl for certain development and kernel maintainer activities. For example, Linux ships with an infamous script called \u2018checkpatch.pl\u2019 that verifies whether patches submitted by developers for inclusion meet the Linux coding style guidelines and community conventions. Of course, this script itself being written in Perl caused a number of jokes suggesting that it should be modified to warn whenever it was run with a Perl warning.\n\n\nLinux 3.19 will include support for the AMD \u201cKFD\u201d kernel driver. This is part of their HSA (Heterogenous System Architecture) Foundation work which seeks to produce a completely open source solution for developing and using GPGPU- based technology with Linux. The KFD works a bit like the existing Linux DRM (Direct Rendering Manager) in communicating with a \u2018Graphics\u2019 Processor Unit (that might not actually be capable of graphics), except KFD doesn\u2019t care about actually driving a display. What it does instead is load \u2018kernels\u2019 (nothing to do with the Linux kernel) of compiled GPU programs onto the GPU and manage them at runtime so that the results of computions can be returned. Using KFD, along with other HSA tooling (such as the r600 OpenCL LLVM Clang-based compiler) will make for a fun project for this author over the coming weeks. AMD have example projects available at their HSAFoundation GitHub, including a Matrix multiplication demo.\n\nOngoing Development\n\nAlan Cox, in following up to another thread on maintainer abuse (which focused on what to do about submissions of patches that happen in the middle of the frenzy of craziness that is the merge window during the first couple of weeks of a kernel development cycle), asked whether it was \u201cthe year for a Google summer of code project or similar to turn Patchwork into a proper patch management tool\u201d. Patchwork is used by a number of kernel folks to track patches that have been posted, revised and merged, but it is far from a polished product with the kinds of capabilities that Alan and others would obviously like to see being made available as Linux continues to grow.\n\nSeth Jennings, Jiri Kosina and many others have been involved in a number of conversations around unifying the approach to Kernel Live Patching. A variety of different projects exist (and of course, there was also the previous Ksplice project that ultimately went more proprietary) that aim to provide live patching to a running kernel. It is hoped that a number of the developers will meet at the upcoming Linux Collaboration Summit and hash out a plan for a collaboration that will land initial support in 3.20. This would be the first time that an upstream kernel had such a solution.\n\nThis author posted about adding support for the SPCR (Serial Port Console Redirection) ACPI table in Linux. This feature will obviate the need to supply \u2018console=\u2019 parameters on servers because the kernel will be able to determine this automatically, based upon platform data provided by the system vendor that has been supported in Microsoft Windows for years.\n\nDan Carpenter announced that version 1.60 of Smatch, his static C language type checker, is available. Smatch understands many Linux kernelisms and is a useful tool for verifying kernel code.",
        "topic_id":2,
        "format_id":11,
        "topic_confidence":0.9380750656,
        "format_confidence":0.7411324382
    }
]