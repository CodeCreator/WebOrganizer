[
    {
        "url":"http:\/\/enc.tfode.com\/FOSS",
        "text":"Free and open-source software\n\n(Redirected from FOSS)\nJump to: navigation, search\nA screenshot of free and open-source software: Linux Mint running the Xfce desktop environment, Firefox, a calculator program, the built-in calendar, Vim, GIMP, and VLC media player\n\nFree and open-source software (FOSS) is software that can be classified as both free software and open-source software.[a] That is, anyone is freely licensed to use, copy, study, and change the software in any way, and the source code is openly shared so that people are encouraged to voluntarily improve the design of the software.[3] This is in contrast to proprietary software, where the software is under restrictive copyright and the source code is usually hidden from the users.\n\nThe benefits of using FOSS can include decreased software costs, increased security and stability (especially in regard to malware), protecting privacy, education, and giving users more control over their own hardware. Free, open-source operating systems such as Linux and descendants of BSD are widely utilized today, powering millions of servers, desktops, smartphones (e.g. Android), and other devices.[4][5] Free software licenses and open-source licenses are used by many software packages. The Free software movement and the open-source software movement are online social movements behind widespread production and adoption of FOSS.\n\n\nFree and open source software is an umbrella term for software that is free and open source software. Free and open source software allows the user to inspect the source code and provides a high level of control of the software's functions compared to proprietary software.\n\nAccording to the Free Software Foundation, \"Nearly all open source software is free software. The two terms describe almost the same category of software, but they stand for views based on fundamentally different values.\"[6] Thus, the Open Source Initiative considers many free software licenses to also be open-source. These include the latest versions of the FSF's three main licenses: the GPL, the Lesser General Public License (LGPL), and the GNU Affero General Public License (AGPL).[7] Thus, terminology of free and open source software is intended to be neutral on these philosophical disagreements.\n\nThere are a number of related terms and abbreviations for free and open source software (FOSS or F\/OSS) or free\/libre and open source software (FLOSS).\n\nFree software\n\nRichard Stallman's Free Software Definition, adopted by the Free Software Foundation (FSF), defines free software as a matter of liberty, not price.[8] The earliest known publication of the definition of his free software idea was in the February 1986 edition[9] of the FSF's now-discontinued GNU's Bulletin publication. The canonical source for the document is in the philosophy section of the GNU Project website. As of August 2017, it is published there in 40 languages.[10]\n\nOpen source\n\nThe Open Source Definition is used by the Open Source Initiative to determine whether a software license qualifies for the organization's insignia for open-source software. The definition was based on the Debian Free Software Guidelines, written and adapted primarily by Bruce Perens.[11][12] Perens did not base his writing on the four freedoms of free software from the Free Software Foundation, which were only later available on the web.[13] Perens subsequently stated that he felt Eric Raymond's promotion of open source unfairly overshadowed the Free Software Foundation's efforts and reaffirmed his support for free software.[14] In the following 2000s he spoke about Open source again.[15][16]\n\n\nIn the 1950s through the 1980s, it was common for computer users to have the source code for all programs they used, and the permission and ability to modify it for their own use. Software, including source code, was commonly shared by individuals who used computers, often as public domain software.[17] Most companies had a business model based on hardware sales, and provided or bundled software with hardware, free of charge.[18] Organizations of users and suppliers were formed to facilitate the exchange of software; see, for example, SHARE and DECUS.\n\nBy the late 1960s, the prevailing business model around software was changing. A growing and evolving software industry was competing with the hardware manufacturer's bundled software products; rather than funding software development from hardware revenue, these new companies were selling software directly. Leased machines required software support while providing no revenue for software, and some customers able to better meet their own needs did not want the costs of software bundled with hardware product costs. In United States vs. IBM, filed 17 January 1969, the government charged that bundled software was anticompetitive.[19] While some software might always be free, there would be a growing amount of software that was for sale only. In the 1970s and early 1980s, some parts of the software industry began using technical measures (such as distributing only binary copies of computer programs) to prevent computer users from being able to use reverse engineering techniques to study and customize software they had paid for. In 1980, the copyright law was extended to computer programs in the United States[20]\u2014previously, computer programs could be considered ideas, procedures, methods, systems, and processes, which are not copyrightable.[21][22]\n\nEarly on, closed-source software was uncommon until the mid-1970s to the 1980s, when IBM implemented in 1983 a \"object code only\" policy, no longer distributing source code.[23][24][25]\n\nIn 1983, Richard Stallman, longtime member of the hacker community at the MIT Artificial Intelligence Laboratory, announced the GNU project, saying that he had become frustrated with the effects of the change in culture of the computer industry and its users.[26] Software development for the GNU operating system began in January 1984, and the Free Software Foundation (FSF) was founded in October 1985. An article outlining the project and its goals was published in March 1985 titled the GNU Manifesto. The manifesto included significant explanation of the GNU philosophy, Free Software Definition and \"copyleft\" ideas.\n\nThe Linux kernel, started by Linus Torvalds, was released as freely modifiable source code in 1991. Initially, Linux was not released under a free or open-source software license. However, with version 0.12 in February 1992, he relicensed the project under the GNU General Public License.[27] Much like Unix, Torvalds' kernel attracted the attention of volunteer programmers.[citation needed]\n\nFreeBSD and NetBSD (both derived from 386BSD) were released as free software when the USL v. BSDi lawsuit was settled out of court in 1993. OpenBSD forked from NetBSD in 1995. Also in 1995, The Apache HTTP Server, commonly referred to as Apache, was released under the Apache License 1.0.\n\nIn 1997, Eric Raymond published The Cathedral and the Bazaar, a reflective analysis of the hacker community and free software principles. The paper received significant attention in early 1998, and was one factor in motivating Netscape Communications Corporation to release their popular Netscape Communicator Internet suite as free software. This code is today better known as Mozilla Firefox and Thunderbird.\n\nNetscape's act prompted Raymond and others to look into how to bring the FSF's free software ideas and perceived benefits to the commercial software industry. They concluded that FSF's social activism was not appealing to companies like Netscape, and looked for a way to rebrand the free software movement to emphasize the business potential of sharing and collaborating on software source code. The new name they chose was \"open source\", and quickly Bruce Perens, publisher Tim O'Reilly, Linus Torvalds, and others signed on to the rebranding. The Open Source Initiative was founded in February 1998 to encourage use of the new term and evangelize open-source principles.[28]\n\nWhile the Open Source Initiative sought to encourage the use of the new term and evangelize the principles it adhered to, commercial software vendors found themselves increasingly threatened by the concept of freely distributed software and universal access to an application's source code. A Microsoft executive publicly stated in 2001 that \"open source is an intellectual property destroyer. I can't imagine something that could be worse than this for the software business and the intellectual-property business.\"[29] This view perfectly summarizes the initial response to FOSS by some software corporations.[citation needed] However, while FOSS has historically played a role outside of the mainstream of private software development, companies as large as Microsoft have begun to develop official open-source presences on the Internet. IBM, Oracle, Google and State Farm are just a few of the companies with a serious public stake in today's competitive open-source market. There has been a significant shift in the corporate philosophy concerning the development of free and open-source software (FOSS).[30]\n\n\nBenefits over proprietary software\n\nPrivacy and security\n\nManufacturers of proprietary, closed-source software are sometimes pressured to building in backdoors or other covert, undesired features into their software.[31][32][33][34] Instead of having to trust software vendors users of FOSS can inspect and verify the source code themselves and can put trust on a community of volunteers and users.[35] As proprietary code is typically hidden from public view, only the vendors themselves and hackers may be aware of any vulnerabilities in them[35] while FOSS involves as many people as possible for exposing bugs quickly.[36][37]\n\nPersonal control, customizability and freedom\n\nUsers of FOSS benefit from the freedoms to making unrestricted use, study, copy, modify, and redistribute such software. If they would like to change the f... (truncated)",
        "topic_id":2,
        "format_id":7,
        "topic_confidence":0.7420488596,
        "format_confidence":0.9777898788
    },
    {
        "url":"http:\/\/webdesigndomain.com.au\/component\/content\/article\/135-latest-news\/32-responsive-web-design.html?Itemid=437",
        "text":"0417 655 247\n\nResponsive Web Design\n\n\nresp appResponsive Web Design (RWD) is a Web design approach aimed at crafting sites to provide an optimal viewing experience\u2014easy reading and navigation with a minimum of resizing, panning, and scrolling\u2014across a wide range of devices (from mobile phones to desktop computer monitors).\n\nA site designed with RWD adapts the layout to the viewing environment by using fluid, proportion-based grids, flexible images, and CSS3 media queries.\n\n\"If you've ignored previous warnings and your business isn't taking advantage of responsive web design right now or planning on it in the very near future, you are in danger of going out of business in 2014.\"1\n\n\nGo Back",
        "topic_id":2,
        "format_id":7,
        "topic_confidence":0.8401029706,
        "format_confidence":0.5967438221
    },
    {
        "url":"http:\/\/xml.coverpages.org\/ni2001-08-07-a.html",
        "text":"\nAdvanced Search\nSite Map\nCP RSS Channel\nContact Us\nSponsoring CP\nAbout Our Sponsors\n\nCover Stories\nArticles & Papers\nPress Releases\n\nXML Query\n\nXML Applications\nGeneral Apps\nGovernment Apps\nAcademic Apps\n\nTechnology and Society\nTech Topics\nRelated Standards\nCreated: August 07, 2001.\nNews: Cover StoriesPrevious News ItemNext News Item\n\nWebsign Markup Language Supports Ubiquitous, Location-Aware Computing.\n\nResearchers in Hewlett-Packard's CoolTown research program are developing a \"Websign\" application for wireless devices which combines the advantages of wireless technology and ubiquitous computing \"to provide a transparent linkage between the physical world and resources available on the Web.\" The websign technology \"uses commonly available Internet-enabled wireless devices such as PDAs or smart phones equipped with client software, a positioning system such as GPS, and a digital compass to visualize services for physical entities. Devices sense physical entities in the environment and map them to a Web browser. When the user requests new information, the mobile device connects to a Web server and downloads and caches XML descriptions of websigns in a wide surrounding area. Websigns essentially bind location coordinates, control parameters such as access range, and a service represented by a URL. The Websign Markup Language (WsML), an XML application, is used to express the binding semantics: the Web servers host WsML for mobile devices to download over a cellular wireless connection. Mobile devices can also host WsML for other peer-to-peer devices. Typically, peer devices can communicate over short-range radio networks such as Bluetooth or send WsML embedded in text-message-over systems such as Short Message Service.\" WsML, similar to Geography Markup Language (GML), \"provides a compact format for transmitting binding information over a low-bandwidth wireless network.\"\n\n\n\"Using a Web interface lets the [Websign] infrastructure remain both simple and open. The Web interface provides a geographical map as a reference for placing websigns. In the back end, a database stores the location, URL, and other binding information entered through the Web interface. When users connect to this server via HTTP and optionally post their coarse granular location to a Web server, they are served WsML based on the information in the database as well as their profile. An alternative to dynamically generating WsML is to statically create and host it on a Web server, but doing so removes the ability to personalize content.\"\n\nWebsign clients can either autodiscover or manually select WsML servers. The system uses information transmitted from IR or radio beacons to autoconfigure clients to communicate with known servers. Autoconfiguration is particularly useful in an enterprise scenario. For example, when employees or visitors enter a building, an IR or short-range radio message can configure their PDAs to communicate with a websign server. The PDA downloads WsML, which contains pointers to physical resources such as printers and smart conference rooms and their associated services.\"\n\nFor a number of reasons, the development team \"chose to define a new, simple markup language instead of using the, another XML application, to bind complex geospatial areas to URLs. GML's attribute set contains complex descriptions that the websign system does not need, and it cannot meet some websign system requirements. To keep mathematical computation manageable in a PDA, for example, they bind a URL to a simple point with a surrounding square area as the access zone. Web-signs also have temporal activation parameters, which GML normally does not describe. WsML is still an evolving format, and keeping it compact requires restricting the core language to a bare minimum...\"\n\n\"Websigns: Hyperlinking Physical Locations to the Web.\" By Salil Pradhan, Cyril Brignone, Jun-Hong Cui, Alan McReynolds, and Mark T. Smith. In IEEE Computer Volume 34, Number 8 (August 2001), pages 42-46. [Special Issue on \"Location Aware Computing.\"]\n\nPrincipal references:\n\n  \u2022 CoolTown research program, Hewlett-Packard Laboratories\n  \u2022 \"Websign: Hyperlinks from a Physical Location to the Web.\" By Pradhan, Salil; Brignone, Cyril; Cui, Jun-Hong; McReynolds, Alan; Smith, Mark. HP Labs Technical Reports. HPL-2001-140. 2001-07-16. \"In the CoolTown research program at Hewlett Packard Laboratories, we are building Ubiquitous Computing systems by embedding and embodying web technologies into physical environments. One of the newer components of this research is Websign. By using a simple form of augmented reality, the system allows users to visualize services related to physical objects of interest. The websign system provides infrastructure not just for detecting websigns but also for creating and deploying them. In this paper we present the concept, an overview of the prototype and the algorithms used in the implementation.\"\n  \u2022 See also: \"Geography Markup Language (GML)\" - Main reference page.\n\nHosted By\nOASIS - Organization for the Advancement of Structured Information Standards\n\nSponsored By\n\nIBM Corporation\nISIS Papyrus\nMicrosoft Corporation\nOracle Corporation\n\n\nXML Daily Newslink\nReceive daily news updates from Managing Editor, Robin Cover.\n\n\u00a0Newsletter Subscription\n\u00a0Newsletter Archives\nBottom Globe Image\n\nDocument URI:\u00a0\u00a0\u2014\u00a0\u00a0Legal stuff\nRobin Cover, Editor:",
        "topic_id":2,
        "format_id":7,
        "topic_confidence":0.673517406,
        "format_confidence":0.7360419631
    },
    {
        "url":"http:\/\/www.informit.com\/articles\/article.aspx?p=1681034",
        "text":"Home > Articles > Software Development & Management > Agile\n\nThe Big Picture of Agile Requirements\n\n  \u2022 Print\n  \u2022 + Share This\nWe are now at the point in time where a number of organizations have made the transition before us and some common patterns for lean and agile software process success have started to emerge -- a Big Picture. In this chapter, Dean Leffingwell offers a quick gestalt of this new, agile, leaner, and yet fully scalable software requirements model.\nThis chapter is from the book\n  \u2022 This would all be a lot easier to understand if you could just draw me a picture.\n  \u2022 \u2014Anonymous senior executive\n\nEffectively implementing a new set of lean and agile requirements principles and practices in a project team, program, or enterprise is no small feat. Even the language is different and seemingly odd (user stories, sprints, velocity, story points, epics, backlog?). In addition, further \"leaning\" the organization often requires eliminating or reducing requirements specifications, design specifications, stage-gated governance models (with incumbent requirements reviews), sign-offs (with incumbent delays...), implementing work-in-process limits (which may seem counterproductive to those who measure \"utilization\"), and so on. So, there will likely be many challenges.\n\nEven for the fully committed, it can take six months to a year to introduce and implement the basic practices and even more time to achieve the multiples of productivity and quality results that pay the ultimate dividends in customer satisfaction, revenue, or market share. To achieve these benefits, we must change many things, including virtually all of our former requirements management practices. However, many of the existing required artifacts, milestones, and so on, serve as safeguards to \"help\" avoid the types of project problems that software has often experienced. So, we have a dilemma\u2014how do we practice this new high-wire act without a safety net, when the safety net itself is a big part of the problem?\n\nFortunately, we are now at the point in time where a number of organizations have made the transition before us and some common patterns for lean and agile software process success have started to emerge. In our discussions with teams, managers, and executives during this transition, we often struggled to find a language for discussion, a set of abstractions, and an appropriate graphic that we could use to quickly describe \"what your enterprise would look like and how it would work after such an agile transformation.\"\n\nTo do so, we need to be able to describe the new software development and delivery process mechanisms, the new teams and organizational units, and some of the roles key individuals play in the new agile paradigm. In addition, any such Big Picture should highlight the requirements practices of the model, because those artifacts are the proxy for the value stream.\n\nEventually, and with help from others, we arrived at something that worked reasonably well for its purpose.1 We call it the Agile Enterprise Big Picture, and it appears in Figure 2-1.\n\nFigure 2-1\n\nFigure 2-1 The Agile Enterprise Big Picture\n\nThe Big Picture Explained\n\nIn this chapter, we'll explain the Big Picture in a summary format intended to provide the reader with a quick gestalt of this new, agile, leaner, and yet fully scalable software requirements model.\n\nIn the remaining chapters of Part I of this book, we'll describe the basic big-picture requirements management practices for the individual Team, Program, and Portfolio levels. In Parts II, III, and IV, we'll further elaborate on the requirements management artifacts, roles, and activities at a level of detail suitable for implementation and action.\n\nBig-Picture Highlights\n\nBecause this picture serves as both the organizational and process model for our agile requirements practices, we'll have time throughout this book to explore its many nuances. However, from an overview perspective, the following highlights emerge.\n\nThe Team Level\n\nAt the Team level, agile teams of 7\u00b12 team members define, build, and test user stories in a series of iterations and releases. In the smallest enterprise, there may be only a few such teams. In larger enterprises, groups, or pods, of agile teams work together to support building up larger functionality into complete products, features, architectural components, subsystems, and so on. The responsibility for managing the backlog of user stories and other things the team needs to do belongs to the team's product owner.\n\nThe Program Level\n\nAt the Program level, the development of larger-scale systems functionality is accomplished via multiple teams in a synchronized Agile Release Train (ART). The ART is a standard cadence of timeboxed iterations and milestones that are date- and quality-fixed, but scope is variable (no iron triangle). The ART produces releases or potentially shippable increments (PSIs) at frequent, typically fixed, 60- to 120-day time boundaries. These evaluable increments can be released to the customer, or not, depending on the customer's capacity to absorb new product as well as external events that can drive timing.\n\nWe'll use the generic product manager label as the title for those who are responsible for defining the features of the system at this level, though we'll also see that many other titles can be applied to this role.\n\nThe Portfolio Level\n\nAt the Portfolio level, we'll talk about a mix of investment themes that are used to drive the investment priorities for the enterprise. We'll use that construct to assure that the work being performed is the work necessary for the enterprise to deliver on its chosen business strategy. Investment themes drive the portfolio vision, which will be expressed in as a series of larger, epic-scale initiatives, which will be allocated to various release trains over time.\n\nIn the rest of this chapter, we'll walk through the various elements of the Big Picture to describe how it works. While we'll highlight the requirements value delivery stream, we'll also expose the rest of the picture including the roles, teams, and processes that are necessary to deliver value. In this way, we'll provide a systemic view of our lean and agile requirements process that works for teams and yet scales to the full needs of the enterprise.\n\n  \u2022 + Share This\n  \u2022 \ud83d\udd16 Save To Your Account",
        "topic_id":2,
        "format_id":7,
        "topic_confidence":0.9793431759,
        "format_confidence":0.5128322244
    },
    {
        "url":"https:\/\/wiki.eclipse.org\/index.php?title=Eclipse_DemoCamps_November_2010\/Bay_Area&oldid=224079",
        "text":"Skip to main content\nJump to: navigation, search\n\nEclipse DemoCamps November 2010\/Bay Area\n\n< Eclipse DemoCamps November 2010\nRevision as of 16:11, 18 October 2010 by (Talk | contribs) (New page: Image:Eclipse_DemoCamp_New.jpg What is an Eclipse DemoCamp? === Location === Replay Solutions<br> 2100 Seaport Blvd.<br> Top Floor<br> Redwood C...)\n\n(diff) \u2190 Older revision | Latest revision (diff) | Newer revision \u2192 (diff)\n\nEclipse DemoCamp Old.jpg What is an Eclipse DemoCamp?\n\n\nReplay Solutions\n2100 Seaport Blvd.\nTop Floor\nRedwood City, CA 94063\n\nDate and Time\n\nComing soon\n\n\nEclipse logo.png\n\n\nJonathan Lindo, Replay Solutions\nPieter Humphrey, Oracle\n\n\nIf you would like to present at the DemoCamp, please feel free to add your name and topic to the list.\n\n  1. Name, Company, Topic\n\nWho Is Attending\n\nIf you plan on attending please add your name and company to the list below. If you have any trouble with the wiki, just send an email to jonathan.lindo at replaysolutions dot com.\n\n  1. Jonathan Lindo, Replay Solutions\n  2. Pieter Humphrey, Oracle\n\nBack to the top",
        "topic_id":2,
        "format_id":7,
        "topic_confidence":0.9592922926,
        "format_confidence":0.6923756003
    },
    {
        "url":"https:\/\/www.owasp.org\/index.php?title=A7_2004_Improper_Error_Handling&diff=42834&oldid=42726",
        "text":"Difference between revisions of \"A7 2004 Improper Error Handling\"\n\nJump to: navigation, search\n(New page: ==Description== Improper handling of errors can introduce a variety of security problems for a web site. The most common problem is when detailed internal error messages such as stack tra...)\nm (Protected \"A7 2004 Improper Error Handling\" [edit=sysop:move=sysop])\n(2 intermediate revisions by the same user not shown)\nLine 1: Line 1:\n''This article is for the OWASP Top 10 2004''\nLine 17: Line 19:\n==Examples and References==\n==Examples and References==\n* OWASP discussion on generation of error codes: http:\/\/www.owasp.org\/documentation\/guide\/\n* [[Error_Handling,_Auditing_and_Logging|OWASP Development Guide, Error Handling]]\n==How to Determine If You Are Vulnerable ==\n==How to Determine If You Are Vulnerable ==\n\nLatest revision as of 07:19, 10 October 2008\n\nThis article is for the OWASP Top 10 2004\n\n\nImproper handling of errors can introduce a variety of security problems for a web site. The most common problem is when detailed internal error messages such as stack traces, database dumps, and error codes are displayed to the user (hacker). These messages reveal implementation details that should never be revealed. Such details can provide hackers important clues on potential flaws in the site and such messages are also disturbing to normal users.\n\nWeb applications frequently generate error conditions during normal operation. Out of memory, null pointer exceptions, system call failure, database unavailable, network timeout, and hundreds of other common conditions can cause errors to be generated. These errors must be handled according to a well thought out scheme that will provide a meaningful error message to the user, diagnostic information to the site maintainers, and no useful information to an attacker.\n\nEven when error messages don\u2019t provide a lot of detail, inconsistencies in such messages can still reveal important clues on how a site works, and what information is present under the covers. For example, when a user tries to access a file that does not exist, the error message typically indicates, \u201cfile not found\u201d. When accessing a file that the user is not authorized for, it indicates, \u201caccess denied\u201d. The user is not supposed to know the file even exists, but such inconsistencies will readily reveal the presence or absence of inaccessible files or the site\u2019s directory structure.\n\nOne common security problem caused by improper error handling is the fail-open security check. All security mechanisms should deny access until specifically granted, not grant access until denied, which is a common reason why fail open errors occur. Other errors can cause the system to crash or consume significant resources, effectively denying or reducing service to legitimate users.\n\nGood error handling mechanisms should be able to handle any feasible set of inputs, while enforcing proper security. Simple error messages should be produced and logged so that their cause, whether an error in the site or a hacking attempt, can be reviewed. Error handling should not focus solely on input provided by the user, but should also include any errors that can be generated by internal components such as system calls, database queries, or any other internal functions.\n\nEnvironments Affected\n\nAll web servers, application servers, and web application environments are susceptible to error handling problems.\n\nExamples and References\n\nHow to Determine If You Are Vulnerable\n\nTypically, simple testing can determine how your site responds to various kinds of input errors. More thorough testing is usually required to cause internal errors to occur and see how the site behaves.\n\nAnother valuable approach is to have a detailed code review that searches the code for error handling logic. Error handling should be consistent across the entire site and each piece should be a part of a well-designed scheme. A code review will reveal how the system is intended to handle various types of errors. If you find that there is no organization to the error-handling scheme or that there appear to be several different schemes, there is quite likely a problem.\n\nHow to Protect Yourself\n\nA specific policy for how to handle errors should be documented, including the types of errors to be handled and for each, what information is going to be reported back to the user, and what information is going to be logged. All developers need to understand the policy and ensure that their code follows it.\n\nIn the implementation, ensure that the site is built to gracefully handle all possible errors. When errors occur, the site should respond with a specifically designed result that is helpful to the user without revealing unnecessary internal details. Certain classes of errors should be logged to help detect implementation flaws in the site and\/or hacking attempts. Very few sites have any intrusion detection capabilities in their web application, but it is certainly conceivable that a web application could track repeated failed attempts and generate alerts. Note that the vast majority of web application attacks are never detected because so few sites have the capability to detect them. Therefore, the prevalence of web application security attacks is likely to be seriously underestimated.\n\nThe OWASP Filters project is producing reusable components in several languages to help prevent error codes leaking into user\u2019s web pages by filtering pages when they are constructed dynamically by the application.",
        "topic_id":2,
        "format_id":7,
        "topic_confidence":0.956305027,
        "format_confidence":0.6166646481
    },
    {
        "url":"https:\/\/wiki.python.org\/moin\/SecretLabs?highlight=XmlRpcLib",
        "text":"Secret Labs AB is a Swedish company that produces open-source software for Python and runs the PythonWare and Web sites. They are responsible for producing the Python Imaging Library (PIL), the XML-RPC Library (xmlrpclib) as well as several other products and portions of the Python Interpreter itself.\n\nThey could previously be found at\n\nSecretLabs (last edited 2008-11-15 14:00:36 by localhost)\n\nUnable to edit the page? See the FrontPage for instructions.",
        "topic_id":2,
        "format_id":7,
        "topic_confidence":0.9573401809,
        "format_confidence":0.9258438349
    },
    {
        "url":"https:\/\/clean.cs.ru.nl\/index.php?title=ITasks&oldid=872",
        "text":"From Clean\nRevision as of 13:10, 2 October 2012 by Bas Lijnse (talk | contribs) (Get from subversion)\nJump to: navigation, search\n\nThe iTask system (iTasks) is a task-oriented programming toolkit for programming workflow support applications in Clean.\n\nWith this toolkit workflows can be speficied using combinators in a very high level declarative monadic style. Workflows consist of typed tasks that produce results that can be passed as parameters to other tasks. Tasks are constructed by combining single steps sequentially or in parallel. From iTask specifications, executable workflow support systems are generated automatically.\n\nThe iTask2 GUI\n\nThis latest version features:\n\n  \u2022 A highly declarative API for specification of dynamic workflows\n  \u2022 A rich Ajax client interface for working on tasks\n  \u2022 Automatically generated editors for entering and updating data in workflows.\n  \u2022 A JSON based service API for spawning and interacting with running workflow instances\n  \u2022 The possibility to change running workflow instances\n\nGet from subversion\n\nThe iTask system is actively developed, but not regularly released. You can get the development trunk from our subversion repository. You can get started with iTasks by following the following three steps:\n\n  1. Install a Clean 2.4 system for 32-bit Windows.\n  2. Check out the iTask trunk to a directory named 'iTasks-SDK' in your Clean 2.4 directory.\n  3. Follow the setup instructions in README.txt\n\n\nDocumentation of iTasks is still mostly done in scientific papers (look for iTasks on the Publications page). On this Wiki the following additional resources are available:\n\n\nThe iTask system works with most browsers. IE 9 is a known exception, we are working on that.\n\nUsing the iTask system with Firefox can be very slow. This is caused by a delay that is added by Firefox for all events for localhost. There are several solutions and workarounds:\n\n  1. type as url: about:config\n  2. search for v6, you will see network.dns.disableIPv6\n  3. toggle the value of this parameter to True.\n\nRead more",
        "topic_id":2,
        "format_id":7,
        "topic_confidence":0.9809758663,
        "format_confidence":0.6642042994
    }
]