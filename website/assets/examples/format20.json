[
    {
        "url":"https:\/\/github.com\/ConfluxHQ\/software-delivery-assessment",
        "text":"Skip to content\n\n\nSwitch branches\/tags\n\nLatest commit\n\n\nGit stats\n\n\nFailed to load latest commit information.\nLatest commit message\nCommit time\n\nMulti-team Software Delivery Assessment\n\nThe Multi-team Software Delivery Assessment is a simple, easy-to-execute approach to assessing software delivery across many different teams within an organisation. Devised by Matthew Skelton of Conflux, it is used as a key part of the Software Delivery Assessment at Conflux, but can be used freely by anyone (subject to the CC BY-SA license below).\n\nThe assessment uses and builds on the well-known and proven Spotify Squad Health Check model.\n\nTranslations: Japanese (ja \ud83c\uddef\ud83c\uddf5)\n\nThe assessment covers ten dimensions in total:\n\n  1. Team Health\n  2. Deployment\n  3. Flow\n  4. Continuous Delivery\n  5. Operability\n  6. Testing and Testability\n  7. Reliability and SRE\n  8. On-call\n  9. Security and Securability\n  10. Team Topologies team interactions\n\nThese ten dimensions cover key aspects of modern software delivery in a form that enables teams to self-assess their strengths and practices.\n\n\ud83d\ude80 Overview: see slides 32-38 in Continuous Delivery at scale\n\n\ud83c\udccf Card deck: make the assessment fun and interactive by using the Software Delivery Assessment printed card deck from Agile Stationery. Developed in collaboration with Conflux, the card deck has Tired and Inspired indicators for each of the assessment criteria, together with emoji cards for quick-fire voting from team members. The card deck works for remote assessments too!\n\nAssessment cards from Agile Stationery Five emoji voting cards\n\nCopyright \u00a9 2018-2021 Conflux Digital Ltd\n\nLicenced under CC BY-SA 4.0 CC BY-SA 4.0\n\n\nPurpose of the assessments\n\nThe aim of the assessments is to promote and sustain a positive working environment for building and running software systems where:\n\n  \u2022 Changes to software are built, tested, and deployed to Production rapidly and safely using Continuous Delivery practices\n  \u2022 Processes and practices are optimised for the flow of change towards Production\n  \u2022 Software is designed and built to enable independent, decoupled deployments for separate families of systems\n  \u2022 Software is designed and built in a way that addresses operability, testability, releasability, security, and reliability\n  \u2022 Problems in Production are always detected by teams before customers and users notice\n  \u2022 Responsibility and accountability for software changes lead to empowerment and ownership\n  \u2022 Working with software is rewarding and interesting\n  \u2022 Being on-call and supporting the software is sustainable and valuable\n  \u2022 People feel confident to challenge poor practices and approaches\n  \u2022 Teams have a clear mission and well-defined interaction patterns with other teams\n\nFundamentally, the assessments should help to unblock and enable teams so they can succeed. The assessments should help teams to improve how they build, test, and deploy software systems through identifying different kinds of improvements:\n\n  1. Team-focused improvements\n  2. Product-focused and Service-focused improvements\n  3. Organisation-wide improvements\n\nThe assessments should NOT be used to penalize teams, but to provide a shared drive towards improving practices and quality.\n\nTeams included in the assessments\n\nEvery team writing code, scripts, and\/or configuration for application software or infrastructure will benefit from being included in the assessments:\n\n  \u2022 Teams building user- and customer-facing websites and services\n  \u2022 Teams building internal services\n  \u2022 Teams building infrastructure to support other systems (including Platform teams)\n  \u2022 Teams building build & deployment tooling and scripts\n  \u2022 Teams configuring and testing COTS products as part of the software & infrastructure estate\n  \u2022 Any other teams with a primary focus on building, configuring, and testing software and infrastructure\n\nBy \"team\", we mean 6-10 person group that works together closely, usually called a\u00a0SquadScrum team, Product team, or Stream-aligned team\n\nAssessment criteria\n\nThe criteria for each dimension are taken from existing published books and online sources:\n\nHow to run the assessments\n\nThe assessment session itself should feel somewhat like a team retrospective session. The main difference from a normal retrospective session is that in the team assessment session the facilitator guides the discussion more firmly. There are many questions to discuss and it's important that the team discusses all of the criteria in the time available.\n\nAt the end of the assessment session, the team should feel encouraged and empowered to decide on what actions they want to take to improve their processes and practices based on the discussions.\n\n\nMany organisations find that running team assessments every 3 months provides a good result.\n\n\n  1. Find someone to Facilitate the assessment. This should be someone from outside the team, who is familiar with running team retrospectives.\u00a0\n  2. Book a time slot for 2 or 3 hours: for online sessions you could split this over 2 or more video call sessions, and for in-person sessions book\u00a0a room large enough for the team.\n  3. For online sessions, either use the card deck from Agile Stationery and screenshot or note down the results in a spreadsheet, or use an online poll tool to capture responses from people in the session. For in-person sessions, either use the card deck from Agile Stationery, or print the assessment sheets for each set of criteria, either using the ready-made A1 PDF (see Releases), or the individual assessment pages at A1 size if possible (use small margins):\n  4. For online sessions, show the Tired and Inspired criteria on the screen alongside the question. For in-person sessions, either print the details pages\u00a0as a guide or have the pages open on-screen to understand the context and details of each of the assessment criteria:\n    1. Team Health\n    2. Deployment\n    3. Flow\n    4. Continuous Delivery\n    5. Operability\n    6. Testing and Testability\n    7. Reliability and SRE\n    8. On-call\n    9. Security and Securability\n    10. Team Topologies team interactions\n  5. It is valuable to capture details and nuances of the discussions around each question. For online sessions, have someone take notes in a document or shared whiteboard. For in-person sessions, bring lots of marker pens or whiteboard markers: red, blue, and green are best.\n  6. Include\u00a0someone who is familiar with facilitating retrospectives\u00a0(possibly a scrum master) in the session. They will be shadowing the facilitator during the session so the person from your team can facilitate other assessment sessions later.\n\nMake sure that the Facilitator understands the purpose of the session and is familiar with the assessment pages and questions.\n\n\nThe facilitator should familiarise themselves with the Spotify Squad Health Check approach before running the session. See How I Used the Spotify Squad Health Check Model for a good experience report, Squad Health Checks from SkyBet, and download instructions from Spotify (PDF).\n\nDuring the assessment:\n\n  \u2022 Keep the team on schedule by asking for some discussions to be held outside the session\n  \u2022 Write down the team scores and notes on the printed assessment sheets or ensure that scores and notes are captured in a digital tool.\n  \u2022 Take photographs of the completed assessment sheets for in-person sessions.\n  \u2022 Get feedback from the team on the VALUE and EXECUTION of the engineering assessment - smiley faces are sufficient!\n\n\nEach team assessment runs for around 3-4 hours, and the facilitator will run the team\u00a0through 10 sets of questions:\n\n  1. Team health check -\u00a035 mins\n  2. Deployment health check -\u00a010 mins\n  3. Flow check -\u00a010 mins\n  4. Continuous Delivery check -\u00a020 mins\n  5. Operability check -\u00a020 mins\n  6. Test coverage check -\u00a020 mins\n  7. Reliability and SRE - 30 mins\n  8. On-call - 15 mins\n  9. Security check - 30 mins\n  10. Team Topologies check - 20 mins\n\nThese timings leave space for two 10 minute breaks\u00a0during the assessment.\n\nRunning the Assessment session\n\nEach section has several questions. Each question should be answered as follows:\n\n  \u2022 The team (either as individuals or as a team) rate each of the criteria using\u00a0SAD (1 OR 2)\u00a0\/\u00a0MEH (3)\u00a0\/\u00a0YAY (4 OR 5)\u00a0based on the\u00a0Tired\u00a0and\u00a0Inspired\u00a0guidelines\n\n    \u2022 Tired\u00a0aligns to a low rating (1), and\u00a0Inspired\u00a0aligns to a high rating (5)\n\n    \u2022 If you used individual ratings, tally the ratings and\/or decide on a single team score from 1 to 5. You may find it useful to use different coloured pens on the printed sheet to indicate visually the different ratings.\n\n  \u2022 The\u00a0Trend\u00a0since the previous time is identified (going up, staying roughly the same, going down), if applicable\n\n  \u2022 An\u00a0Action\u00a0agreed to improve the score for that question over the coming months.\n\n  \u2022 Use the\u00a0Notes\u00a0column to indicate further information that you think is valuable for the coordinating team to know about.\n\n  \u2022 Make sure to complete to the\u00a0Date\/Name\/Facilitator\u00a0details\n\n  \u2022 For in-person sessions, take a photo of each completed sheet and send to\u00a0the person coordinating the assessments\n\n  \u2022 \u00a0Get team members to rate the assessment session itself in terms of: Value, Execution (sad, meh, happy faces)\n\nViral facilitation\n\nEach team assessment session has present one person who is shadowing the facilitator so that they can themselves facilitate future sessions. Each new facilitator should facilitate at least 2 sessions with other teams. In this way, the number of facilitators expands rapidly, enabling a minimal burden on the initial facilitators.\n\nCoordinating and interpreting the results\n\nAfter teams have each run an assessment session and sent their results, the coordinating group should collate the results from different teams to identify any areas that need improving across the organisation. Ask questions such as:\n\n  \u2022 Why does team ABC rate themselves as 1 for test coverage? What is hindering them?\n  \u2022 What can we do as an organisation to help... (truncated)",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9756202102,
        "format_confidence":0.9293102026,
        "weight":0.077275185
    },
    {
        "url":"http:\/\/svn.pan-starrs.ifa.hawaii.edu\/trac\/ipp\/wiki\/DetectorLinearity",
        "text":"Previous version of this page can now be found here: DetectorLinearityArchive\n\nDetector Linearity\n\nThe final linearity correction is actually more of a flux dependent bias level. The method used to generate the correction data was:\n\n  1. Run ppImage on input flat images with nothing but the OVERSCAN correction enabled.\n  2. Measure cell and edge row\/column median values.\n  3. Foreach chip\/cell\/region\/observation date, find the best fit trend f_region = gain * t_exp + bias_0 over a range of exposure times 3 < t_exp < 12.\n  4. Define the expected model value as MODEL = gain * t_exp and correction CORR = f_region - (gain * t_exp). This ensures that the bias offset bias_0 is included in the correction.\n  5. Foreach exptime\/chip\/cell\/region, take the median of all correction values and fluxes, and write the table of (flux,correction) pairs.\n  6. Apply the correction by linear interpolation of the pairs for the appropriate chip\/cell\/region.\n\nI have attached two images that show the original overscan corrected image and the linearity corrected version of this image. These two images are from 2009-12-30 and 2010-11-23, showing that the correction is stable.",
        "topic_id":19,
        "format_id":20,
        "topic_confidence":0.9035362601,
        "format_confidence":0.9058995247,
        "weight":0.0092053485
    },
    {
        "url":"http:\/\/www.docstoc.com\/docs\/87226350\/Risk-Assessment-Templates-Firing-Range---DOC",
        "text":"Risk Assessment Templates Firing Range - DOC by wpc49900\n\nVIEWS: 383 PAGES: 10\n\nMore Info\n\t\t\t\t\t\t\t\t\t                                                         OPEN RANGE                                                  MOD Form 907B-5\n                                                                                                                    (Revised Mar 2010)\n\n                                                     Tick this box if completing\n                              COVER SHEET             Cover Sheet Only\n    Range Serial No:                                      Range Name :                                          Inspection Date:\n\nComplex Ser No (if any):                         Range Complex Name (if any):                                   Current Status:\n                                                                                                                  See Note 4\n                                                          Type of Range:\nRange Authorising Headquarters:                                      RFCA\/DTE:\n\nEmail:                                                               Email:\nRange Address:                                                       Range Administering Unit Address:\n\nEmail:                                                               Email:\n\nMap Sheet:                                                           Grid Reference:\n\nAir Danger Area:                         Sea Danger Area:                          Bylaws (SI Number and Date):\n\nMain users of the range:\n\nBrief description of the range:\n\nRange Officer: (The officer appointed by the CO of the Range Administering Unit for the overall management of the range.)\nName and Rank:                                                       Appointment:\n\nCivilian Telephone Number:        Military Telephone Number:         E-Mail Address:\n\nThis report comprises:\nCover Sheet.\nSection 1 Range Documentation.\nSection 2 Range Safety Questionnaire.\nSection 3 Inspector's Comments and Recommendations.\nSection 4 Follow-up action ordered by the Range Authorising Headquarters. (Not applicable to RN)\nSection 5 DLRSC comments and recommendations.\n1. Inspectors are to complete Sections 1, 2 and 3 then distribute them as detailed in Section 3.\n2. Service Inspection Coordinators are to liaise with the relevant Range Authorising Headquarters to ensure that Section 4 is completed.\n3. Range Safety Gradings:\n   Grade A: The range meets all the criteria specified in JSP 403.\n   Grade B: The range does not fully meet the criteria specified in JSP 403 but its continued use does not present an unacceptable\n   hazard provided remedial action is taken as soon as is reasonably practicable.\n   Grade C: The range does not meet the essential safety criteria specified in JSP 403 to the extent that continued use presents an\n   unacceptable hazard. Before further use, the identified failings should be rectified and\/or control measures put in place; or a\n   dispensation should be granted.\n4. Current Status:\n   a) Closed: The range has been decommissioned, all documentation has been permanently withdrawn by the Range Authorising\n   Headquarters and Annex C to Chapter 6 of JSP 403 Vol I has been completed. No inspection required.\n   b) Temporarily Closed: MOD Form 905 and 906, and\/or other range documentation have been temporarily withdrawn by the\n   Range Authorising Headquarters or Range Administering Unit. Situation should be reported to DLRSC but no inspection is required.\n   c) Active: Where the range is in normal use; or when neither a. nor b. above is applicable, the Inspector should consider the range\n   to be active and a full range safety inspection and report should be completed.\n\n                                                             Page 1 of 10                                          OPEN RANGE\n                                                                                                                            SECTION 1\n     Range Serial No:                                     Range Name (if any):                                       Inspection Date:\n\n                                                     SECTION 1\n                                               RANGE DOCUMENTATION\n1.      Enter the following information pertaining to the documents listed below:\n\n        a.    MOD Form 904:        Reference No:                                                Date of Signature:\n\n        b.    Type of WDA in use:\n        c.    If the range operates under dispensation\/s for non-standard construction and\/or\n              practices, summarise the circumstances and enter the dates of validity below:            If none, indicate here:\n\nGranted by:                                                                           Rank\/Grade:\n\nAppointment:                                                                    Valid from:                     to:\n                                                                                                                              Y         N\nHas the above dispensation been reviewed in past 12 months?\n\n        d.    AF K1309\/MOD Form 1057 or other technical records: N\/A\n        Reference                                                                                         Approval\n        No:                                                                                                 Dated:\n        e.    MOD Form 905:       Date of expiry:\n                                                                                                                          All actions\n2.      Most Recent Inspections            Date:             Inspected by:                                            N\/A     Y         N\n        a.    MOD Form 907A\n        b.    MOD Form 907B\n              Works Technical\n        d.    Estate Inspection\n\n  (See Note 3 on Cover Sheet)\n\n                                                              Page 2 of 10                                           OPEN RANGE\n                                                                                                          SECTION 2\n     Range Serial No:                            Range Name (if any):                              Inspection Date:\n\n                                         SECTION 2\nRANGE ADMINISTRATION                                                                                N\/A     Y     N\n1.      Range Orders:\n        a.   Are Range Orders promulgated in the Orders of the Range Administering Unit?\n        b.   Have Range Orders been reviewed during the last 12 months?\n        c.   Are Range Orders available to the RCO on the range?\n2.      Do Range Orders clearly describe or include:\n        a.   The name and location of the range?\n        b.   The Unit and appointment of the officer responsible to the CO of the Range\n             Administering Unit for the overall management of the range?\n        c.   That firing is only to be conducted under the supervision of properly qualified or\n             authorised RCOs?\n        d.   That the Air Weapons Range is only to be operated by properly qualified\n        e.   The weapons and ammunition limitations applicable to the range, in accordance\n             with the schedule of the Range Authorisation Certificate (MOD Form 904)?\n        f.   That the RCO is to sign the MOD Form 906\/906A and\/or other special to Arms\n             Certificate before any firing practice to affirm that he has read and understood\n             Range Standing Orders?\n        g.   The requirement for the RCO to sign in the Range Log (MOD Form 906) after\n             firing to confirm that all ammunition and explosives have been cleared?\n        h.   The documents and maps required by Safety personnel?\n        i.   The Certificates to be submitted to the Range Officer prior to obtaining\n             clearance to fire, and at the end of practice?\n        j    The procedure for reporting accidents, incidents and defects, including blinds\n             and the entries to be made in the Range Log (MOD Form 906\/906A)\n        k    The medical cover and equipment to be provided?\n        l.   The location of the nearest means of communication by which emergency\n             services may be called?\n        m.   The actions to be taken in an emergency?\n        n.   That ear protection is to be worn in accordance with current regulations?\n        o    That other protective clothing is to be worn?\n        p.   Specific instructions for each authorised type of practice (e.g. NGS, Air to\n             Ground, Artillery, Proof, Demolitions etc.)?\n        q.   That Field Firing exercises, Battle Runs etc, may only be conducted with the\n             written authority of the Range Officer\/RLO?\n        r.   The procedures for firing from air-mounted weapon platforms?\n        s.   That all misfires and blinds are to be dealt with by the firing unit or reported to\n             Range Control at the end of practice; including the entries to be made in the\n             MOD Form 906\/906A and any other special to arm certificate required by the\n        t.   The procedure to ensure that before firing\/practice begins, the danger area into\n             which any weapon is directed is clear of personnel?\n        u.   That, before firing begins, the officer\/s responsible for range safety ensures that\n             all sentries, barriers, warning flags and\/or lights relevant to his practice are in\n        v.   RCO's verbal pre-firing briefings to firing party\/s, sentries and other personnel\n             affected by the firing, on the intended practice and any relevant safety\n\n                                                     Page 3 of 10                                  OPEN RANGE\n                                                                                                          SECTION 2\n     Range Serial No:                            Range Name (if any):                              Inspection Date:\n\n                  ... (truncated)",
        "topic_id":3,
        "format_id":20,
        "topic_confidence":0.7175825238,
        "format_confidence":0.8407893181,
        "weight":0.0006477574
    },
    {
        "url":"https:\/\/msdn.microsoft.com\/en-us\/library\/h55she1h.aspx",
        "text":"Assembly.ReflectionOnlyLoad Method (Byte[])\n\n\nThe .NET API Reference documentation has a new home. Visit the .NET API Browser on to see the new experience.\n\nLoads the assembly from a common object file format (COFF)-based image containing an emitted assembly. The assembly is loaded into the reflection-only context of the caller's application domain.\n\nNamespace: \u00a0 System.Reflection\nAssembly: \u00a0mscorlib (in mscorlib.dll)\n\npublic static Assembly ReflectionOnlyLoad(\n\tbyte[] rawAssembly\n\n\nType: System.Byte[]\n\nA byte array that is a COFF-based image containing an emitted assembly.\n\nReturn Value\n\nType: System.Reflection.Assembly\n\nThe loaded assembly.\n\nException Condition\n\nrawAssembly is null.\n\n\nrawAssembly is not a valid assembly.\n\n\nVersion 2.0 or later of the common language runtime is currently loaded and rawAssembly was compiled with a later version.\n\n\nrawAssembly cannot be loaded.\n\nYou cannot execute code from an assembly loaded into the reflection-only context. To execute code, the assembly must be loaded into the execution context as well, using the Load method.\n\nThe reflection-only context is no different from other contexts. Assemblies that are loaded into the context can be unloaded only by unloading the application domain.\n\n.NET Framework\nAvailable since 2.0\nReturn to top",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.9871003628,
        "format_confidence":0.9829006791,
        "weight":0.0817314086
    },
    {
        "url":"https:\/\/china.xilinx.com\/support\/answers\/54363.html",
        "text":"\n\nAR# 54363\n\nRelease Notes and Known Issues for LogiCORE IP MII to RMII for Vivado 2013.1 and newer tool versions\n\n\nThis answer record contains the Release Notes and Known Issues for the MII to RMII IP Core and includes the following:\n\n  \u2022 General Information\n  \u2022 Known and Resolved Issues\n  \u2022 Revision History\n\nThis Release Notes and Known Issues Answer Record is for the core generated in Vivado 2013.1 and newer tool versions.\n\nLogiCORE MII to RMII Core IP Page:\n\n\n\nGeneral Information\n\nSupported Devices can be found in the following three locations:\n\nFor a list of new features and added device support for all versions, see the Change Log file available with the core in Vivado.\n\nVersion Table\n\nThis table correlates the core version to the first Vivado design tools release version in which it was included.\n\nCore VersionVivado Tools Version\nv2.0 (Rev.11)2016.2\nv2.0 (Rev. 10)2016.1\nv2.0 (Rev. 9)2015.4\nv2.0 (Rev. 8)2015.3\nv2.0 (Rev. 7)2015.1\nv2.0 (Rev. 6)2014.3\nv2.0 (Rev. 5)2014.2\nv2.0 (Rev. 4)2014.1\nv2.0 (Rev. 3)2013.4\nv2.0 (Rev. 1)2013.2\n\nGeneral Guidance\n\nThe table below provides Answer Records for general guidance when using the LogiCORE MII to RMII IP Core.\n\nAnswer RecordTitle\n(Xilinx Answer 55248)Vivado Timing and IP Constraints\n\nKnown and Resolved Issues\n\nThe following table provides known issues for the MII to RMII IP core, starting with v2.0, initially released in the Vivado 2013.1 tool.\n\nNote: The \"Version Found\" column lists the version the problem was first discovered.\u00a0\n\nThe problem might also exist in earlier versions, but no specific testing has been performed to verify earlier versions.\n\nPlease also refer to the Change Log for details.\n\nAnswer RecordTitleVersion FoundVersion Resolved\n(Xilinx Answer 66942)MII to RMII v2.0 - Design fails to meet timing with hold violation for MII to RMII corev2.02016.1\n\nRevision History\n\n04\/03\/2013Initial release\nAR# 54363\n\u65e5\u671f 06\/08\/2016\n\u72b6\u6001 Active\nType \u7248\u672c\u8bf4\u660e\n  \u2022 Vivado Design Suite - 2013.1\n  \u2022 Reduced Media Independent Interface",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.6704647541,
        "format_confidence":0.7516396642,
        "weight":0.0625012982
    },
    {
        "url":"https:\/\/docs.bitnami.com\/installer\/apps\/opencart\/get-started\/understand-default-config\/",
        "text":"Understand the default MySQL configuration\n\nBefore running the commands shown on this page, you should load the Bitnami stack environment by executing the installdir\/use_APPNAME script (Linux and Mac OS X) or by clicking the shortcut in the Start Menu under \u201cStart -> Bitnami APPNAME Stack -> Application console\u201d (Windows). Learn more.\n\nThe grant tables define the initial MySQL user accounts and their access privileges. The default configuration consists of:\n\n  \u2022 A privileged account with a username of root. The root user has remote access to the database.\n  \u2022 An anonymous user without remote access to the database server. This user can only connect from the local machine and it is only intended for testing.\n  \u2022 A test database only intended for testing.\n\nCheck our recommendations for a production server.\n\nMySQL version\n\nIn order to see which MySQL version your system is running, execute the following command:\n\n$ mysqld --version\n\nMySQL configuration file\n\nThe MySQL configuration file is located at installdir\/mysql\/my.cnf. Some configuration overrides are stored in installdir\/mysql\/bitnami\/my.cnf.\n\nThe MySQL official documentation has more details about how to configure the MySQL database.\n\nMySQL socket\n\nOn Unix, MySQL clients can connect to the server in the local machine using an Unix socket file at installdir\/mysql\/tmp\/mysql.sock.\n\nMySQL port\n\nThe default port for MySQL is 3306.\n\nMySQL Process Identification Number\n\nThe MySQL .pid file allows other programs to find out the PID (Process Identification Number) of a running script. Find it at installdir\/mysql\/data\/\n\nMySQL error log\n\nThe log-error file contains information indicating when mysqld was started and stopped and also any critical errors that occur while the server is running. If mysqld notices a table that needs to be automatically checked or repaired, it writes a message to the error log.\n\nFind it at installdir\/mysql\/data\/mysqld.log.",
        "topic_id":2,
        "format_id":20,
        "topic_confidence":0.5806062222,
        "format_confidence":0.9450239539,
        "weight":0.0785818349
    },
    {
        "url":"https:\/\/www.chinesestandard.net\/PDF\/English.aspx\/WS392-2012",
        "text":"\nPowered by Google-Search & Google-Books www.ChineseStandard.net Database: 189760 (6 Aug 2022)\n\nWS 392-2012\n\nWS 392-2012_English: PDF (WS392-2012)\nStandard IDContents [version]USDSTEP2[PDF] delivered inStandard Title (Description)Related StandardStatusPDF\nWS 392-2012English459 Add to Cart 4 days Clinical application of ventilator WS 392-2012 Valid WS 392-2012\n\nStandard ID WS 392-2012 (WS392-2012)\nDescription (Translated English) Clinical application of ventilator\nSector \/ Industry Health Industry Standard\nClassification of Chinese Standard C05\nClassification of International Standard 11.020\nWord Count Estimation 20,274\nQuoted Standard GB 9706.28; WS 310\nDrafting Organization Huazhong University of Science and Technology, Union Hospital, Tongji Medical College,\nAdministrative Organization ?PRC Ministry of Health\nRegulation (derived from) ?Health-Communication (2012) 15; Industry Standard Filing Announcement 2012 No.10 (Total No.154)\nSummary This standard specifies the use of personnel and unit ventilator basic requirements. Clinical application process, monitoring indicators. Scope ventilator, the ventilator classification and methods of use. Nursing principles. Ventilator during treatment s\n\nWS 392-2012\nClinical application of ventilator\nICS 11.020\nC 05\nPeople's Republic of China Health Industry Standard\nClinical application of ventilator\n2012-09-04 released\n2013-04-01 Implementation\nPublished by the Ministry of Health of the People's Republic of China\n1 Scope\nThis standard specifies the basic requirements of the ventilator users and units, the clinical application process, monitoring indicators, the scope of application of the ventilator, and the classification of the ventilator.\nAnd methods of use, nursing principles, application specifications of sedatives, analgesics, and muscle relaxants during ventilator treatment, and ventilator-related complications.\nThis standard is applicable to the clinical application of ventilators by medical personnel at various medical institutions at various levels throughout the country.\n2 Normative references\nThe following documents are essential for the application of this document. For dated references, only the dated version applies to this document.\nFor undated references, the latest version (including all amendments) applies to this document.\nGB 9706.2 8 Medical electrical equipment Part 2. Particular requirements for ventilator safety. Therapeutic ventilator\nWS 3 1 0 Hospital Disinfection Supply Center\n3 terms and definitions\nThe following terms and definitions apply to this document.\nLife support device capable of improving respiratory function, reducing respiratory energy consumption and saving heart reserve capacity.\nNoninvasive positive pressure ventilation\nConnect the patient with the mechanical ventilation mode by nasal or face mask.\nArtificial airway\nTemporary gas passages created by catheters through nasal, oral or tracheotomy 3\nVolume-controlled ventilation, VCV\nThe ventilator achieves ventilation with a preset ventilation volume.\nPressure-controlled ventilation, PCV\nThe ventilator achieves ventilation with a preset airway pressure.\nControlled ventilation; CV\nThe ventilator completely replaces the patient's spontaneous breathing, providing all the work of breathing.\nAssisted ventilation\nRely on the patient's inhalation effort to trigger the ventilator's inspiratory valve to achieve communication\nRespiratory rale\nNumber of breaths per minute.\nBreathing ratio, I. E\nThe ratio of inspiratory time to expiratory time.\nInspiratory flow rate\nGas volume passing through a certain point per unit time.\nTrigger sensitivity\nCan trigger ventilator-assisted ventilation, changes in pressure or flow rate in the tube caused by patient's spontaneous breathing.\nPositive end-expiratory pressure; PEEP\nA ventilation-assisted method to make the airway pressure higher than atmospheric pressure by artificial measures (such as a device that restricts air flow at the exhalation end) during the expiratory phase.\n4 Basic requirements for ventilator users\nPractitioners or registered nurses who meet the following requirements are eligible to use a ventilator.\na) Grasp the pathophysiology of respiratory system, respiratory physiology, and respiratory failure;\nb) Grasp the working principle, performance characteristics and commonly used mechanical ventilation modes and parameter settings of the breathing scabs used;\nc) Grasp the clinical significance and determination methods of commonly used breathing and circulation monitoring indicators;\nd) master the routine maintenance and disinfection methods of the ventilator used;\nc) Can judge the working status of the ventilator used and deal with it accordingly.\n5 Basic requirements for units using ventilator\nThe unit using the ventilator should have the following conditions.\na) Have personnel who meet the requirements of Chapter 4\nb) monitoring equipment with vital signs;\nc) Conditions for monitoring commonly used respiratory indicators;\nd) have oxygen source and sputum suction equipment;\nc) have rescue equipment and personnel;\nf) Have basic maintenance and disinfection capabilities of the ventilator;",
        "topic_id":12,
        "format_id":20,
        "topic_confidence":0.9860357046,
        "format_confidence":0.6799244285,
        "weight":0.0014958751
    },
    {
        "url":"https:\/\/www.readnaturally.com\/research\/technical-adequacy",
        "text":"Technical Adequacy of Assessments\n\nTechnical Adequacy of the Progress Monitoring Passages\n\nThe following information provides evidence for the validity of the progress monitoring passages in Read Naturally Progress Monitor, based on the correlation between the monitoring passages and the benchmark passages in Read Naturally\u2019s \u00a0Reading Fluency Benchmark Assessor and Benchmark Assessor Live.\n\npointer\u00a0Learn more about Reading Fluency Progress Monitor\u00a0(Read Naturally's progress monitoring tool)\n\n\nOver the course of three school years (2002-2003, 2003-2004, and 2004-2005), Read Naturally field tested dozens of reading passages at each grade level for the purpose of continuous progress monitoring. We sent passages to classrooms throughout the country and piloted with a diversity of students. We tested students on these passages as well as on grade level benchmark passages from the Reading Fluency Benchmark Assessor and compared students' words correct per minute (WCPM) on the monitoring passages with their WCPM on the benchmark passages. Some monitoring passages were too hard for a particular grade level and some were too easy. We modified these passages and\/or tested them in a different grade. Ultimately, each passage met the benchmark passage in difficulty or we discarded it. We calibrated a total of 30 passages per grade level. The table below summarizes the results of these analyses.\n\nCorrelation and Difficulty Data\n\nThe following table lists correlation data about the progress monitoring passages compared to each other and to the grade level benchmarks. The correlation between the benchmark and monitoring passages is a measure of validity. Correlations range from -1.0 to +1.0. High correlations (.8 or higher) between the monitoring passage and the benchmark passage means that the two passages are measuring the same skill (reading fluency) with a high degree of consistency. Monitoring passages with high correlations to the benchmark and to other monitoring passages have high reliability and validity.\n\nThe table also lists data on the difficulty of the monitoring passages compared to the benchmarks. A difficulty of -2.1, for example, would indicate that the median WCPM of all students tested on the monitoring passage was 2.1 words less than their median WCPM on the benchmark passage. If the median WCPM for students on the benchmark was 50.5 and the median WCPM for students on the monitoring passage was 48.4, the difficulty on that passage would be 48.4 minus 50.5, which is -2.1.\n\nGrade (30 passages per grade) Median Correlation with Benchmark Benchmark Correlation Range Median Correlation with Other Passages Other Passage Correlation Range Median Difficulty (words correct per minute difference Range of Difficulty\n1 .96 .91 to .99 .96 .92 to .99 -2.0 -6.0 to 4.0\n2 .95 .90 to .99 .96 .92 to .99 -1.5 -5.9 to 4.1\n3 .94 .89 to .99 .94 .90 to .99 -0.5 -5.5 to 3.8\n4 .94 .92 to .98 .94 .90 to .97 1.3 -4.9 to 5.3\n5 .93 .81 to .98 .93 .79 to .96 0.3 -5.1 to 5.3\n6 .94 .89 to .97 .95 .85 to .97 -0.7 -5.6 to 6.0\n7 .94 .87 to .97 .95 .88 to .97 -1.2 -5.7 to 5.8\n8 .95 .88 to .98 .95 .91 to .97 -0.7 -5.5 to 5.2\n\nPlease let us know what questions you have so we can assist. For Technical Support, please call us or submit a software support request.\n\nClick to refresh image",
        "topic_id":4,
        "format_id":20,
        "topic_confidence":0.9726179242,
        "format_confidence":0.7229132652,
        "weight":0.001216566
    }
]