[
    {
        "url":"http:\/\/chandnisingh.blogspot.ca\/2017\/06\/thoughts-on-two-new-papers-on.html",
        "text":"Sunday, 25 June 2017\n\nThoughts on two new papers from vulnerability and adaptation research\n\nI read two very interesting papers from adaptation and vulnerability research last week.\n\nIn\u00a0Operationalizing longitudinal approaches to climate change vulnerability assessment, Fawcett et al. (2017) make a case for longitudinal methodological approaches when studying vulnerability and adaptation. The lack of attention paid to temporality has been a long-held peeve of mine (it's gotten so bad that in team meetings, colleagues crack jokes about it). Fawcett et al. use three illustrative cases from Arctic communities to highlight how longitudinal approaches, two in particular \u2014\u00a0cohort studies (following a group of individuals over time) and trend studies (repeated data collected at a community level to reveal patterns of change) \u2014 can strengthen the methodological toolbox of vulnerability research.\n\nI particularly liked how they tease out the benefits of using a longitudinal approach. It helps\n  1. build a more nuanced understanding of adaptation processes and 'causal chains' of vulnerability (something Jesse Ribot has written a lot about, see his work from 1995, 2010, and 2014),\u00a0\n  2. construct a more robust portrayal of what has worked and hasn't, and why, which is crucial for anyone interesting in strengthening, investing in, and implementing adaptation, and\u00a0\n  3. diagnose maladaptive behaviour by looking into past pathways. This last approach is similar to what colleagues and I have taken in a recent project where we use cases from rural and urban India to argue that tracing historical trajectories of development and adaptation actions can help understand how development choices can narrow adaptation option spaces, often leading to potential maladaptation.\nComing back to Fawcett et al. (2017), I thoroughly enjoyed reading it and wholly support their key argument of expanding vulnerability research to involve more longitudinal approaches that begin to capture temporality. While the authors discuss the challenges in doing such research (notably, potential attrition of respondents from original cohorts, the need for sustained funding), I have two questions probing the practice of longitudinal vulnerability assessment approaches:\n  1. The political economy of vulnerability\u00a0assessments: Currently, many NGOs, donors, and governments conduct VAs. How do the authors imagine such actors to undertake longitudinal studies given the budgets, project cycle-time frames, and capacity constraints they work under? This is especially true in government departments in India (we did a review of 120 VAs in India\u00a0found 35% use static, indicator-based approaches). Moreover, in many cases, governments undertake VAs under tight deadlines (for e.g. assessments after a disaster event to inform humanitarian action). The underlying question is how can we build processes and demand for longitudinal vulnerability assessment approaches that feed into shorter-term cycles but also contribute to the larger narratives of vulnerability?\u00a0\n  2. The place of researchers:\u00a0Researchers are a possible 'actor group' that don't face all the challenges noted above, or at least not to a similar extent. However, researchers are increasingly being squeezed into shorter project cycles, insecure employment arrangements, tighter funding structures, and greater calls for impact \u2014\u00a0all of which, may not necessarily be conducive to longitudinal research \u00a0design which needs 1) sustained financial backing, 2) strong, clear and continual leadership, 3) an underlying recognition of the importance of such work, 4) acceptance of delayed gratification.\n\nIn\u00a0Adaptive capacity: exploring the research frontier, Mortreux and Barnett (2017) discuss the emphasis that adaptation research has on quantified assessments of adaptive capacity (predominantly through Sustainable Livelihoods Framework-based (SLF) approaches) without an equal emphasis on how and to what extent adaptive capacity translates in adaptation outcomes. They highlight valuable gaps in adaptation research, most notably, the lack of focus on the process of adaptive capacity (a potential to adapt) being translated into an adaptation outcome (with concrete implications for peoples' vulnerability). The paper reviews emerging literature on risk perception and adaptation decision-making, cognitive barriers to adaptive behaviour, and place attachment, to discuss how these gaps can be addressed.\n\nTo add to their review of literature from disaster risk management and behavioural sciences, I wanted to highlight a few empirical studies exploring the drivers of adaptation behaviour and adaptation outcomes:\n  1. Burnham and Ma (2017) examine farmer adaptation decisions in Loess Plateau, China and find that self efficacy, i.e. one's perception of one's own efficacy to adapt, shapes adaptation behaviour and outcomes significantly. They crucially highlight that in addition to household assets and entitlements, state-society dependencies may reduce farmer perceived self-efficacy. \u00a0\n  2. Drawing on data examining household and intra-household risk perceptions and decisions in rainfed farming families in north-west India, I have argued that different households perceive risk differently and this shapes the adaptation pathways they take. Moreover, I found that adaptation outcomes (measured through environmental, social and economic lenses) change over time based on changing household assets but also changing social structures, policy regimes, and cultural beliefs.\n  3. In a completely different context, Evans et al. (2016) examine social limits to adaptation int he Great Barrier Reef region.They argue that social limits affect adaptation outcomes by dissuading people to take up adaptive action in the first place! Also interactions between psycho-social (or what I call socio-cognitive) and structural factors can adaptation ineffective.\u00a0\nConceptually, it's exciting times for adaptation research. Practically, I feel that while there is a growing body of work around understanding the need for longitudinal vulnerability assessments and factoring in socio-cognitive barriers to adaptation, it hasn't begun to filter into mainstream adaptation implementation and negligibly in policy circles (especially in India, the context I am most familiar with).\u00a0\n\nNo comments:\n\nPost a Comment\n\nRelated Posts Plugin for WordPress, Blogger...",
        "topic_id":19,
        "format_id":15,
        "topic_confidence":0.9903951287,
        "format_confidence":0.9788410664,
        "weight":0.0174281937
    },
    {
        "url":"http:\/\/zenpundit.com\/?p=2675",
        "text":"zenpundit.com \u00bb Blog Archive \u00bb Taking Aim at the Black Swan\n\nTaking Aim at the Black Swan\n\nShane Deichman reviews Nassim Nicholas Taleb\u2019s\u00a0The Black Swan: The Impact of the Highly Improbable\u00a0and finds it wanting:\n\nPerhaps it\u2019s my na\u00efvet\u00e9 (or perhaps that I\u2019m a product of the California public school system), but I honestly don\u2019t see our civilization marching toward \u201cExtremistan\u201d. Quite the opposite: While our awareness of remote events has increased, and our networks have grown exponentially, I believe that the diffuse topology of our networks actually dampens the impact of an extreme event. Consider the \u201cButterfly Effect\u201d. Do you really think a butterfly flapping its wings in Jakarta is going to eventually cause a hurricane in New York City? Or do you think the minor perturbation is absorbed locally without cascading into some kind of resonance? Yes, there are examples that illustrate the dire consequences of unplanned resonance. Taleb (who waffles at the end of his book as half hyperskeptic, half intransigently certain) abandons the Gaussian bell curve, yet \u2014 with only a single mention of Albert-L\u00e1szl\u00f3 Barab\u00e1si \u2014 firmly embraces Power Law scale invariance as normative.Despite Taleb\u2019s too-casual treatment of scale, I think he would agree with George E.P. Box\u2019s statement (c. 1987) that \u201c\u2026[A]ll models are wrong, but some are useful.\u201d Abandoning our dogmatic devotion to certainty is essential in any creative, innovative enterprise \u2014 and can reveal hidden opportunities, and hidden abilities.\n\nRead the whole thing here.\n\nUnlike most reviewers, Shane could go head-to-head with Taleb on things mathematical ( though you hardly need a math background to understand The Black Swan) and Shane is right that networks that are intrinsically and generally resilient are better suited to enduring unexpected, system perturbing, black swans.\n\nHope to have my review up Sunday evening.\n\n7 Responses to \u201cTaking Aim at the Black Swan\u201d\n\n  1. Fabius Maximus Says:\n\n    Nice to see someone with strong math expertise dispute the \"fragile networks\" meme that has become so widespread, with so little supporting analysis or data.\u00a0 I wonder if this pessimism about \"systems\" results from the combination of our dependence on them AND our lack of understanding how they work.\n\n    An engineer might understand our utility systems, an MBA the finanical systems, a geologist the extrative sytems \u2014 but to each of us\u00a0there are aspects that are unknown and disturbingly vulnerable.\n\n    I was at a community meeting in Fall 1999, organized to discuss Y2k.\u00a0 They were considering wind towers, water storage, etc.\u00a0 I asked each of them what preparations had been made at their place of work.\u00a0 Each said that their own piece of the system would run after Y2k \u2014 the traffic lights were fine, the hospital were fine, etc.\u00a0 Each person worried that the others were unprepared.\n\n  2. zen Says:\n\n    Hi FM\n    \"Fragility\" always depends on the direction the stress is coming from. A pillar is immensely resilient to vertical stress; horizontal stress\u2026.not so much.\n\n  3. deichmans Says:\n\n    Thx for the link, Zen!\u00a0 Looking forward to your review this weekend.\n\n    FM: We\u2019re a cynical bunch, aren\u2019t we? \ud83d\ude42\u00a0 Your story reminded me of the best Y2K practical joke I\u2019ve heard.\u00a0 A friend was at a 12\/31\/1999 NYEve party in a rural neighborhood.\u00a0 The host\u2019s son said (shortly after 11pm) that he wasn\u2019t feeling well and left the room; he later snuck into the basement and, at exactly Midnight, flipped the master circuit breaker for the home\u2019s electrical supply.\u00a0 What\u2019s funniest about the story is that nobody was surprised by the abrupt loss of electricity!\u00a0 So I guess we\u2019re gullible as well as cynical\u2026.\n\n  4. TDL Says:\n\n    \u00a0 I wouldn\u2019t take the MBA example of understanding, anything, too far!\n\n    MBA 2006\n\n  5. The Lounsbury Says:\n\n    I was not particularly moved by his criticisms as such.\n\n  6. Larry Says:\n\n    \"\"Fragility\" always depends on the direction the stress is coming from. A pillar is immensely resilient to vertical stress; horizontal stress\u2026.not so much\"\n    The force inside a pillar is the same in all directions. The difference is that the outside force is in shear vertically, and in is in a bending force horizontally, if applied to the center. Hard to shear a pillar vertically, but relatively easy to bend it horizontally. If the pillar is made out of something like stone it brakes when bent. The conservative\u00a0republican, now days,\u00a0hopes the pillar is made out of something resilient so the totalitarian force applied\u00a0horizontally will bend the pillar instead of\u00a0braking it.\u00a0Considering the totalitarian force has nukes and millions to lose, not a bad hope, I suppose.\n\n  7. Granite Tiles&nbsp; Says:\n\n    our circuit breaker is always manufactured by General Electric and they last very long;-*\n\nSwitch to our mobile site",
        "topic_id":19,
        "format_id":4,
        "topic_confidence":0.9106000662,
        "format_confidence":0.5256019831,
        "weight":0.0076423617
    },
    {
        "url":"http:\/\/visibleearth.nasa.gov\/view.php?id=79468",
        "text":"Haze over Eastern China\n\nHaze over Eastern China\n\nThick haze hovered over eastern China on October 20, 2012. The haze stretched from Beijing southward, covering much of the coastal plain bordering Bo Hai and the Yellow Sea. The Moderate Resolution Imaging Spectroradiometer (MODIS) on NASA\u2019s Aqua satellite acquired this natural-color image the same day.\n\nIn many places, the haze was thick enough to completely hide the land or water surface below. A wide band of haze extended eastward over Bo Hai. In the west, isolated mountain peaks poked above the haze, which clogged valleys between the peaks.\n\nAirborne particles are often measured in microns (also micrometers): one-millionth of a meter. Particles with diameters of 2.5 microns or smaller\u2014about one-thirtieth the width of a human hair\u2014are believed to pose the greatest health risks because they can lodge deeply in the lungs, according to the U.S. Environmental Protection Agency. The U.S. Embassy in Beijing measures fine particles, known as PM2.5, and reports the measurements though the BeijingAir Twitter feed. On October 20, the same day that MODIS took this picture, reported air quality ratings ranged from \u201cUnhealthy for Sensitive Groups\u201d to \u201cHazardous.\u201d\n\nIn addition to the U.S. Embassy in Beijing, another site, China Air Daily, provides satellite images, ground-based photos, PM2.5 readings, and other air quality readings for multiple cities in China. The ground-based photo of Beijing showed thick smog on October 20, consistent with the heavy haze in the MODIS image shown above. Developed in in response to a growing desire for more information on air quality, China Air Daily is a project of the Center on U.S.-China Relations at Asia Society.\n\n  1. References\n\n  2. AirNow. (2011, December 9) Air Quality Index. Accessed October 22, 2012.\n  3. U.S. Embassy in Beijing. (2012, October 22) BeijingAir. Accessed October 22, 2012.\n  4. U.S. Environmental Protection Agency. (2012, July 19). Fine Particle Designations: Frequently Asked Questions. Accessed October 22, 2012.\n  5. Zhu, C. (2012, October 17) A visual guide to Chinese air pollution. The Atlantic. Accessed October 23, 2012.\n\nImages & Animations\n\n\nFile Dimensions\n\n  \u2022 720x480\n  \u2022 JPEG\n  \u2022 6200x4800\n  \u2022 JPEG 6 MB\n  \u2022 GeoTIFF 43 MB\n  \u2022 KMZ 2 KB\n\nNote: Often times, due to the size, browsers have a difficult time opening and displaying images. If you experiece an error when clicking on an image link, please try directly downloading the image (using a right click, save as method) to view it locally.\n\n\n  \u2022 Data Date:\n\n    October 20, 2012\n  \u2022 Visualization Date:\n\n    October 22, 2012\n  \u2022 Sensor(s):\n\n    Aqua - MODIS\nNASA - National Aeronautics and Space Administration",
        "topic_id":19,
        "format_id":7,
        "topic_confidence":0.981349647,
        "format_confidence":0.6144602895,
        "weight":0.0105091252
    },
    {
        "url":"http:\/\/www.cs.cmu.edu\/~avrim\/ML06\/lect0206.txt",
        "text":"15-859(B) Machine Learning Theory 02\/06\/06 * Recap * Kalai-Vempala algorithm ========================================================================= Recap from last time ==================== \"Combining expert advice\": Have n prediction rules and want to perform nearly as well as best of them. Or, in game-theoretic setting, have n strategies and want to do nearly as well of best of them in repeated play. Randomized weighted majority algorithm: - Give each a weight (starting at 1). - Choose rule at random with probability proportional to its weight. - When you are told the correct label, penalize rules that made a mistake by multiplying weight by 1-epsilon. (If costs in [0,1] then multiply by (1-epsilon)^cost). Get: E[Alg cost] <= (1+epsilon)*OPT + (1\/epsilon)*log(n). \"Sleeping experts\"\/\"specialists\": at each time step, only some rules fire. Goal: for all i, E[cost_i(Alg)] <= (1+epsilon)*cost(i) + (1\/epsilon)*log(n). where cost_i(Alg) = cost of algorithm in the times when rule i fires. - Generalized version of randomized WM. - Initialize all rules to have weight 1. - At each time step, of the rules i that fire, select one with probability p_i proportional to w_i To update weights: - If didn't fire, leave weight alone. - If did fire, raise or lower depending on performance compared to weighted average: * R_i = [sum_j p_j cost(j)]\/(1+epsilon) - cost(i) * w_i <- w_i(1+epsilon)^{R_i} So, if rule i does exactly as well as weighted average, its weight drops a little. Weight increases if does better than weighted average by more than a (1+epsilon) factor. Can then prove that total sum of weights never goes up. Now notice that one way to look at weights is: * w_i = (1+epsilon)^{E[cost_i(alg)]\/(1+epsilon) - cost_i(i)} I.e., we are explicitly giving large weights to rules for which we have large regret. Since sum of weights <= n, exponent must be at most log_{1+epsilon}(n ) ========================================================================== Today: Notice that RWM bounds are good even if \"n\" is exponential in the natual parameters of the problem. But the running time in that case is bad because we have to explicitly maintain a probability distribution over experts. Are there interesting settings where we can get these kinds of bounds *efficiently*? Today we will discuss an algorithm that can do this in many such cases. Kalai-Vempala Algorithm ======================== The Kalai-Vempala algorithm applies to the following setting. - We have a set S of feasible points in R^n. Each time step t, we (probabilistically) pick x_t in S. We are then given a cost vector c_t and we pay the dot-product c_t . x_t. - The set S may have exponentially-many points, so we don't have time even to list its elements. However, we have a magic offline algorithm M that, given any cost vector c, finds the x in S that minimizes c.x. Want to use to produce an *online* algorithm so that our overall expected cost is not too much worse than for best x in S in hindsight. Specifically, we want for any sequence c_1,...,c_T to get: E[Alg's cost] <= min_{x in S} (c_1 + ... + c_T).x + [regret term] - To have any hope, we will need to assume that S is bounded and so are the cost vectors c_t. What are some things we can model this way? =========================================== - Consider the following adaptive route-choosing problem. Imagine each day you have to drive between two points s and t. You have a map (a graph G) but you don't know what traffic will be like (what the cost of each edge will be that day). Say the costs are only revealed to you after you get to t (on the evening news). We can model this by having one dimension per edge, and each path is represented by the indicator vector listing the edges in the path. Then the cost vector c is just the vector with the costs of each edge that day. Notice that you *could* represent this as an experts problem also, with one expert per path, but the number of s-t paths can easily be exponential in the number of edges in the graph (e.g., think of the case of a grid). However, given any set of edge lengths, we can efficiently compute the best path for that cost vector, since that is just a shortest-path problem. You don't have to explicitly list all possible paths. - We can also model the standard experts problem this way by having each expert be its own coordinate, and c_t is the vector of costs at time t. (But bounds as a fn of n might be worse). - More generally, this is like online linear programming. - Can also model the problem of performing search-tree lookups in a way that is competitive with the best binary search tree in hindsight. For this problem, there is a cost of \"movement\" too (choosing x_{t+1} that is different from x_t), but that can be handled by running the algorithm at a very slow learning rate. For this problem, we have one dimension per element, and a given search tree is represented as the vector of depths of its elements. Requests are indicator vectors. Even though there are exponentially-many search trees on n elements, if we are given a vector c (which can be viewed as listing how many times each element is requested), we can compute the optimal tree for c using dynamic programming. So, we can plug into this framework. The KV Algorithm ================ One natural thing to try is just run M to pick the best x in hindsight, argmin_{x in S} (c_1 + ... + c_{t-1}).x, and use that on day t. But this doesn't work. E.g., imagine there are two routes to work, one fast and one slow, but they alternate which is fast and which is slow from day to day. This procedure will always choose the wrong one. Instead, we will \"hallucinate\" a day 0 in which we have random costs, from an appropriate distribution. Then we will run M to pick the x that minimizes (c_0 + c_1 + ... + c_{t-1}).x. To analyze this, we will argue in stages. Step 1: First, we will show that picking argmin_{x in S} (c_1 + ... + c_t).x *does* work. I.e., if you find the best in hindsight tomorrow and use that today. This is maybe not surprising but also not obvious. Note this is *not* the same as the x that minimizes c_t . x, which obviously would be super-optimal. Now, let algorithm A be the one that minimizes (c_1 + ... + c_{t-1}).x. Let B be the one that minimizes (c_1 + ... + c_t).x. They both go to the bar and start drinking. As they drink, their objective functions get fuzzier and fuzzier. We end up getting: - A' that minimizes (c_0A + c_1 + ... + c_{t-1}).x. - B' that minimizes (c_0B + c_1 + ... + c_t).x. where c_0A, c_0B are picked at random in [-1\/epsilon,1\/epsilon]^n. (Could use a ball instead of a cube and get somewhat different bounds. Cube is simpler anyway). As epsilon gets smaller, A and B start behaving more and more alike. Step 2 is to show that for an appropriate value of epsilon, B' is not too much worse than B, and A' is close to B'. This then shows that A' is a good algorithm. Preliminaries ============= - When we analyzed the experts algorithm, we got bounds like: E[Alg's cost] < OPT(1+epsilon) + (1\/epsilon)*(log n). If you then set epsilon = sqrt(log(n)\/T), where T is the number of time steps you plan to run the algorithm, you get: E[Alg's cost] < OPT + OPT*sqrt(log(n)\/T) + sqrt(T*log(n)). If you then use the fact that OPT <= T, you get: E[Alg's cost] < OPT + 2*sqrt(T log n). (This is the way a game-theory person would usually phrase it: in terms of additive regret.) - In our case, we will assume that the maximum L_1 length (sum of absolute values) of any cost vector is 1. If D is the L_1 diameter of S, the bound we will show is: E[Alg's cost] < OPT + D(epsilon*T\/2 + 1\/epsilon) So, setting epsilon = sqrt(2\/T), we get: E[Alg's cost] < OPT + D*sqrt(2T) If you convert to the experts setting, where c \\in [0,1]^n, the result you will get is an \"n\" inside the square root. This is not as good as the standard experts bounds. They give a different distribution for hallucinating c_0 that fixes that problem. Step 1 ====== Define x_t = argmin_{x in S} (c_1 + ... + c_t) . x. We need to show that: c_1 . x_1 + ... + c_T . x_T <= (c_1 + ... + c_T) . x_T In other words, we do at least as well using algorithm B as the optimal fixed x in hindsight does. We can do this by induction: - By induction, we can assume: c_1.x_1 + ... + c_{T-1}.x_{T-1} <= (c_1+...+c_{T-1}).x_{T-1}. And we know, by definition of x_{T-1}, that: (c_1 + ... + c_{T-1}) . x_{T-1} <= (c_1 + ... + c_{T-1}) . x_T So, putting these together, and adding c_T . x_T to both sides, we get what we want. Step 2: ======= Let's start with the easier part, showing that A' and B' are close. - A' is picking a random objective function from a box of side-length 2\/epsilon centered at c_1 + ... + c_{t-1}. B' is picking a random objective function from a box of side-length 2\/epsilon centered at c_1 + ... c_t. Let's call the boxes box(A) and box(B). - These boxes each have volume V = (2\/epsilon)^n. Since their centers differ by a vector of L_1 length at most 1, the intersection I of the boxes has volume at least (2\/epsilon)^n - (2\/epsilon)^{n-1} = V(1 - epsilon\/2). - This means that A' can't be much worse than B'. In particular, say alpha is your expected loss if you were to pick x by feeding to M a random point c in I. We can view B' as with probability 1-epsilon\/2 doing exactly this and getting expected loss alpha, and with probability epsilon\/2 choosing a random c in box(B) - I, and at best getting a loss of 0. We can view A' as with probability 1-epsilon\/2 getting an expected loss of alpha, and with probability epsilon\/2 getting at worst a loss of D. [Actually, we never said the losses had to be non-negative. The point is that the diameter of S is at most D, so the biggest possible gap between the losses of 2 points in S on any given c is D. Actually, for this part of the argument, we only need L_infty diameter <= D and not L_1 diameter.] The difference between the two is D*epsilon\/2. This happens for T time steps, so that counts for the DT*epsilon\/2 part. Now, to fin... (truncated)",
        "topic_id":19,
        "format_id":0,
        "topic_confidence":0.5479893088,
        "format_confidence":0.90950495,
        "weight":0.0128774028
    },
    {
        "url":"https:\/\/birdsofvermont.org\/2013\/06\/21\/the-second-atlas-of-breeding-birds-of-vermont\/",
        "text":"The Second Atlas of Breeding Birds of Vermont\n\nGuest post by Kir Talmage, Outreach and IT Coordinator for the Birds of Vermont Museum. This article also appeared in the Vermont Great Outdoor Magazine.\n\natlas-cover-1800The Second Atlas of Breeding Birds of Vermont is out! As you likely know, an Atlas is\n\na : a bound collection of maps often including \nillustrations, informative tables, or textual \nb : a bound collection of tables, charts, or plates\n\nThis meager definition masks the huge intention and effort that goes into the creation and revision of an Atlas. This particular Atlas is the product of a state-wide breeding birds research project that has spanned ten years, brought together some 57,000 observations, and drew on 350 volunteers. It epitomizes a successful citizen science project. The data (observations) were pulled together by Vermont Center for Ecostudies into one beautiful reference book, which was published in April of this year. The completed Atlas\u2014with maps, individual species accounts, discussions of Vermont\u2019s habitat and land use changes, and analyses of the data\u2014has already helped scientists and policy makers decide how best to work and plan for avian conservation.\n\nBuilding the Atlas\n\nA Bird Atlas maps the spatial distribution of birds (individual species and groups of species) in a particular place or set of places (e.g., a state) over some set period of time. This Atlas is specific to Vermont (although there are some sampling areas that reach over our borders somewhat), and focuses strictly on those birds that are known to breed in Vermont. A first Atlas was published in 1985; this is one of the first \u201csecond-round\u201d Atlases completed in North America.\n\nIn 2002, conservationists met and began the project of the second Atlas. For the next 5 years, volunteer birders\u2013newbies, professional ornithologists, and experienced birders\u2013looked, listened, and recorded data on birds on specific blocks of land across the state. This phase of the Atlas was generally coordinated through the Vermont Institute of Natural Sciences (VINS). Dr Rosalind Renfrew was the director of the project (and later, the editor of the published edition). Experienced birders trained others and coordinated survey efforts. Their shared goals included:\n\n  \u2022 documenting which birds were in fact breeding and where (the current spatial distribution)\n  \u2022 identify rare species and important breeding areas\n  \u2022 share the work of the Atlas via field trips, public presentation and other outreach activities\n  \u2022 increase citizen scientist participation and skills\n  \u2022 use the current data in comparison with other atlases to look for changes\n\nThe Atlas project defined a clear sampling protocol in order to provide scientific robustness, consistency across the varied experience-levels of the volunteer researchers, and comparability to the first edition. This protocol included specifying where to survey (blocks of land) and how to encode the observations of birds. The blocks of land covered included all those surveyed in the first Atlas project. More were added in order to get a more complete assessment of Vermont. Actual surveying for birds followed standard procedures used in many Atlas projects across North American (see the North America Atlas Committee website for more). Surveying effort was guided by likely breeding dates (\u201csafe dates\u201d), the land itself, and the number of likely species in that type of land (habitat).\n\nAnd all that went on for five years, from 2003-2007.\n\nPutting it all together\n\nThe Vermont Center for Ecostudies (VCE) managed the next phase of the Atlas project: error checking and mapping, and data summaries, and coordinating species accounts. Every observation was reviewed independently to ensure data quality. Much of the actual data is publicly available (some online and more by request to VCE).\n\nWith 350 volunteers logging over 37,000 hours or work, they achieved all the original goals and and more. Every species known to breed in the state is documented and has an accompanying occurrence map. Species richness (how many species are in an area, a useful first-order measure of biodiversity) has been estimated and analyzed both in terms of effort and in comparison to various landscape features, such as elevation or road density. Species have also been grouped by habitat, and these data analyzed as well.\n\nThis Atlas also documents some significant changes since the first Atlas. Some species increased due to management programs, range expansion, and forest habitat changes. Other decreased, apparently due to loss of their habitat types.\n\nUsing the Atlas\n\nBecause this is the second edition, we can use the comparisons with the first to discover conservation issues that are already succeeding as well as those that need new or greater attention. We can see where management programs are succeeding, and which habitats (and thus groups of species) are more vulnerable. Individual species data contribute to better recovery and management plans for endangered species. Individual and groups species data can be used in other research projects, e.g., forest assessments and management issues, or to give solid research-based recommendations to land managers (from single private landowners to municipalities to the Vermont National Guard). You can find out more about what is going on locally by picking up your own copy of the Atlas, checking one out from local libraries (150 Vermont libraries received a free copy), or viewing some of the maps online.\n\nThe Birds of Vermont Museum is collaborating on an exhibit with Vermont Center for Ecostudies to highlight some of stories that have emerged. Which birds have expanded into our changed landscape? What habitats have changed? What species need our focus for better protection? We\u2019ve chosen eight birds and fourteen artists to illustrate these stories and issues. Conservation relies on all of us, from many walks of life and many perspectives, to be successful. We hope this blend of our varied voices and solid data will add to that, just as the many participants have contributed to, and thus strengthened, the Atlas project.\n\nMore information:\nThe Vermont Breeding Bird Atlas website:\nThe Atlas data only:\nThe North American Ornithological Atlas Committee: About Atlases\nAudrey Clark\u2019s article at\nVermont Digger, \u201cFive years in the making, new Vermont Breeding Bird Atlas takes flight\n\nLeave a Reply\n\nYour email address will not be published. Required fields are marked *\n\nThis site uses Akismet to reduce spam. Learn how your comment data is processed.",
        "topic_id":19,
        "format_id":11,
        "topic_confidence":0.9837983847,
        "format_confidence":0.5540487766,
        "weight":0.00876906
    },
    {
        "url":"http:\/\/nationalzoo.si.edu\/scbi\/migratorybirds\/blog\/default.cfm?id=66",
        "text":"Migratory Mystery of a Secretive Sparrow Comes to Light\n\nJanuary 5, 2005 by Russ Greenberg\n\nBrian Olsen takes the measurements of a female swamp sparrow captured in a mist net in a marsh at the Woodland Beach Wildlife Area in Delaware. Olsen is conducting a survey for the National Zoological Park on the success rate of swamp sparrow nests in the marsh. This bird was measured, banded and released. \u00a9 John Barrat\n\nStaring intently at a tangled clump of salt hay and marsh elder growing in Delaware's Woodland Beach Wildlife Area, Russell Greenberg is waiting for the \"chip, chip, chip\" of a female coastal plain swamp sparrow. It's mid-May and the start of nesting season for these secretive, rusty brown birds.\n\nYears ago in a different marsh, Greenberg, director of the Migratory Bird Center at the Smithsonian's National Zoological Park, discovered that each time a female swamp sparrow leaves her nest-about every 15 minutes-she sings the \"chip, chip, chip.\" By glimpsing the bird at the instant she sings, a seasoned ornithologist like Greenberg can usually locate her well-concealed nest.\n\nToday, he is helping graduate student Brian Olsen, from Virginia Polytechnic Institute, find and mark coastal plain swamp sparrow nests as part of a National Zoo study to document how many nests are lost to high tides, raccoons and other perils of the salt marsh.\n\nWith binoculars and rubber hip waders, Greenberg has been patiently observing swamp sparrows in the wild for more than 20 years. Recently, his knowledge of these birds, combined with a bit of cutting-edge scientific sleuthing, paid off. In March, he and Peter Marra, senior scientist at the Smithsonian Environmental Research Center on the Chesapeake Bay, solved a mystery that has clouded scientists' understanding of the coastal plain swamp sparrow since the discovery of this subspecies some 50 years ago.\n\nEach September, after a summer of laying eggs and raising chicks in the wetlands of New Jersey, Delaware and Maryland, the coastal plain swamp sparrow disappeared somewhere south. No one knew its migratory destination or had seen it during the winter months.\n\nSeconds after hearing a female swamp sparrow's nest departure call, Russell Greenberg locates her nest in dry marsh grass at the Woodland Beach Wildlife Area. \u00a9 John Barrat\n\n\"Here was an entire North American taxa for which no one knew where they wintered,\" Greenberg says. \"That's kind of incredible.\"\n\nThrough analysis of the hydrogen and carbon isotopes in the male sparrow's cap feathers, Greenberg and Marra were able to \"trick the birds into revealing where they winter,\" Greenberg says.\n\nIsotopic signature\n\nBy hand-rearing a number of coastal plain swamp sparrows at the National Zoo, Greenberg confirmed that, like other swamp sparrows, males of the coastal plain subspecies sprout a reddish patch of feathers on their heads in February or March. This plumage badge appears just as the birds are getting ready to head north to their breeding grounds.\n\n\"It's just a little rusty cap,\" Greenberg says, opening a field guide during an interview at the Migratory Bird Center to a picture of the sparrow.\n\nLater, Greenberg collected samples of the rusty cap feathers from wild sparrows he had caught in Delaware salt marshes. He and Marra sent the feathers to Matt Wooller of the University of Alaska at Fairbanks for analysis using isotope-ratio mass spectrometry. This scientific technique yielded what Greenberg calls an \"isotopic signature\" of the feathers.\n\nThe research is based on the knowledge that birds, like all creatures, are what they eat. Elements such as oxygen, carbon and hydrogen that occur naturally in soil, air and water are absorbed into the body tissues of animals through eating, breathing and drinking.\n\nA swamp sparrow nest containing 4 eggs. \u00a9 John Barrat\n\nThe number of neutrons in an element's nucleus can vary by geographic location. For example, a carbon atom found in one region of North America may have six neutrons. In another habitat, a carbon atom may have seven or eight neutrons. By mapping these variations geographically, isotopic signatures found in animal tissues can be linked to specific regions.\n\nMarra likens isotope readings from tissues to \"snapshots\" of where a bird has been spending its time. He has been using isotope analysis to draw connections between wintering and breeding populations of American redstarts, another species of migratory songbird.\n\nDeuterium pattern\n\nFeathers are an especially useful tissue for tracking male coastal plain swamp sparrows because, once the sparrows sprout their rust-colored caps, those feathers stop growing. No longer metabolically active, the feathers effectively freeze the isotopic signal the birds have picked up from their winter habitat.\n\nDeuterium, a stable isotope of hydrogen, is the particular signal Greenberg and Marra \"tuned in\" during their coastal plain swamp sparrow study. Scientists have long known that rain in southern latitudes contains more deuterium than precipitation farther north. This pattern of deuterium variation has been mapped.\n\nGreenberg and Marra were able to match the isotopic signature of male coastal plain swamp sparrow cap feathers to latitudes of an area stretching from Charleston, S.C., to Beaufort, N.C.\n\nSimilar analysis of carbon isotopes in the feathers suggested that the birds stuck to coastal marshes and did not venture inland during the winter. In March, Greenberg and Marra traveled to North Carolina and, for the first time, found and photographed the coastal plain swamp sparrow in its winter habitat. They located 12 coastal plain swamp sparrows in three different North Carolina sites. All the birds were in marshes on the mainland bordering Pamlico and Albemarle sounds.\n\nIt does at first seem odd that these birds only migrate such a short distance, Greenberg notes. \"Also, the migration takes place, we think, quite early. So they do not seem forced down by bad weather.\n\n\"Although the distance is small, the climate of the coastal Carolinas is very different\" from coastal New Jersey, Maryland and Delaware, Greenberg continues. \"It is really the first place where winter freezes don't occur regularly. For a sparrow that pokes around in the mud to feed, unfrozen ground is probably a premium.\"\n\nExtended studies\n\nThe dark back, black forehead and dark eyestripe of the coastal plain swamp sparrow distinguish it from other swamp sparrows. This photo was taken at the Bombay Hook National Wildlife Refuge in Delaware. \u00a9 Gerhard Hofmann\n\nLocating their winter home is just one of several studies the National Zoo's Migratory Bird Center is carrying out on this subspecies. Researchers also are examining how the coastal plain swamp sparrow has adapted to life in the salt marshes. Most swamp sparrows live inland, in freshwater marshes and swamps.\n\nThe coastal plain subspecies has a number of physical traits that set it apart from its inland cousins. It shares these traits with other North American sparrow species that live in salt marshes. \"There's a whole suite of differences\" between freshwater and saltwater swamp sparrows, Greenberg says. \"For example, salt-marsh swamp sparrows all tend to be very gray.\" Their plumage is probably a camouflaging adaptation to the gray and black muds usually found in salt marshes.\n\nCoastal plain swamp sparrows, like other salt-marsh sparrows, also have large kidneys-probably because they need to flush more fluid through their bodies to eliminate the salt they ingest from brackish water.\n\nIn addition, salt-marsh sparrows have longer, thinner bills, which are better for eating invertebrates. Inland sparrows, with short, broad bills, eat more seeds, which are in short supply in salt marshes.\n\nFor years, the Migratory Bird Center has worked with experienced volunteers from the Delmarva Ornithological Society to census populations of coastal plain swamp sparrows during their breeding season.\n\nGreenberg notes that the subspecies has disappeared from some sites where it once bred. \"Their geographic range is small, and ecologically, they're restricted,\" he says. Greenberg and his colleagues estimate as few as 25,000 breeding pairs of coastal plain swamp sparrows exist. It's a situation \"of conservation concern,\" Greenberg says.\n\nWith the discovery of the coastal plain swamp sparrow's winter range, an important chapter of this bird's life history has been revealed, Marra says.\n\n\u201cTo fully understand the ecology of this subspecies, and to devise appropriate measures to protect them, we need to know where they are year round. This knowledge is critical.\u201d\n\nExpedition Blog Archives",
        "topic_id":19,
        "format_id":11,
        "topic_confidence":0.994363308,
        "format_confidence":0.6943537593,
        "weight":0.0088632301
    },
    {
        "url":"https:\/\/extension.umd.edu\/hgic\/topics\/poison-ivy",
        "text":"University of Maryland Extension\n\nPoison ivy\n\nPoison ivy\nToxicodendron radicans\n\npoison ivy\n\nHow to identify poison ivy - Learn to recognize the many look-a-likes.\n\nLife cycle: perennial deciduous, woody vine; contains urushiol that causes inflammation, blisters, and itching\nGrowth habit: climbs trees or structures by aerial roots that form hairy, fibrous ropes; vining habit in shade, more bush-like in sun; leaves have 3 glossy 2-4 in. long leaflets, margins variably toothed, lobed, or nearly entire\nReproduction: seeds spread by animals and birds that eat the fruit and by stems that root\nConditions that favor growth: can invade landscapes, woodlands, wetlands by creeping stems or seeds; thrives under a variety of conditions\nCultural control: identify seedlings and remove early in the season; on trees, sever roots growing up the trunk to eliminate the flow of moisture from roots; always take care to protect all parts of the body from coming in contact with all parts of the plant; never burn the plant as toxins can be inhaled in smoke.\n\nHGIC Publication:\u00a0(PDF) HG34 Poison Ivy\n(PDF)\u00a0The Ohio State University - Poison Ivy Information\n\n\u00a0Return to weed gallery\n\nMaintained by the IET Department of the College of Agriculture and Natural Resources. \u00a9 2019. Web Accessibility",
        "topic_id":19,
        "format_id":7,
        "topic_confidence":0.8401914835,
        "format_confidence":0.9860693216,
        "weight":0.0089974837
    },
    {
        "url":"http:\/\/itsastatlife.blogspot.com\/2012\/06\/",
        "text":"Sunday, June 24, 2012\n\nNational Traits and the Ecological Fallacy\n\nThis article caught my eye, headlined \"Why criminals believe in heaven.\" [The Daily Mail's 'Science' and 'Health' pages are an unlimited resource for writing blog posts about bad statistics, but I promise to try not to\u00a0only\u00a0pick on them in future.]\n\nThe original paper is here\u00a0(predictably the DM don't bother to link to it), and here is the university's press release which has been pretty slavishly copied to create the article. \u00a0\u00a0The summary is, the researchers combined crime data from one source (the United Nations Office on Drugs and Crime), with survey data on people's attitudes to religion and other things (from World Values Surveys and European Value Surveys). \u00a0They found that the belief in heaven and hell is a strong predictor of crime rates at a country-wide level, whilst rates of particular religious beliefs were not.\n\nSaturday, June 23, 2012\n\nReview: The Geek Manifesto\n\nThis week I've been reading Mark Henderson's book\u00a0The Geek Manifesto: Why Science Matters, which discusses the role of science and its geeky proponents in public life. Henderson's principal thesis is that politicians and scientists have been wilfully ignorant of each other for too long, and that it is time for science, its methods, and its practitioners to take a more central place in politics and policy. \u00a0This includes statistics, of course, which is why it seemed relevant to the blog.\n\nSunday, June 17, 2012\n\nHow should statistics be taught? Some thoughts.\n\nInspired by Timothy Gowers' recent post on how mathematics should be taught to non-mathematicians, I thought it might be prudent to ask how statistics should be taught. \u00a0If you need to be motivated as to how important teaching statistics is, watch\u00a0Arthur Benjamin's short TED talk.\n\nSaturday, June 2, 2012\n\nNewsflash: everyone being healthier reduces deaths\n\nPeople like me often complain about the accuracy of scientific journalism, and the selective nature of the reports we see in the media; indeed, there's plenty to complain about. \u00a0However it's perfectly possible to misrepresent a story simply by choosing to place a particular spin upon it, usually one which makes the story seem more political than the original research. \u00a0It's easy to blame the newspapers for this, but often the problem is compounded by academic press releases, which are themselves designed to catch the eye of media outlets.",
        "topic_id":19,
        "format_id":15,
        "topic_confidence":0.967551291,
        "format_confidence":0.9020753503,
        "weight":0.0170262059
    }
]